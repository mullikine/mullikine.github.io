<!doctype html>
<html lang="en-us">
  <head>
    <title>Review of &#39;GPT-3: Language Models are Few-Shot Learners (Paper Explained) - YouTube&#39; // Bodacious Blog</title>
    <meta charset="utf-8" />
    <meta name="generator" content="Hugo 0.71.0" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="author" content="Shane Mulligan" />
    <meta name="description" content="" />
    <link rel="stylesheet" href="https://mullikine.github.io/css/main.min.c93a97e54f16df99439e5c4acf79edb18fcb9e5384f1edc008a7a4639844b254.css" />

    
    <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Review of &#39;GPT-3: Language Models are Few-Shot Learners (Paper Explained) - YouTube&#39;"/>
<meta name="twitter:description" content="Work in progress
 Original video GPT-3: Language Models are Few-Shot Learners (Paper Explained) - YouTube  Demonstration of me reading this video and taking notes 
GPT-3 GPT 3 has 175 billion parameters which this is absolutely crazy is an order of magnitude higher than anything that ever existed.
GPT-2 parameters by comparison This is what people are talking about when they say parameters.
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  Parameters Single Transformer block Conv1d attn/c_attn w 768 2304 1769472 b 2304 2304 attn/c_proj w 768 768 589824 b 768 768 mlp/c_fc w 768 3072 2359296 b 768 768 mlp/c_proj w 3072 3072 2359296 b 768 768 Norm ln_1 g 768 768 b 768 768 ln_2 g 768 768 b 768 768 total 7085568 per block X 12 blocks 85026816 In all blocks Embeddings 50257 768 38597376 Positional Embeddings 2024 768 786432 Grand Total 124410624   What you can do with giant LMs?"/>

    <meta property="og:title" content="Review of &#39;GPT-3: Language Models are Few-Shot Learners (Paper Explained) - YouTube&#39;" />
<meta property="og:description" content="Work in progress
 Original video GPT-3: Language Models are Few-Shot Learners (Paper Explained) - YouTube  Demonstration of me reading this video and taking notes 
GPT-3 GPT 3 has 175 billion parameters which this is absolutely crazy is an order of magnitude higher than anything that ever existed.
GPT-2 parameters by comparison This is what people are talking about when they say parameters.
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  Parameters Single Transformer block Conv1d attn/c_attn w 768 2304 1769472 b 2304 2304 attn/c_proj w 768 768 589824 b 768 768 mlp/c_fc w 768 3072 2359296 b 768 768 mlp/c_proj w 3072 3072 2359296 b 768 768 Norm ln_1 g 768 768 b 768 768 ln_2 g 768 768 b 768 768 total 7085568 per block X 12 blocks 85026816 In all blocks Embeddings 50257 768 38597376 Positional Embeddings 2024 768 786432 Grand Total 124410624   What you can do with giant LMs?" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://mullikine.github.io/posts/review-of-gpt-3-language-models-are-few-shot-learners-paper-explained-youtube/" />
<meta property="article:published_time" content="2020-05-30T00:00:00+08:00" />
<meta property="article:modified_time" content="2020-05-30T00:00:00+08:00" /><meta property="og:site_name" content="Bodacious Blog" />


  </head>
  <body>
    <header class="app-header">

<a href="https://mullikine.github.io"><img class="app-header-avatar" src="http://mullikine.github.io/fievel.png" alt="Shane Mulligan" /></a>
      <h1>Bodacious Blog</h1>
      
      <div class="app-header-social">
        
          <a target="_blank" href="https://github.com/mullikine"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-github">
  <title>github</title>
  <path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22"></path>
</svg></a>
        
          <a target="_blank" href="https://linkedin.com/in/shane-mulligan-811b942b/"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-linkedin">
  <title>linkedin</title>
  <path d="M16 8a6 6 0 0 1 6 6v7h-4v-7a2 2 0 0 0-2-2 2 2 0 0 0-2 2v7h-4v-7a6 6 0 0 1 6-6z"></path><rect x="2" y="9" width="4" height="12"></rect><circle cx="4" cy="4" r="2"></circle>
</svg></a>
        
          <a target="_blank" href="https://gitlab.com/mullikine"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-gitlab">
  <title>gitlab</title>
  <path d="M22.65 14.39L12 22.13 1.35 14.39a.84.84 0 0 1-.3-.94l1.22-3.78 2.44-7.51A.42.42 0 0 1 4.82 2a.43.43 0 0 1 .58 0 .42.42 0 0 1 .11.18l2.44 7.49h8.1l2.44-7.51A.42.42 0 0 1 18.6 2a.43.43 0 0 1 .58 0 .42.42 0 0 1 .11.18l2.44 7.51L23 13.45a.84.84 0 0 1-.35.94z"></path>
</svg></a>
        
          <a target="_blank" href="https://twitter.com/mullikine"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-twitter">
  <title>twitter</title>
  <path d="M23 3a10.9 10.9 0 0 1-3.14 1.53 4.48 4.48 0 0 0-7.86 3v1A10.66 10.66 0 0 1 3 4s-4 9 5 13a11.64 11.64 0 0 1-7 2c9 5 20 0 20-11.5a4.5 4.5 0 0 0-.08-.83A7.72 7.72 0 0 0 23 3z"></path>
</svg></a>
        
        <a href="https://mullikine.github.io/cv/">CV</a>
      </div>
<div class="highlights">
<a class="big" href="http://otagoai.com/blog/generating-poetry-with-gpt-2/">TakaheAI Blog</a>
<br />
<a href="https://mullikine.github.io/posts/practical-macros-in-racket-and-how-to-work-with-them/">Practical macros in Racket</a>
<br />
<a href="https://mullikine.github.io/posts/generating-hyperlinks-for-glossaries-and-other-parsers-in-emacs/">Hyperlink generation in emacs</a>
<br />
<a href="https://mullikine.github.io/posts/github-search-with-bigquery/">Searching GitHub with BigQuery</a>

<br />
<a href="https://mullikine.github.io/posts/arbitrary-interpreters-for-babel/">Arbitrary interpreters for Babel</a>
<br />
<a href="https://mullikine.github.io/glossary.html">Glossary A-Z (298 topics)</a> <a href="https://github.com/mullikine/glossaries-gh">code</a>
<br />
<a href="https://mullikine.github.io/posts/dwarf-fortress-macros-with-emacs-and-tmux/">Automating Dwarf Fortress</a>
<br />
<a href="https://mullikine.github.io/posts/review-of-the-illustrated-transformer/">The Illustrated Transformer</a>
<br />
<a href="https://mullikine.github.io/codelingo-vs-linters/summary/">CodeLingo vs Linters</a>
<br />
<div class="taglist">

<a class="tag" href="https://mullikine.github.io/tags/biosemiotics/">biosemiotics</a>
<a class="tag" href="https://mullikine.github.io/tags/xenolinguistics/">xenolinguistics</a>
<a class="tag big" href="https://mullikine.github.io/tags/emacs/">emacs</a>
<a class="tag" href="https://mullikine.github.io/tags/gpt/">GPT (Generative Pre-trained Transformer)</a>
<a class="tag" href="https://mullikine.github.io/tags/elisp/">elisp</a>
<a class="tag" href="https://mullikine.github.io/tags/racket/">racket</a>
<a class="tag" href="https://mullikine.github.io/tags/haskell/">haskell</a>
<a class="tag" href="https://mullikine.github.io/tags/nlp/">NLP</a>
<a class="tag" href="https://mullikine.github.io/tags/docker/">docker</a>
<a class="tag" href="https://mullikine.github.io/tags/feature-engineering/">feature-engineering</a>
<a class="tag big" href="https://mullikine.github.io/tags/ir/">IR</a>
<a class="tag" href="https://mullikine.github.io/tags/games/">games</a>
<a class="tag" href="https://mullikine.github.io/tags/data/">data</a>
<a class="tag" href="https://mullikine.github.io/tags/info/">info theory</a>
<a class="tag" href="https://mullikine.github.io/tags/probability/">probability</a>
<a class="tag" href="https://mullikine.github.io/tags/problog/">problog</a>
<a class="tag big" href="https://mullikine.github.io/tags/shell/">shell</a>
<a class="tag" href="https://mullikine.github.io/tags/gcp/">GCP</a>
<a class="tag big" href="https://mullikine.github.io/tags/github/">GitHub</a>
<a class="tag" href="https://mullikine.github.io/tags/parsers/">parsers</a>
<a class="tag" href="https://mullikine.github.io/tags/rust/">rust</a>
<a class="tag" href="https://mullikine.github.io/tags/cpp/">c++</a>
<a class="tag" href="https://mullikine.github.io/tags/review/">review</a>
<a class="tag" href="https://mullikine.github.io/tags/kaggle/">kaggle</a>
<a class="tag" href="https://mullikine.github.io/tags/dl/">deep learning</a>
<a class="tag" href="https://mullikine.github.io/tags/dsl/">DSL</a>
<a class="tag" href="https://mullikine.github.io/tags/df/">dwarf fortress</a>
<a class="tag" href="https://mullikine.github.io/tags/spacy/">spacy</a>
<a class="tag" href="https://mullikine.github.io/tags/latex/">latex</a>
<a class="tag" href="https://mullikine.github.io/tags/nix/">Nix</a>
<a class="tag" href="https://mullikine.github.io/tags/diagrams/">diagrams</a>
<a class="tag" href="https://mullikine.github.io/tags/python/">python</a>
<a class="tag" href="https://mullikine.github.io/tags/golang/">golang</a>
<a class="tag" href="https://mullikine.github.io/tags/codelingo/">codelingo</a>
<a class="tag" href="https://mullikine.github.io/tags/aws/">AWS</a>
<a class="tag" href="https://mullikine.github.io/tags/perl/">perl</a>
<a class="tag big" href="https://mullikine.github.io/tags/vim/">vim</a>
<a class="tag" href="https://mullikine.github.io/tags/telco/">telco</a>
<a class="tag" href="https://mullikine.github.io/tags/automation/">automation</a>
<a class="tag" href="https://mullikine.github.io/tags/terminals/">terminals</a>
<a class="tag" href="https://mullikine.github.io/tags/transformer/">transformer</a>
<a class="tag big" href="https://mullikine.github.io/tags/codegen/">code-gen</a>
<a class="tag" href="https://mullikine.github.io/tags/optimisation/">optimisation</a>
<a class="tag" href="https://mullikine.github.io/tags/release/">release</a>
<a class="tag" href="https://mullikine.github.io/tags/dotnet/">.NET</a>
<a class="tag" href="https://mullikine.github.io/tags/csharp/">csharp</a>
<a class="tag" href="https://mullikine.github.io/tags/tooling/">tooling</a>
<a class="tag" href="https://mullikine.github.io/tags/iac/">IaC</a>
<a class="tag big" href="https://mullikine.github.io/tags/facebook/">Facebook</a>
<a class="tag" href="https://mullikine.github.io/tags/wfh/">WFH</a>


<a class="tag" href="https://mullikine.github.io/tags/babel/">babel</a>
<a class="tag big" href="https://mullikine.github.io/tags/uber/">Uber</a>
<a class="tag" href="https://mullikine.github.io/tags/math/">math</a>
<a class="tag" href="https://mullikine.github.io/tags/microsoft/">Microsoft</a>
<a class="tag big" href="https://mullikine.github.io/tags/openai/">OpenAI</a>
<a class="tag" href="https://mullikine.github.io/tags/rosie/">rosie</a>
<a class="tag" href="https://mullikine.github.io/tags/terraform/">Terraform</a>
<a class="tag" href="https://mullikine.github.io/tags/elasticsearch/">ELK</a>
<a class="tag" href="https://mullikine.github.io/tags/semmle/">Semmle</a>
<a class="tag" href="https://mullikine.github.io/tags/expect/">tcl/expect</a>
</div>
</div>



</div>
<div>
    </header>
    <main class="app-container">
      
  <article class="post">
    <header class="post-header">
      <h1 class ="post-title">Review of &#39;GPT-3: Language Models are Few-Shot Learners (Paper Explained) - YouTube&#39;</h1>
      <div class="post-meta">
        <div>
          <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-calendar">
  <title>calendar</title>
  <rect x="3" y="4" width="18" height="18" rx="2" ry="2"></rect><line x1="16" y1="2" x2="16" y2="6"></line><line x1="8" y1="2" x2="8" y2="6"></line><line x1="3" y1="10" x2="21" y2="10"></line>
</svg>
          May 30, 2020
        </div>
        <div>
          <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-clock">
  <title>clock</title>
  <circle cx="12" cy="12" r="10"></circle><polyline points="12 6 12 12 16 14"></polyline>
</svg>
          6 min read
        </div><div>
          <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tag">
  <title>tag</title>
  <path d="M20.59 13.41l-7.17 7.17a2 2 0 0 1-2.83 0L2 12V2h10l8.59 8.59a2 2 0 0 1 0 2.82z"></path><line x1="7" y1="7" x2="7" y2="7"></line>
</svg>
          <a class="tag" href="https://mullikine.github.io/tags/gpt/">gpt</a></div></div>
    </header>
    

<link rel="stylesheet" type="text/css" href="https://mullikine.github.io/css/magit.css"/>

<script src="https://mullikine.github.io/js/mathjax-config.js"></script>
 
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_HTML"></script>


    <div class="post-content">
      <p><em><span class="underline"><strong>Work in progress</strong></span></em></p>
<dl>
<dt>Original video</dt>
<dd><a href="https://www.youtube.com/watch?v=SY5PvZrJhLE">GPT-3: Language Models are Few-Shot Learners (Paper Explained) - YouTube</a></dd>
</dl>
<h2 id="demonstration-of-me-reading-this-video-and-taking-notes">Demonstration of me reading this video and taking notes</h2>
<p><a title="asciinema recording" href="https://asciinema.org/a/q6dDDOC1fIGBB2FgSreMK06GE" target="_blank"><img alt="asciinema recording" src="https://asciinema.org/a/q6dDDOC1fIGBB2FgSreMK06GE.svg" /></a></p>
<h2 id="gpt-3"><code>GPT-3</code></h2>
<p>GPT 3 has 175 billion parameters which this is
absolutely crazy is an order of magnitude
higher than anything that ever existed.</p>
<h3 id="gpt-2-parameters-by-comparison"><code>GPT-2</code> parameters by comparison</h3>
<p>This is what people are talking about when they say <code>parameters</code>.</p>
<div class="highlight"><div style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">
<table style="border-spacing:0;padding:0;margin:0;border:0;width:auto;overflow:auto;display:block;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">15
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">16
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">17
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">18
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">19
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">20
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">21
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">                                                                                      Parameters
Single Transformer block     Conv1d      attn/c_attn     w       768     2304         1769472
                                                         b               2304         2304
                                         attn/c_proj     w       768     768          589824
                                                         b               768          768
                                         mlp/c_fc        w       768     3072         2359296
                                                         b               768          768
                                         mlp/c_proj      w       3072    3072         2359296
                                                         b               768          768
                             Norm        ln_1            g               768          768
                                                         b               768          768
                                         ln_2            g               768          768
                                                         b               768          768
                                                                         total        7085568     per block

                                                                         X 12 blocks  85026816    In all blocks

                                         Embeddings              50257   768          38597376
                                         Positional Embeddings   2024    768          786432

                                                                         Grand Total  124410624</code></pre></td></tr></table>
</div>
</div>
<h2 id="what-you-can-do-with-giant-lms">What you can do with giant LMs?</h2>
<ul>
<li>
<p>Absolutely crazy things</p>
<ul>
<li>Translate to french in few shots</li>
</ul>
</li>
<li>
<p>Nothing but a model that can
kind of generate language in a probabilistic
way.</p>
</li>
<li>
<p>The cool thing about LMs is:</p>
<ul>
<li>You can train it on any sort of text data.</li>
</ul>
</li>
</ul>
<p>A transformer model is just several layers of
attention mechanism.</p>
<div class="highlight"><div style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">
<table style="border-spacing:0;padding:0;margin:0;border:0;width:auto;overflow:auto;display:block;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">15
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">16
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">17
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">18
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">19
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">20
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">21
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">22
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">23
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">24
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">25
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">26
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">27
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">28
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">29
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">30
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">31
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">32
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">33
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">34
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">35
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">36
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">37
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">38
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">39
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">40
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">41
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">42
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">43
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">44
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">45
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">46
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">47
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">48
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">49
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">50
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">51
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">52
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">53
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">54
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">55
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">56
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">57
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">58
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">59
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">60
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">61
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">62
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">63
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">64
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">65
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">66
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">67
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">68
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">69
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">70
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">71
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">72
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">73
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">74
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">75
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">76
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">77
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">78
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">79
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">attention mechanism
attention
    [mechanism]

    https://skymind.ai/wiki/attention-mechanism-memory-network

    Attention mechanisms and structures such
    as CNNs and RNNs are not mutually
    exclusive. In fact, all of them can be
    combined with no perjury.

    Types of attention (in the Transformer)
    https://youtu.be/BhlOGGzC0Q0?t=569
    - encoder-decoder attention
    - encoder self-attention
    - decoder self-attention

    attention != choice
    attention == focus

    https://www.coursera.org/lecture/nlp-sequence-models/attention-model-intuition-RDXpX

    It&#39;s possible to learn the attention
    matrix.

    Concepts:
    - Attention can be directed at the present
      and the past.

      What is the neural network paying
      attention to?

    Serves:
    - as a memory-access mechanism.
      to orient memory access
    - to orient perception as well as

    You might even say perception is just a
    very short-term subset of all memory.

    Filters the perceptions that can be stored
    in memory, and filters them again on a
    second pass when they are to be retrieved
    from memory.

    Matters because:
    - it has been shown to produce
      state-of-the-art results in machine
      translation and other natural language
      processing tasks, when combined with
      neural word embeddings, and
    - is one component of breakthrough
      algorithms such as Transformer and BERT,
      which is setting new records in accuracy
      in NLP.

    Part of our best effort to date to create
    real natural-language understanding in
    machines.

    If that succeeds, it will have an enormous
    impact on society and almost every form of
    business.

    Describes the mind’s ability to allocate
    consideration unevenly across a field of
    sensation, thought and proprioception, to
    focus and bring certain inputs to the
    fore, while ignoring or diminishing the
    importance of others.

    https://www.youtube.com/watch?v=SY5PvZrJhLE
        Now an attention mechanism is
        basically a way where information is
        routed in between the different tokens
        [right here] and as it goes up the
        layer, (basically) the information is
        routed around and the model can make
        various inferences and at the end.</code></pre></td></tr></table>
</div>
</div>
<div class="highlight"><div style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">
<table style="border-spacing:0;padding:0;margin:0;border:0;width:auto;overflow:auto;display:block;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">15
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">16
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">17
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">18
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">19
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">20
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">21
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">22
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">23
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">24
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">25
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">26
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">27
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">28
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">29
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">30
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">31
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">32
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">33
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">34
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">35
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">36
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">37
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">38
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">39
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">transformer
    [architecture]

    https://rubikscode.net/2019/07/29/introduction-to-transformers-architecture/

    Transformer architectures are not designed
    to operate efficiently on CPU, so we
    recommend you have a GPU available for
    both training and usage.

    https://explosion.ai/blog/spacy-pytorch-transformers

    Let’s roll back a bit to understand the
    basic concept of Transformers.

    Multi-Head Attention block.
        We are now familiar with RNN’s
        shortcoming is that it is not well
        versed in handling dependencies
        between input or output tokens
        themselves.

        To handle this flaw, the Transformer
        just allows the encoder and decoder to
        see the entire input sequence all at
        once, directly modelling these
        dependencies using self-attention.

        This fundamental idea of transformer
        is implemented by its vital component,
        the Multi-Head Attention block.

    https://jalammar.github.io/illustrated-transformer/

    Types of attention (in the Transformer)
    https://youtu.be/BhlOGGzC0Q0?t=569
    - encoder-decoder attention
    - encoder self-attention
    - decoder self-attention</code></pre></td></tr></table>
</div>
</div>
<p>It&rsquo;s not like Bert; it&rsquo;s not by direction, it
is autoregressive, it goes from left to right.</p>
<div class="highlight"><div style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">
<table style="border-spacing:0;padding:0;margin:0;border:0;width:auto;overflow:auto;display:block;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">4
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">5
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">6
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">autoregressive model
    An autoregressive model is a
    representation of a type of random
    process; as such, it is used to describe
    certain time-varying processes in nature,
    economics, etc.</code></pre></td></tr></table>
</div>
</div>
<div class="highlight"><div style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">
<table style="border-spacing:0;padding:0;margin:0;border:0;width:auto;overflow:auto;display:block;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">15
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">16
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">17
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">18
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">19
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">20
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">21
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">22
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">23
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">24
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">25
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">26
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">27
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">28
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">29
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">30
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">31
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">32
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">33
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">34
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">35
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">36
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">37
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">38
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">39
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">40
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">41
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">42
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">43
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">44
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">45
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">46
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">47
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">48
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">49
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">50
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">51
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">52
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">53
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">54
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">55
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">56
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">57
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">58
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">59
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">60
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">61
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">62
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">63
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">64
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">65
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">66
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">67
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">68
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">69
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">70
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">71
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">72
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">73
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">74
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">75
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">76
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">77
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">78
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">BERT
Bidirectional Encoder Representations from Transformers
    [model]
    - contextual
    - late 2018
    - can do transfer learning

    Uses MLM and NSP to learn text
    representation.

    MLM is a way to mask some tokens and using
    the rest of tokens to predict the masked
    token.

    open https://towardsdatascience.com/breaking-bert-down-430461f60efb
    open https://www.kdnuggets.com/2019/09/bert-changing-nlp-landscape.html

    ewwlinks +/&#34;a new language representation model called BERT&#34; &#34;https://www.topbots.com/most-important-ai-nlp-research/?fbclid=IwAR13D2E2metFFXeRpYPG6FHtJGvxTlAH04jZpAM2OePeet993xjBz5zjZSY&#34;

    vim +/&#34;## What is BERT?&#34; &#34;$MYGIT/google-research/bert/README.md&#34;

    &#34;Bidirectional&#34; refers to its ability to
    understand language ambiguity.

    Separates itself from other training
    models by learning the relationships
    between sentences and accurately applying
    those to pretrain deep neural networks.

    https://arxiv.org/abs/1906.04341

    What Makes BERT so Amazing?
    - BERT is a contextual model

      Word embeddings are generated based on
      the context of the word’s use in a
      sentence, and thus a single word can
      have multiple embeddings. For example,
      BERT would produce different embeddings
      for Mercury in the following two
      sentences: “Mercury is visible in the
      night sky” and “Mercury is often
      confused with Hermes, the fleet-footed
      messenger of Greek gods.”

    - BERT enables transfer learning

      This is referred to as “NLP’s ImageNet
      Moment.” Google has pre-trained BERT on
      Wikipedia, and this pre-trained model
      can now be used on other more specific
      datasets like a customer support bot for
      your company. And remember this
      pre-training is expensive, which you can
      now skip. So, your starting point is a
      smart model (trained on general human
      speech) not just an algorithm in need of
      training.

    - BERT can be fine-tuned cheaply and
      quickly on a small set of
      domain-specific data and will yield more
      accurate results than by training on
      these same domain-s

    Bidirectional
        BERT is naturally bidirectional.
    Generalizable
        Pre-trained BERT model can be
        fine-tuned easily for downstream NLP
        task.
    High-Performance
        Fine-tuned BERT models beat
        state-of-the- art results for many NLP
        tasks.
    Universal
        Trained on Wikipedia + BookCorpus. No
        special dataset needed.</code></pre></td></tr></table>
</div>
</div>

    </div>
    <div class="post-footer">
        <p>
        <a href="https://mullikine.github.io/about/">About this weblog</a>
        &nbsp;
        ⚔
        &nbsp;
        <a href="https://asciinema.org/~mullikine">Asciinema recordings</a>
        &nbsp;
        ⚔
        &nbsp;
        <a href="https://mullikine.github.io/posts/blogs-and-vlogs/">Blogs and Vlogs</a>
        &nbsp;
        ⚔
        &nbsp;
        <a href="https://mullikine.github.io/tags/">Site Map</a>
        </p>
    </div>
  </article>

    </main>
  </body>
</html>
