<!doctype html>
<html lang="en-us">
  <head>
    <title>Controlled Text Generation // Bodacious Blog</title>
    <meta charset="utf-8" />
    <meta name="generator" content="Hugo 0.59.0-DEV" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="author" content="Shane Mulligan" />
    <meta name="description" content="" />
    <link rel="stylesheet" href="https://mullikine.github.io/css/main.min.e6381a73553ad4d469d9101ef472370acba58b6ff4efbf1f553e5a44e4cd11b3.css" />

    
    <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Controlled Text Generation"/>
<meta name="twitter:description" content="Original article https://eng.uber.com/pplm/ https://github.com/uber-research/PPLM https://github.com/huggingface/transformers/blob/master/examples/pplm/README.md Uber AI Plug and Play Language Model  Controlling Text Generation with Plug and Play Language Model (PPLM) PPLM builds on top of other large transformer- based generative models (like GPT-2), where it enables finer-grained control of attributes of the generated language (e.g. gradually switching topic üê± or sentiment üòÉ).
This controlled LG method consists of plugging in simple bag-of-words or one-layer classifiers as attribute controllers, and making updates in the activation space, without changing any model parameters."/>

    <meta property="og:title" content="Controlled Text Generation" />
<meta property="og:description" content="Original article https://eng.uber.com/pplm/ https://github.com/uber-research/PPLM https://github.com/huggingface/transformers/blob/master/examples/pplm/README.md Uber AI Plug and Play Language Model  Controlling Text Generation with Plug and Play Language Model (PPLM) PPLM builds on top of other large transformer- based generative models (like GPT-2), where it enables finer-grained control of attributes of the generated language (e.g. gradually switching topic üê± or sentiment üòÉ).
This controlled LG method consists of plugging in simple bag-of-words or one-layer classifiers as attribute controllers, and making updates in the activation space, without changing any model parameters." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://mullikine.github.io/posts/controlled-text-generation/" />
<meta property="article:published_time" content="2019-12-06T00:00:00+13:00" />
<meta property="article:modified_time" content="2019-12-06T00:00:00+13:00" /><meta property="og:site_name" content="Bodacious Blog" />


  </head>
  <body>
    <header class="app-header">



<a href="https://mullikine.github.io"><img class="app-header-avatar" src="http://mullikine.github.io/fievel.png" alt="Shane Mulligan" /></a>
      <h1>Bodacious Blog</h1>
      
      <div class="app-header-social">
        
          <a target="_blank" href="https://github.com/mullikine"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-github">
  <title>github</title>
  <path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22"></path>
</svg></a>
        
          <a target="_blank" href="https://linkedin.com/in/shane-mulligan-811b942b/"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-linkedin">
  <title>linkedin</title>
  <path d="M16 8a6 6 0 0 1 6 6v7h-4v-7a2 2 0 0 0-2-2 2 2 0 0 0-2 2v7h-4v-7a6 6 0 0 1 6-6z"></path><rect x="2" y="9" width="4" height="12"></rect><circle cx="4" cy="4" r="2"></circle>
</svg></a>
        
          <a target="_blank" href="https://gitlab.com/mullikine"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-gitlab">
  <title>gitlab</title>
  <path d="M22.65 14.39L12 22.13 1.35 14.39a.84.84 0 0 1-.3-.94l1.22-3.78 2.44-7.51A.42.42 0 0 1 4.82 2a.43.43 0 0 1 .58 0 .42.42 0 0 1 .11.18l2.44 7.49h8.1l2.44-7.51A.42.42 0 0 1 18.6 2a.43.43 0 0 1 .58 0 .42.42 0 0 1 .11.18l2.44 7.51L23 13.45a.84.84 0 0 1-.35.94z"></path>
</svg></a>
        
          <a target="_blank" href="https://twitter.com/mullikine"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-twitter">
  <title>twitter</title>
  <path d="M23 3a10.9 10.9 0 0 1-3.14 1.53 4.48 4.48 0 0 0-7.86 3v1A10.66 10.66 0 0 1 3 4s-4 9 5 13a11.64 11.64 0 0 1-7 2c9 5 20 0 20-11.5a4.5 4.5 0 0 0-.08-.83A7.72 7.72 0 0 0 23 3z"></path>
</svg></a>
        
        <a href="https://mullikine.github.io/cv/">CV</a>
      </div>
<div class="highlights">
<a href="https://mullikine.gitlab.io/fortescue-blog-hugo/">‚òÖ Splunky Splog (FMG) ‚òÖ</a>
<br />
<a href="https://mullikine.github.io/posts/practical-macros-in-racket-and-how-to-work-with-them/">Practical macros in Racket</a>
<br />
<a href="https://mullikine.github.io/posts/github-search-with-bigquery/">Searching GitHub with BigQuery</a>

<br />
<a href="https://mullikine.github.io/posts/arbitrary-interpreters-for-babel/">Arbitrary interpreters for Babel</a>
<br />
<a href="https://mullikine.github.io/posts/review-of-the-illustrated-transformer/">The Illustrated Transformer</a>
<br />
<a href="https://mullikine.github.io/glossary.html">Glossary A-Z (70 topics)</a>
<br />
<a href="https://mullikine.github.io/codelingo-vs-linters/summary/">CodeLingo vs Linters</a>
<br />
<div class="taglist">

<a class="tag" href="https://mullikine.github.io/tags/gpt-2/">GPT-2</a>
<a class="tag" href="https://mullikine.github.io/tags/nlp/">NLP</a>
<a class="tag big" href="https://mullikine.github.io/tags/emacs/">emacs</a>
<a class="tag" href="https://mullikine.github.io/tags/elisp/">elisp</a>
<a class="tag" href="https://mullikine.github.io/tags/racket/">racket</a>
<a class="tag" href="https://mullikine.github.io/tags/haskell/">haskell</a>
<a class="tag" href="https://mullikine.github.io/tags/biosemiotics/">biosemiotics</a>
<a class="tag" href="https://mullikine.github.io/tags/docker/">docker</a>
<a class="tag" href="https://mullikine.github.io/tags/feature-engineering/">feature-engineering</a>
<a class="tag big" href="https://mullikine.github.io/tags/ir/">IR</a>
<a class="tag" href="https://mullikine.github.io/tags/games/">games</a>
<a class="tag" href="https://mullikine.github.io/tags/data/">data</a>
<a class="tag" href="https://mullikine.github.io/tags/info/">info theory</a>
<a class="tag" href="https://mullikine.github.io/tags/probability/">probability</a>
<a class="tag" href="https://mullikine.github.io/tags/problog/">problog</a>
<a class="tag big" href="https://mullikine.github.io/tags/bash/">bash</a>
<a class="tag" href="https://mullikine.github.io/tags/gcp/">GCP</a>
<a class="tag" href="https://mullikine.github.io/tags/github/">github</a>
<a class="tag" href="https://mullikine.github.io/tags/parsers/">parsers</a>
<a class="tag" href="https://mullikine.github.io/tags/rust/">rust</a>
<a class="tag" href="https://mullikine.github.io/tags/cpp/">c++</a>

<a class="tag" href="https://mullikine.github.io/tags/review/">review</a>
<a class="tag" href="https://mullikine.github.io/tags/kaggle/">kaggle</a>
<a class="tag" href="https://mullikine.github.io/tags/dl/">deep learning</a>
<a class="tag" href="https://mullikine.github.io/tags/dsls/">DSLs</a>
<a class="tag" href="https://mullikine.github.io/tags/df/">dwarf fortress</a>
<a class="tag" href="https://mullikine.github.io/tags/spacy/">spacy</a>
<a class="tag" href="https://mullikine.github.io/tags/latex/">latex</a>
<a class="tag" href="https://mullikine.github.io/tags/nix/">Nix</a>
<a class="tag" href="https://mullikine.github.io/tags/graphviz/">graphviz</a>
<a class="tag" href="https://mullikine.github.io/tags/python/">python</a>
<a class="tag" href="https://mullikine.github.io/tags/golang/">golang</a>
<a class="tag" href="https://mullikine.github.io/tags/codelingo/">codelingo</a>
<a class="tag" href="https://mullikine.github.io/tags/perl/">perl</a>
<a class="tag" href="https://mullikine.github.io/tags/math/">math</a>
<a class="tag big" href="https://mullikine.github.io/tags/vim/">vim</a>
<a class="tag" href="https://mullikine.github.io/tags/telco/">telco</a>
<a class="tag" href="https://mullikine.github.io/tags/automation/">automation</a>
<a class="tag" href="https://mullikine.github.io/tags/terminals/">terminals</a>
<a class="tag" href="https://mullikine.github.io/tags/transformer/">transformer</a>
<a class="tag big" href="https://mullikine.github.io/tags/codegen/">code-gen</a>
<a class="tag" href="https://mullikine.github.io/tags/optimisation/">optimisation</a>
<a class="tag" href="https://mullikine.github.io/tags/release/">release</a>
<a class="tag" href="https://mullikine.github.io/tags/dotnet/">.NET</a>
<a class="tag" href="https://mullikine.github.io/tags/csharp/">csharp</a>
<a class="tag" href="https://mullikine.github.io/tags/tooling/">tooling</a></div>
</div>
<div>
    </header>
    <main class="app-container">
      
  <article class="post">
    <header class="post-header">
      <h1 class ="post-title">Controlled Text Generation</h1>
      <div class="post-meta">
        <div>
          <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-calendar">
  <title>calendar</title>
  <rect x="3" y="4" width="18" height="18" rx="2" ry="2"></rect><line x1="16" y1="2" x2="16" y2="6"></line><line x1="8" y1="2" x2="8" y2="6"></line><line x1="3" y1="10" x2="21" y2="10"></line>
</svg>
          Dec 6, 2019
        </div>
        <div>
          <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-clock">
  <title>clock</title>
  <circle cx="12" cy="12" r="10"></circle><polyline points="12 6 12 12 16 14"></polyline>
</svg>
          5 min read
        </div><div>
          <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tag">
  <title>tag</title>
  <path d="M20.59 13.41l-7.17 7.17a2 2 0 0 1-2.83 0L2 12V2h10l8.59 8.59a2 2 0 0 1 0 2.82z"></path><line x1="7" y1="7" x2="7" y2="7"></line>
</svg>
          <a class="tag" href="https://mullikine.github.io/tags/nlp/">NLP</a><a class="tag" href="http://mullikine.github.io/tags/emacs/">emacs</a><a class="tag" href="http://mullikine.github.io/tags/huggingface/">huggingface</a></div></div>
    </header>
    

<link rel="stylesheet" type="text/css" href="https://mullikine.github.io/css/magit.css"/>

<script src="https://mullikine.github.io/js/mathjax-config.js"></script>
 
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_HTML"></script>


    <div class="post-content">
      

<dl>
<dt>Original article</dt>
<dd><a href="https://eng.uber.com/pplm/">https://eng.uber.com/pplm/</a> <br />
<a href="https://github.com/uber-research/PPLM">https://github.com/uber-research/PPLM</a> <br />
<a href="https://github.com/huggingface/transformers/blob/master/examples/pplm/README.md">https://github.com/huggingface/transformers/blob/master/examples/pplm/README.md</a> <br />
<a href="https://transformer.huggingface.co/model/pplm"> Uber AI Plug and Play Language Model</a></dd>
</dl>

<h2 id="controlling-text-generation-with-plug-and-play-language-model--pplm">Controlling Text Generation with Plug and Play Language Model (PPLM)</h2>

<p>PPLM builds on top of other large transformer-
based generative models (like GPT-2), where it
enables finer-grained control of attributes of
the generated language (e.g. gradually
switching topic üê± or sentiment üòÉ).</p>

<p>This controlled LG method consists of plugging
in simple bag-of-words or one-layer
classifiers as attribute controllers, and
making updates in the activation space,
without changing any model parameters.</p>

<p>Kindly implemented by the Uber AI team in
ü§ó/transformers.</p>

<h3 id="key-point">Key point</h3>

<p>What if we wanted the generated text to start
with the same prefix, The food is awful, but
then to turn in a positive direction? Or
gradually to change the topic of the generated
text to being about politics?</p>

<h3 id="proposals-up-till-now">Proposals up till now</h3>

<p>Researchers around the world have proposed
multiple ways of conditioning text generation,
including:</p>

<ul>
<li>starting with a pre-trained LM and fine-tuning it to always produce
positive sentences,</li>
<li>training a large conditional model from scratch, or</li>
<li>turning a given sentence into a more positive one by substituting new
text in for key n-grams.</li>
</ul>

<h3 id="proposal-pplm">Proposal; PPLM</h3>

<p>Flexibly plug in one or more simple attribute
models representing the desired control
objective into a large, unconditional LM.</p>

<p>The method has the key property that it uses
the LM as is‚Äîno training or fine-tuning is
required‚Äîwhich enables researchers to leverage
best-in-class LMs even if they do not have the
extensive hardware required to train them.</p>

<h2 id="attribute-model--steering-a-mammoth">Attribute model (steering a mammoth)</h2>

<p>Many attribute models used in PPLM are 100,000
times smaller than the LM, roughly the weight
ratio of a field mouse to a wooly mammoth.</p>

<p>The PPLM method is plug and play: it can
combine any generative neural LM (mammoth) and
any differentiable attribute model or models
(mouse) representing the desired steering
objective(s).</p>

<p>It is also resource efficient: the LM is used
as-is without training or updating any of its
weights (mammoths are hard to train, after
all).</p>

<h2 id="how-it-works">How it works</h2>

<table>
<thead>
<tr>
<th></th>
<th>name</th>
<th>description</th>
<th>modeled by</th>
</tr>
</thead>

<tbody>
<tr>
<td>p(x)</td>
<td>the unconditional LM</td>
<td>a probability distribution over all text</td>
<td>unconditional LMs like GPT-2</td>
</tr>

<tr>
<td>p(x<bar>a) =  p(x<bar>a) ‚àù p(a<bar>x) p(x)</td>
<td>the conditional LM (that we‚Äôd like to create)</td>
<td>It‚Äôs a hypothetical model that can, given an attribute a, generate sentences with those attributes (like the positive sentiment example above).</td>
<td></td>
</tr>

<tr>
<td>p(a<bar>x)</td>
<td>the attribute model</td>
<td>a sentence x and outputs the probability that it possesses the attribute a</td>
<td></td>
</tr>
</tbody>
</table>

<h3 id="p--x"><code>p(x)</code></h3>

<p>This is the distribution modeled by
unconditional LMs like GPT-2, our mammoth in
Figure 1, above.</p>

<p>It is general and can generate fluent text
about a broad array of topics.</p>

<h3 id="p--x-a"><code>p(x|a)</code></h3>

<p>The conditional LM that we‚Äôd like to create.</p>

<p>It‚Äôs a hypothetical model that can, given an
attribute a, generate sentences with those
attributes (like the positive sentiment
example above).</p>

<h3 id="p--a-x"><code>p(a|x)</code></h3>

<p>Third, there‚Äôs an attribute model <code>p(a|x)</code>,
which takes a sentence x and outputs the
probability that it possesses the attribute a.</p>

<p>This model might judge a sentence as being 10
percent likely to have positive sentiment or
of being 85 percent likely to be about
politics.</p>

<p>These models can be tiny and easy to train
because, intuitively, recognizing positivity
is easier than being positive and recognizing
political speech is easier than writing it;
this is particularly true when the recognition
is learned atop representations provided by a
pre-trained LM, as demonstrated in Radford et
al. (2019).</p>

<p>Attribute models with only a single layer
containing 4,000 parameters perform well at
recognizing attributes and guiding generation.</p>

<ul>
<li>The second model in terms of the first and the third:: <code>p(x|a) ‚àù p(a|x) p(x)</code></li>
</ul>

<h2 id="sampling">Sampling</h2>

<div class="highlight"><div style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">
<table style="border-spacing:0;padding:0;margin:0;border:0;width:auto;overflow:auto;display:block;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text"><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">15
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">16
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">17
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">18
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">19
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">20
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">21
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">22
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">23
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">24
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">25
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">26
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">27
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">28
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">29
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">30
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">rejection sampling
    A basic technique used to generate
    observations from a distribution.

    It is also commonly called the acceptance-
    rejection method or &#34;accept-reject
    algorithm&#34; and is a type of exact
    simulation method.

    Based on the observation that to sample a
    random variable in one dimension, one can
    perform a uniformly random sampling of the
    two-dimensional Cartesian graph, and keep
    the samples in the region under the graph
    of its density function.

importance sampling
    A general technique for estimating
    properties of a particular distribution,
    while only having samples generated from a
    different distribution than the
    distribution of interest.

    It is related to umbrella sampling in
    computational physics.

    Depending on the application, the term may
    refer to the process of sampling from this
    alternative distribution, the process of
    inference, or both.</code></pre></td></tr></table>
</div>
</div>

<h2 id="naive-approach">Naive approach</h2>

<p>Such a process to generate a sentence about
politics could loosely work like this:</p>

<ul>
<li><span class="underline"><strong>Step 1:</strong></span> generate a sentence x from <code>p(x)</code></li>
<li><span class="underline"><strong>Step 2:</strong></span> give it to =p(a|x) =to see if it‚Äôs about politics</li>
<li><span class="underline"><strong>Step 3:</strong></span> if it‚Äôs not, go to (1).</li>
</ul>

<p>Unfortunately, it may take exponentially long
to randomly generate a sentence about
politics, because there are so many other
topics to talk about, just as it would take
eons for monkeys randomly typing to produce
the works of Shakespeare, because there are
just so many other pages that can be typed.</p>

<h3 id="solution">Solution</h3>

<p>PPLM resolves this issue by approximately
implementing the more efficient <code>Metropolis-
adjusted Langevin sampler</code> of Roberts and
Tweedie (1996) as implemented for pairs of NNs
by Nguyen et al.</p>

<p>(2016) in their Plug-and-Play Generative
Networks (PPGN) model.</p>

<p>In this vein, the PPLM algorithm entails three
simple steps to generate a sample:</p>

<ul>
<li><span class="underline"><strong>Step 1:</strong></span> Given a partially generated sentence,
compute <code>log(p(x))</code> and <code>log(p(a|x))</code> and the gradients of each with respect to the hidden representation of the underlying language model.
These quantities are both available using an efficient forward and backward pass of both models.</li>
<li><span class="underline"><strong>Step 2:</strong></span> Use the gradients to move the hidden representation of the language model a small step in the
direction of increasing <code>log(p(a|x))</code> and increasing <code>log(p(x))</code>.</li>
<li><span class="underline"><strong>Step 3:</strong></span> Sample the next word.</li>
</ul>

<p>Intuitively, as a PPLM generates text one
token at a time, it continuously steers its
representation of the text in a direction that
will be more likely to possess the desired
attribute‚Äîhigh <code>log(p(a|x))</code> ‚Äî while still
retaining fluency under the original LM‚Äîhigh
<code>log(p(x))</code>.</p>

<h2 id="code">Code</h2>

<h3 id="generate-text-pplm"><code>generate_text_pplm</code></h3>

<div class="highlight"><div style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">
<table style="border-spacing:0;padding:0;margin:0;border:0;width:auto;overflow:auto;display:block;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sh" data-lang="sh"><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sh" data-lang="sh">cd <span style="color:#e6db74">&#34;</span>$MYGIT<span style="color:#e6db74">/uber-research/PPLM&#34;</span> ; $HOME/scripts/ead generate_text_pplm</code></pre></td></tr></table>
</div>
</div>

<h3 id="perturb"><code>perturb</code></h3>

<div class="highlight"><div style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">
<table style="border-spacing:0;padding:0;margin:0;border:0;width:auto;overflow:auto;display:block;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sh" data-lang="sh"><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sh" data-lang="sh">cd <span style="color:#e6db74">&#34;</span>$MYGIT<span style="color:#e6db74">/uber-research/PPLM&#34;</span> ; $HOME/scripts/ead perturb</code></pre></td></tr></table>
</div>
</div>

    </div>
    <div class="post-footer">
      
    </div>
  </article>

    </main>
  </body>
</html>