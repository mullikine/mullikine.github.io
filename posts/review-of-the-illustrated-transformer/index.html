<!doctype html>
<html lang="en-us">
  <head>
    <title>Overview of The Illustrated Transformer // Bodacious Blog</title>
    <meta charset="utf-8" />
    <meta name="generator" content="Hugo 0.59.0-DEV" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="author" content="John Doe" />
    <meta name="description" content="" />
    <link rel="stylesheet" href="https://mullikine.github.io/css/main.min.84fb5225eed3e6e0eb59b446d47c2e71f4c53566ff658936a2669534426ce0e6.css" />

    
    <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Overview of The Illustrated Transformer"/>
<meta name="twitter:description" content="Original article https://jalammar.github.io/illustrated-transformer/  Helpful glossary Multilayer perceptron Feed-Forward Neural Network FFNN Basically, these are multi-level logistic regression classifiers. Many layers of scales are separated by non-linearities. Can be used for as autoencoders. Can be used to train a classifier or extract functions as autoencoders. self-attention intra-attention [attention mechanism] Intuition: Reflects on its own position/context within a greater whole. Relates different positions of a single sequence in order to compute a representation of the sequence."/>

    <meta property="og:title" content="Overview of The Illustrated Transformer" />
<meta property="og:description" content="Original article https://jalammar.github.io/illustrated-transformer/  Helpful glossary Multilayer perceptron Feed-Forward Neural Network FFNN Basically, these are multi-level logistic regression classifiers. Many layers of scales are separated by non-linearities. Can be used for as autoencoders. Can be used to train a classifier or extract functions as autoencoders. self-attention intra-attention [attention mechanism] Intuition: Reflects on its own position/context within a greater whole. Relates different positions of a single sequence in order to compute a representation of the sequence." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://mullikine.github.io/posts/review-of-the-illustrated-transformer/" />
<meta property="article:published_time" content="2019-10-19T00:00:00+13:00" />
<meta property="article:modified_time" content="2019-10-19T00:00:00+13:00" /><meta property="og:site_name" content="Bodacious Blog" />


  </head>
  <body>
    <header class="app-header">
      <a href="https://mullikine.github.io"><img class="app-header-avatar" src="http://mullikine.github.io/brainy.png" alt="John Doe" /></a>
      <h1>Bodacious Blog</h1>
      <p>Biosemiotics üåª Xenolingustics üëΩ and emacs üêÑ</p>
      <div class="app-header-social">
        
          <a target="_blank" href="https://github.com/mullikine"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-github">
  <title>github</title>
  <path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22"></path>
</svg></a>
        
          <a target="_blank" href="https://linkedin.com/in/shane-mulligan-811b942b/"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-linkedin">
  <title>linkedin</title>
  <path d="M16 8a6 6 0 0 1 6 6v7h-4v-7a2 2 0 0 0-2-2 2 2 0 0 0-2 2v7h-4v-7a6 6 0 0 1 6-6z"></path><rect x="2" y="9" width="4" height="12"></rect><circle cx="4" cy="4" r="2"></circle>
</svg></a>
        
          <a target="_blank" href="https://gitlab.com/mullikine"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-gitlab">
  <title>gitlab</title>
  <path d="M22.65 14.39L12 22.13 1.35 14.39a.84.84 0 0 1-.3-.94l1.22-3.78 2.44-7.51A.42.42 0 0 1 4.82 2a.43.43 0 0 1 .58 0 .42.42 0 0 1 .11.18l2.44 7.49h8.1l2.44-7.51A.42.42 0 0 1 18.6 2a.43.43 0 0 1 .58 0 .42.42 0 0 1 .11.18l2.44 7.51L23 13.45a.84.84 0 0 1-.35.94z"></path>
</svg></a>
        
          <a target="_blank" href="https://www.facebook.com/shrubgrub"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-facebook">
  <title>facebook</title>
  <path d="M18 2h-3a5 5 0 0 0-5 5v3H7v4h3v8h4v-8h3l1-4h-4V7a1 1 0 0 1 1-1h3z"></path>
</svg></a>
        
          <a target="_blank" href="https://twitter.com/mullikine"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-twitter">
  <title>twitter</title>
  <path d="M23 3a10.9 10.9 0 0 1-3.14 1.53 4.48 4.48 0 0 0-7.86 3v1A10.66 10.66 0 0 1 3 4s-4 9 5 13a11.64 11.64 0 0 1-7 2c9 5 20 0 20-11.5a4.5 4.5 0 0 0-.08-.83A7.72 7.72 0 0 0 23 3z"></path>
</svg></a>
        
        <a href="https://mullikine.github.io/cv/">CV</a>
      </div>
<div class="highlights">
<a href="https://mullikine.github.io/posts/practical-macros-in-racket-and-how-to-work-with-them/">Practical macros in Racket</a>
<br />
<a href="https://mullikine.github.io/posts/github-search-with-bigquery/">Searching GitHub with BigQuery</a>
<br />
<a href="https://mullikine.github.io/posts/review-of-the-illustrated-transformer/">The Illustrated Transformer</a>
<br />
<a href="https://mullikine.github.io/codelingo-vs-linters/main/">CodeLingo vs Linters</a>
<br />
<a href="https://mullikine.github.io/glossary.html">Glossary A-Z (70 topics)</a>
<div>
    </header>
    <main class="app-container">
      
  <article class="post">
    <header class="post-header">
      <h1 class ="post-title">Overview of The Illustrated Transformer</h1>
      <div class="post-meta">
        <div>
          <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-calendar">
  <title>calendar</title>
  <rect x="3" y="4" width="18" height="18" rx="2" ry="2"></rect><line x1="16" y1="2" x2="16" y2="6"></line><line x1="8" y1="2" x2="8" y2="6"></line><line x1="3" y1="10" x2="21" y2="10"></line>
</svg>
          Oct 19, 2019
        </div>
        <div>
          <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-clock">
  <title>clock</title>
  <circle cx="12" cy="12" r="10"></circle><polyline points="12 6 12 12 16 14"></polyline>
</svg>
          7 min read
        </div></div>
    </header>
    

<link rel="stylesheet" type="text/css" href="https://mullikine.github.io/css/magit.css"/>

<script src="https://mullikine.github.io/js/mathjax-config.js"></script>
 
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_HTML"></script>


    <div class="post-content">
      

<dl>
<dt>Original article</dt>
<dd><a href="https://jalammar.github.io/illustrated-transformer/">https://jalammar.github.io/illustrated-transformer/</a></dd>
</dl>

<h2 id="helpful-glossary">Helpful glossary</h2>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">Multilayer perceptron
Feed-Forward Neural Network
FFNN
    Basically, these are multi-level logistic
    regression classifiers.

    Many layers of scales are separated by
    non-linearities.

    Can be used for as autoencoders.

    Can be used to train a classifier or
    extract functions as autoencoders.

self-attention
intra-attention
    [attention mechanism]

    Intuition:
        Reflects on its own position/context
        within a greater whole.

    Relates different positions of a single
    sequence in order to compute a
    representation of the sequence.

    An attention operation of a single
    sequence in order to calculate the
    representation of the very same sequence.

    This concept has been very useful in NLP
    tasks such as Text summarization, Machine
    Translation and Image Description
    Generation.</code></pre></div>
<h3 id="optional-reading">Optional reading</h3>

<table>
<thead>
<tr>
<th>Topic</th>
<th>URL</th>
</tr>
</thead>

<tbody>
<tr>
<td>Self-attention</td>
<td><a href="https://medium.com/saarthi-ai/transformers-attention-based-seq2seq-machine-translation-a28940aaa4fe">Why do Transformers yield Superior Sequence to Sequence(Seq2Seq)Results?</a></td>
</tr>
</tbody>
</table>

<h2 id="high-level-overview">High level overview</h2>

<p>In a machine translation application, it would
take a sentence in one language, and output
its translation in another.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-dot" data-lang="dot">input [label=&#34;foreign language&#34;]
output [label=&#34;your language&#34;]

subgraph transformer {
    decoders [label=&#34;encoders: stack of N decoders&#34;]
    encoders [label=&#34;encoders: stack of N encoders&#34;] -&gt; decoders
}

input -&gt; encoders [label=&#34;input (embed the training set)&#34;]
decoders -&gt; output [label=output]</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">  ‚àò‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚àò
  ‚îÉ        foreign language         ‚îÉ
  ‚àò‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚àò
    ‚îÉ
    ‚îÉ input (embed the training set)
    v
‚àò‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚àò
‚îÜ             transformer             ‚îÜ
‚îÜ                                     ‚îÜ
‚îÜ ‚àò‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚àò ‚îÜ
‚îÜ ‚îÉ  encoders: stack of N encoders  ‚îÉ ‚îÜ
‚îÜ ‚àò‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚àò ‚îÜ
‚îÜ   ‚îÉ                                 ‚îÜ
‚îÜ   ‚îÉ                                 ‚îÜ
‚îÜ   v                                 ‚îÜ
‚îÜ ‚àò‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚àò ‚îÜ
‚îÜ ‚îÉ  encoders: stack of N decoders  ‚îÉ ‚îÜ
‚îÜ ‚àò‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚àò ‚îÜ
‚îÜ                                     ‚îÜ
‚àò‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚àò
    ‚îÉ
    ‚îÉ output
    v
  ‚àò‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚àò
  ‚îÉ          your language          ‚îÉ
  ‚àò‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚àò</code></pre></div>
<h3 id="a-look-inside-the-encoder-stack">A look inside the <code>encoder</code> stack</h3>

<p>Sadly, <span class="underline">nested</span> subgraphs don&rsquo;t render in ASCII.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">#+BEGIN_SRC graphviz-dot :filter dot-digraph :async :results verbatim
  subgraph encoders {
      label = &#34;Parent&#34;;
      subgraph &#34;encoder1&#34; {
          f1[label=&#34;FFNN&#34;]
          a1[label=&#34;Self-Attention layer&#34;] -&gt; f1
      }
      subgraph &#34;encoder2&#34; {
          f2[label=&#34;FFNN&#34;]
          a2[label=&#34;Self-Attention layer&#34;] -&gt; f2
      }

      f1 -&gt; a2
  }
#+END_SRC</code></pre></div>
<p>Each stack contains <code>N</code> encoders.</p>

<dl>
<dt>list of 512D vectors</dt>
<dd>This is a list of words (initially, at least). It refers to either the input sentence or the output of an encoder.</dd>
</dl>

<p>Each encoder contains a <span class="underline">self-attention layer</span> and an <span class="underline">FFNN</span>:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-graphviz-dot" data-lang="graphviz-dot">subgraph Encoder1 {
    f1[label=&#34;FFNN&#34;]
    a1[label=&#34;Self-Attention layer&#34;] -&gt; f1
}
subgraph Encoder2 {
    f2[label=&#34;FFNN&#34;]
    a2[label=&#34;Self-Attention layer&#34;] -&gt; f2
}
subgraph EncoderN {
    etc [label=&#34;...&#34;]
}

f1 -&gt; a2 [label=&#34;list of 512D vectors&#34;]
f2 -&gt; etc [label=&#34;list of 512D vectors&#34;]</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">‚àò‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚àò
‚îÜ         Encoder1          ‚îÜ
‚îÜ                           ‚îÜ
‚îÜ ‚àò‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚àò ‚îÜ
‚îÜ ‚îÉ Self-Attention layer  ‚îÉ ‚îÜ
‚îÜ ‚àò‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚àò ‚îÜ
‚îÜ   ‚îÉ                       ‚îÜ
‚îÜ   ‚îÉ                       ‚îÜ
‚îÜ   v                       ‚îÜ
‚îÜ ‚àò‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚àò ‚îÜ
‚îÜ ‚îÉ         FFNN          ‚îÉ ‚îÜ
‚îÜ ‚àò‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚àò ‚îÜ
‚îÜ                           ‚îÜ
‚àò‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚àò
    ‚îÉ
    ‚îÉ list of 512D vectors
    v
‚àò‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚àò
‚îÜ         Encoder2          ‚îÜ
‚îÜ                           ‚îÜ
‚îÜ ‚àò‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚àò ‚îÜ
‚îÜ ‚îÉ Self-Attention layer  ‚îÉ ‚îÜ
‚îÜ ‚àò‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚àò ‚îÜ
‚îÜ   ‚îÉ                       ‚îÜ
‚îÜ   ‚îÉ                       ‚îÜ
‚îÜ   v                       ‚îÜ
‚îÜ ‚àò‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚àò ‚îÜ
‚îÜ ‚îÉ         FFNN          ‚îÉ ‚îÜ
‚îÜ ‚àò‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚àò ‚îÜ
‚îÜ                           ‚îÜ
‚àò‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚àò
    ‚îÉ
    ‚îÉ list of 512D vectors
    v
‚àò‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚àò
‚îÜ         EncoderN          ‚îÜ
‚îÜ                           ‚îÜ
‚îÜ ‚àò‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚àò ‚îÜ
‚îÜ ‚îÉ          ...          ‚îÉ ‚îÜ
‚îÜ ‚àò‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚àò ‚îÜ
‚îÜ                           ‚îÜ
‚àò‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚àò</code></pre></div>
<h4 id="explanation-of-the-encoder-above">Explanation of the encoder above</h4>

<p>The first encoder receives one sentence [at a time]
as input.</p>

<p>Subsequent encoders receive the output of the
previous encoder (which is of the same
dimensionality).</p>

<p>Each word of the sentence is embedded into a
vector of size 512 and the length of the
embedded sentence is the number of words of
the longest sentence in our training dataset √ó
512 (the size of a word).</p>

<p>A <span class="underline">self-attention layer</span> helps the encoder
look at other words in the input sentence as
it encodes a specific word.</p>

<p>The exact same <code>FFNN</code> is independently applied
to each position (i.e. each vector flows
through it separately).</p>

<dl>
<dt>My thoughts on this&hellip;</dt>
<dd>If you imagine an encoder is a
<code>CNN</code>, you can think of the self-attention layer
as being the sliding window but it has context of
the entire input text (the sentence), not merely the few words around it.
The <code>FFNN</code> takes the tensor of attention values as its input.</dd>
<dt>A key property of the transformer</dt>
<dd>The word in each position flows through its own path in the encoder.
There are dependencies between
these paths in the self-attention
layer. The feed-forward layer
does not have those dependencies,
however, and thus the various
paths can be executed in parallel
while flowing through the
feed-forward layer.</dd>
</dl>

<h3 id="a-look-inside-the-decoder-stack">A look inside the <code>decoder</code> stack</h3>

<p>Each decoder is the same as an <span class="underline">encoder</span> except with an <span class="underline">encoder-decoder attention</span> layer in between.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-graphviz-dot" data-lang="graphviz-dot">subgraph Decoder1 {
    f1[label=&#34;FFNN&#34;]
    eda1[label=&#34;Encoder-Decoder-Attention layer&#34;] -&gt; f1
    a1[label=&#34;Self-Attention layer&#34;] -&gt; eda1
}
subgraph Decoder2 {
    f2[label=&#34;FFNN&#34;]
    eda2[label=&#34;Encoder-Decoder-Attention layer&#34;] -&gt; f2
    a2[label=&#34;Self-Attention layer&#34;] -&gt; eda2
}
subgraph DecoderN {
    etc [label=&#34;...&#34;]
}

f1 -&gt; a2
f2 -&gt; etc [label=&#34;...&#34;]</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">‚àò‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚àò
‚îÜ              Decoder1               ‚îÜ
‚îÜ                                     ‚îÜ
‚îÜ ‚àò‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚àò ‚îÜ
‚îÜ ‚îÉ      Self-Attention layer       ‚îÉ ‚îÜ
‚îÜ ‚àò‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚àò ‚îÜ
‚îÜ   ‚îÉ                                 ‚îÜ
‚îÜ   ‚îÉ                                 ‚îÜ
‚îÜ   v                                 ‚îÜ
‚îÜ ‚àò‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚àò ‚îÜ
‚îÜ ‚îÉ Encoder-Decoder-Attention layer ‚îÉ ‚îÜ
‚îÜ ‚àò‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚àò ‚îÜ
‚îÜ   ‚îÉ                                 ‚îÜ
‚îÜ   ‚îÉ                                 ‚îÜ
‚îÜ   v                                 ‚îÜ
‚îÜ ‚àò‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚àò ‚îÜ
‚îÜ ‚îÉ              FFNN               ‚îÉ ‚îÜ
‚îÜ ‚àò‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚àò ‚îÜ
‚îÜ                                     ‚îÜ
‚àò‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚àò
    ‚îÉ
    ‚îÉ
    v
‚àò‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚àò
‚îÜ              Decoder2               ‚îÜ
‚îÜ                                     ‚îÜ
‚îÜ ‚àò‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚àò ‚îÜ
‚îÜ ‚îÉ      Self-Attention layer       ‚îÉ ‚îÜ
‚îÜ ‚àò‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚àò ‚îÜ
‚îÜ   ‚îÉ                                 ‚îÜ
‚îÜ   ‚îÉ                                 ‚îÜ
‚îÜ   v                                 ‚îÜ
‚îÜ ‚àò‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚àò ‚îÜ
‚îÜ ‚îÉ Encoder-Decoder-Attention layer ‚îÉ ‚îÜ
‚îÜ ‚àò‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚àò ‚îÜ
‚îÜ   ‚îÉ                                 ‚îÜ
‚îÜ   ‚îÉ                                 ‚îÜ
‚îÜ   v                                 ‚îÜ
‚îÜ ‚àò‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚àò ‚îÜ
‚îÜ ‚îÉ              FFNN               ‚îÉ ‚îÜ
‚îÜ ‚àò‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚àò ‚îÜ
‚îÜ                                     ‚îÜ
‚àò‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚àò
    ‚îÉ
    ‚îÉ ...
    v
‚àò‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚àò
‚îÜ              DecoderN               ‚îÜ
‚îÜ                                     ‚îÜ
‚îÜ ‚àò‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚àò ‚îÜ
‚îÜ ‚îÉ               ...               ‚îÉ ‚îÜ
‚îÜ ‚àò‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚àò ‚îÜ
‚îÜ                                     ‚îÜ
‚àò‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚îÑ‚àò</code></pre></div>
<h4 id="explanation-of-the-decoder-above">Explanation of the decoder above</h4>

<p>The <span class="underline">encoder-decoder attention layer</span> helps
the decoder focus on relevant parts of the
input sentence (similar what attention does in
<span class="underline">seq2seq</span> models).</p>

<dl>
<dt>My thoughts on this&hellip;</dt>
<dd>It seems as though the decoder&rsquo;s extra attention layer
learns a more macroscopic attention than the self-attention layer.</dd>
<dt>The intuition of self-attention&hellip;</dt>
<dd>Reflects on its own position/context within a greater whole.
                                   Each 512D row reflects on its own position/context
within the entire matrix.  If you‚Äôre familiar with RNNs, think of how maintaining a
hidden state allows an RNN to incorporate its representation of previous
words/vectors it has processed with the current one it‚Äôs processing. Self-attention
is the method the Transformer uses to bake the ‚Äúunderstanding‚Äù of other relevant
words into the one we‚Äôre currently processing.</dd>
</dl>

<h2 id="self-attention">Self-attention</h2>

<p>The <span class="underline">query</span>, <span class="underline">key</span>, and <span class="underline">value</span> vectors are
abstractions that are useful for calculating
and thinking about attention.</p>

<p>You create a \(q\), \(k\) and \(v\) vector for each word / 512D input vector.</p>

<h3 id="step-1-make--q-1----k-1--and--v-1--from--x-1----the-first-word-of-the-input-sentence">Step 1: Make \(q_1\), \(k_1\) and \(v_1\) from \(X_1\) (the first word of the input sentence)</h3>

<p>This is the path of one word of the input sentence through an encoder.</p>

<p>Firstly we need the weight matrices.</p>

<p>We trained these during the training process. When was that? This is explained later, maybe.</p>

<table>
<thead>
<tr>
<th>matrix</th>
<th>long name</th>
</tr>
</thead>

<tbody>
<tr>
<td>\(W^Q\)</td>
<td>WQ weight matrix</td>
</tr>

<tr>
<td>\(W^K\)</td>
<td>WK weight matrix</td>
</tr>

<tr>
<td>\(W^V\)</td>
<td>WV weight matrix</td>
</tr>
</tbody>
</table>

<p>Then we compute \(q\), \(k\) and \(v\) from the weight matrices.</p>

<table>
<thead>
<tr>
<th>vector</th>
<th>long name</th>
<th>size</th>
<th>created by</th>
<th>equation</th>
</tr>
</thead>

<tbody>
<tr>
<td>\(X_1\)</td>
<td>first word embedding</td>
<td>512 (input word)</td>
<td>embed a single word from input sentence</td>
<td></td>
</tr>

<tr>
<td>\(q_1\)</td>
<td><span class="underline">Query</span> vector</td>
<td>64 (&lt; the input word)</td>
<td>multiply embedding by weight</td>
<td>\(X_1\cdotp W^Q\)</td>
</tr>

<tr>
<td>\(k_1\)</td>
<td><span class="underline">Key</span> vector</td>
<td>64</td>
<td>multiply embedding by weight</td>
<td>\(X_1\cdotp W^K\)</td>
</tr>

<tr>
<td>\(v_1\)</td>
<td><span class="underline">Value</span> vector</td>
<td>64</td>
<td>multiply embedding by weight</td>
<td>\(X_1\cdotp W^V\)</td>
</tr>
</tbody>
</table>

<dl>
<dt><code>64</code></dt>
<dd>The <code>q,k,v</code> vectors don‚Äôt <strong>have</strong> to be smaller. This is an architecture choice to make the computation
of <code>multiheaded attention</code> (mostly) constant.</dd>
</dl>

<h3 id="step-2-calculate-a-score">Step 2: calculate a score</h3>

<p>We are calculating the <span class="underline">self-attention</span> for
the first word in the sentence.</p>

<p>We score each word of the input sentence
against this word.</p>

<p>The score determines how much focus to place
on otehr parts of the input sentence as we
encode a word at a certain position.</p>

<p>Take the dot product of the <span class="underline">query fector</span>
with the <span class="underline">key vector</span> of the respective word
we&rsquo;re scoring.</p>

    </div>
    <div class="post-footer">
      
    </div>
  </article>

    </main>
  </body>
</html>
