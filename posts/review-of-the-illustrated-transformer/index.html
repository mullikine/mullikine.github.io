<!doctype html>
<html lang="en-us">
  <head>
    <title>The Illustrated Transformer // Bodacious Blog</title>
    <meta charset="utf-8" />
    <meta name="generator" content="Hugo 0.71.0" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="author" content="Shane Mulligan" />
    <meta name="description" content="" />
    <link rel="stylesheet" href="https://mullikine.github.io/css/main.min.c93a97e54f16df99439e5c4acf79edb18fcb9e5384f1edc008a7a4639844b254.css" />

    
    <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="The Illustrated Transformer"/>
<meta name="twitter:description" content="Original article https://jalammar.github.io/illustrated-transformer/ Source https://github.com/tensorflow/tensor2tensor  Prereading https://jalammar.github.io/illustrated-word2vec/
Helpful glossary 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51  Multilayer perceptron Feed-Forward Neural Network FFNN Basically, these are multi-level logistic regression classifiers."/>

    <meta property="og:title" content="The Illustrated Transformer" />
<meta property="og:description" content="Original article https://jalammar.github.io/illustrated-transformer/ Source https://github.com/tensorflow/tensor2tensor  Prereading https://jalammar.github.io/illustrated-word2vec/
Helpful glossary 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51  Multilayer perceptron Feed-Forward Neural Network FFNN Basically, these are multi-level logistic regression classifiers." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://mullikine.github.io/posts/review-of-the-illustrated-transformer/" />
<meta property="article:published_time" content="2019-10-19T00:00:00+08:00" />
<meta property="article:modified_time" content="2019-10-19T00:00:00+08:00" /><meta property="og:site_name" content="Bodacious Blog" />


  </head>
  <body>
    <header class="app-header">

<a href="https://mullikine.github.io"><img class="app-header-avatar" src="http://mullikine.github.io/fievel.png" alt="Shane Mulligan" /></a>
      <h1>Bodacious Blog</h1>
      
      <div class="app-header-social">
        
          <a target="_blank" href="https://github.com/mullikine"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-github">
  <title>github</title>
  <path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22"></path>
</svg></a>
        
          <a target="_blank" href="https://linkedin.com/in/shane-mulligan-811b942b/"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-linkedin">
  <title>linkedin</title>
  <path d="M16 8a6 6 0 0 1 6 6v7h-4v-7a2 2 0 0 0-2-2 2 2 0 0 0-2 2v7h-4v-7a6 6 0 0 1 6-6z"></path><rect x="2" y="9" width="4" height="12"></rect><circle cx="4" cy="4" r="2"></circle>
</svg></a>
        
          <a target="_blank" href="https://gitlab.com/mullikine"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-gitlab">
  <title>gitlab</title>
  <path d="M22.65 14.39L12 22.13 1.35 14.39a.84.84 0 0 1-.3-.94l1.22-3.78 2.44-7.51A.42.42 0 0 1 4.82 2a.43.43 0 0 1 .58 0 .42.42 0 0 1 .11.18l2.44 7.49h8.1l2.44-7.51A.42.42 0 0 1 18.6 2a.43.43 0 0 1 .58 0 .42.42 0 0 1 .11.18l2.44 7.51L23 13.45a.84.84 0 0 1-.35.94z"></path>
</svg></a>
        
          <a target="_blank" href="https://twitter.com/mullikine"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-twitter">
  <title>twitter</title>
  <path d="M23 3a10.9 10.9 0 0 1-3.14 1.53 4.48 4.48 0 0 0-7.86 3v1A10.66 10.66 0 0 1 3 4s-4 9 5 13a11.64 11.64 0 0 1-7 2c9 5 20 0 20-11.5a4.5 4.5 0 0 0-.08-.83A7.72 7.72 0 0 0 23 3z"></path>
</svg></a>
        
        <a href="https://mullikine.github.io/cv/">CV</a>
      </div>
<div class="highlights">
<a href="https://mullikine.github.io/posts/practical-macros-in-racket-and-how-to-work-with-them/">Practical macros in Racket</a>
<br />
<a href="https://mullikine.github.io/posts/github-search-with-bigquery/">Searching GitHub with BigQuery</a>

<br />
<a href="https://mullikine.github.io/posts/arbitrary-interpreters-for-babel/">Arbitrary interpreters for Babel</a>
<br />
<a href="https://mullikine.github.io/posts/review-of-the-illustrated-transformer/">The Illustrated Transformer</a>
<br />
<a href="https://mullikine.github.io/glossary.html">Glossary A-Z (70 topics)</a>
<br />
<a href="https://mullikine.github.io/codelingo-vs-linters/summary/">CodeLingo vs Linters</a>
<br />
<div class="taglist">

<a class="tag" href="https://mullikine.github.io/tags/biosemiotics/">biosemiotics</a>
<a class="tag" href="https://mullikine.github.io/tags/xenolinguistics/">xenolinguistics</a>
<a class="tag big" href="https://mullikine.github.io/tags/emacs/">emacs</a>
<a class="tag" href="https://mullikine.github.io/tags/gpt/">GPT (Generative Pre-Training)</a>
<a class="tag" href="https://mullikine.github.io/tags/elisp/">elisp</a>
<a class="tag" href="https://mullikine.github.io/tags/racket/">racket</a>
<a class="tag" href="https://mullikine.github.io/tags/haskell/">haskell</a>
<a class="tag" href="https://mullikine.github.io/tags/nlp/">NLP</a>
<a class="tag" href="https://mullikine.github.io/tags/docker/">docker</a>
<a class="tag" href="https://mullikine.github.io/tags/feature-engineering/">feature-engineering</a>
<a class="tag big" href="https://mullikine.github.io/tags/ir/">IR</a>
<a class="tag" href="https://mullikine.github.io/tags/games/">games</a>
<a class="tag" href="https://mullikine.github.io/tags/data/">data</a>
<a class="tag" href="https://mullikine.github.io/tags/info/">info theory</a>
<a class="tag" href="https://mullikine.github.io/tags/probability/">probability</a>
<a class="tag" href="https://mullikine.github.io/tags/problog/">problog</a>
<a class="tag big" href="https://mullikine.github.io/tags/shell/">shell</a>
<a class="tag" href="https://mullikine.github.io/tags/gcp/">GCP</a>
<a class="tag big" href="https://mullikine.github.io/tags/github/">GitHub</a>
<a class="tag" href="https://mullikine.github.io/tags/parsers/">parsers</a>
<a class="tag" href="https://mullikine.github.io/tags/rust/">rust</a>
<a class="tag" href="https://mullikine.github.io/tags/cpp/">c++</a>
<a class="tag" href="https://mullikine.github.io/tags/review/">review</a>
<a class="tag" href="https://mullikine.github.io/tags/kaggle/">kaggle</a>
<a class="tag" href="https://mullikine.github.io/tags/dl/">deep learning</a>
<a class="tag" href="https://mullikine.github.io/tags/dsl/">DSL</a>
<a class="tag" href="https://mullikine.github.io/tags/df/">dwarf fortress</a>
<a class="tag" href="https://mullikine.github.io/tags/spacy/">spacy</a>
<a class="tag" href="https://mullikine.github.io/tags/latex/">latex</a>
<a class="tag" href="https://mullikine.github.io/tags/nix/">Nix</a>
<a class="tag" href="https://mullikine.github.io/tags/diagrams/">diagrams</a>
<a class="tag" href="https://mullikine.github.io/tags/python/">python</a>
<a class="tag" href="https://mullikine.github.io/tags/golang/">golang</a>
<a class="tag" href="https://mullikine.github.io/tags/codelingo/">codelingo</a>
<a class="tag" href="https://mullikine.github.io/tags/perl/">perl</a>
<a class="tag big" href="https://mullikine.github.io/tags/vim/">vim</a>
<a class="tag" href="https://mullikine.github.io/tags/telco/">telco</a>
<a class="tag" href="https://mullikine.github.io/tags/automation/">automation</a>
<a class="tag" href="https://mullikine.github.io/tags/terminals/">terminals</a>
<a class="tag" href="https://mullikine.github.io/tags/transformer/">transformer</a>
<a class="tag big" href="https://mullikine.github.io/tags/codegen/">code-gen</a>
<a class="tag" href="https://mullikine.github.io/tags/optimisation/">optimisation</a>
<a class="tag" href="https://mullikine.github.io/tags/release/">release</a>
<a class="tag" href="https://mullikine.github.io/tags/dotnet/">.NET</a>
<a class="tag" href="https://mullikine.github.io/tags/csharp/">csharp</a>
<a class="tag" href="https://mullikine.github.io/tags/tooling/">tooling</a>
<a class="tag" href="https://mullikine.github.io/tags/iac/">IaC</a>
<a class="tag big" href="https://mullikine.github.io/tags/facebook/">Facebook</a>
<a class="tag" href="https://mullikine.github.io/tags/wfh/">WFH</a>
<a class="tag big" href="https://mullikine.github.io/tags/rocketlab/">Rocket Lab</a>
<a class="tag" href="https://mullikine.github.io/tags/prayer/">prayer</a>
<a class="tag" href="https://mullikine.github.io/tags/babel/">babel</a>
<a class="tag big" href="https://mullikine.github.io/tags/uber/">Uber</a>
<a class="tag" href="https://mullikine.github.io/tags/math/">math</a>
<a class="tag" href="https://mullikine.github.io/tags/microsoft/">Microsoft</a>
</div>
</div>
<div class="highlights">
<a href="https://mullikine.github.io/posts/rat/">Automating rat</a>
<br />
<a href="https://mullikine.github.io/posts/dwarf-fortress-macros-with-emacs-and-tmux/">Automating Dwarf Fortress</a>
<br />
</div>


</div>
<div>
    </header>
    <main class="app-container">
      
  <article class="post">
    <header class="post-header">
      <h1 class ="post-title">The Illustrated Transformer</h1>
      <div class="post-meta">
        <div>
          <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-calendar">
  <title>calendar</title>
  <rect x="3" y="4" width="18" height="18" rx="2" ry="2"></rect><line x1="16" y1="2" x2="16" y2="6"></line><line x1="8" y1="2" x2="8" y2="6"></line><line x1="3" y1="10" x2="21" y2="10"></line>
</svg>
          Oct 19, 2019
        </div>
        <div>
          <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-clock">
  <title>clock</title>
  <circle cx="12" cy="12" r="10"></circle><polyline points="12 6 12 12 16 14"></polyline>
</svg>
          9 min read
        </div><div>
          <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tag">
  <title>tag</title>
  <path d="M20.59 13.41l-7.17 7.17a2 2 0 0 1-2.83 0L2 12V2h10l8.59 8.59a2 2 0 0 1 0 2.82z"></path><line x1="7" y1="7" x2="7" y2="7"></line>
</svg>
          <a class="tag" href="https://mullikine.github.io/tags/gpt/">gpt</a><a class="tag" href="http://mullikine.github.io/tags/dl/">DL</a><a class="tag" href="http://mullikine.github.io/tags/transformer/">transformer</a><a class="tag" href="http://mullikine.github.io/tags/attention/">attention</a><a class="tag" href="http://mullikine.github.io/tags/nlp/">NLP</a><a class="tag" href="http://mullikine.github.io/tags/review/">review</a><a class="tag" href="http://mullikine.github.io/tags/tf/">tf</a><a class="tag" href="http://mullikine.github.io/tags/diagrams/">diagrams</a></div></div>
    </header>
    

<link rel="stylesheet" type="text/css" href="https://mullikine.github.io/css/magit.css"/>

<script src="https://mullikine.github.io/js/mathjax-config.js"></script>
 
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_HTML"></script>


    <div class="post-content">
      <dl>
<dt>Original article</dt>
<dd><a href="https://jalammar.github.io/illustrated-transformer/">https://jalammar.github.io/illustrated-transformer/</a></dd>
<dt>Source</dt>
<dd><a href="https://github.com/tensorflow/tensor2tensor">https://github.com/tensorflow/tensor2tensor</a></dd>
</dl>
<h2 id="prereading">Prereading</h2>
<p><a href="https://jalammar.github.io/illustrated-word2vec/">https://jalammar.github.io/illustrated-word2vec/</a></p>
<h2 id="helpful-glossary">Helpful glossary</h2>
<div class="highlight"><div style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">
<table style="border-spacing:0;padding:0;margin:0;border:0;width:auto;overflow:auto;display:block;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">15
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">16
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">17
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">18
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">19
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">20
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">21
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">22
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">23
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">24
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">25
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">26
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">27
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">28
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">29
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">30
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">31
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">32
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">33
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">34
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">35
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">36
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">37
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">38
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">39
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">40
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">41
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">42
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">43
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">44
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">45
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">46
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">47
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">48
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">49
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">50
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">51
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">Multilayer perceptron
Feed-Forward Neural Network
FFNN
    Basically, these are multi-level logistic
    regression classifiers.

    Many layers of scales are separated by
    non-linearities.

    Can be used for as autoencoders.

    Can be used to train a classifier or
    extract functions as autoencoders.

self-attention
intra-attention
    [attention mechanism]

    Intuition:
        Reflects on its own position/context
        within a greater whole.

    Relates different positions of a single
    sequence in order to compute a
    representation of the sequence.

    An attention operation of a single
    sequence in order to calculate the
    representation of the very same sequence.

    This concept has been very useful in NLP
    tasks such as Text summarization, Machine
    Translation and Image Description
    Generation.

    The method the Transformer uses to bake
    the “understanding” of other relevant
    words into the one we’re currently
    processing.

dot product
    [#math]
    [algebraic operation]

    What is dot product used for?

    The original motivation is a geometric
    one:
        The dot product can be used for
        computing the angle α between two
        vectors a and b: a⋅b=|a|⋅|b|⋅cos(α).</code></pre></td></tr></table>
</div>
</div>
<h3 id="optional-reading">Optional reading</h3>
<table>
<thead>
<tr>
<th>Topic</th>
<th>URL</th>
</tr>
</thead>
<tbody>
<tr>
<td>Self-attention</td>
<td><a href="https://medium.com/saarthi-ai/transformers-attention-based-seq2seq-machine-translation-a28940aaa4fe">Why do Transformers yield Superior Sequence to Sequence(Seq2Seq)Results?</a></td>
</tr>
</tbody>
</table>
<h2 id="high-level-overview">High level overview</h2>
<p>In a machine translation application, it would
take a sentence in one language, and output
its translation in another.</p>
<div class="highlight"><div style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">
<table style="border-spacing:0;padding:0;margin:0;border:0;width:auto;overflow:auto;display:block;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fallback" data-lang="fallback">input [label=&#34;foreign language&#34;]
output [label=&#34;your language&#34;]

subgraph transformer {
    decoders [label=&#34;decoders: stack of N decoders&#34;]
    encoders [label=&#34;encoders: stack of N encoders&#34;] -&gt; decoders
}

input -&gt; encoders [label=&#34;input (embed the training set)&#34;]
decoders -&gt; output [label=output]</code></pre></td></tr></table>
</div>
</div>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">  ∘━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━∘
  ┃        foreign language         ┃
  ∘━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━∘
    ┃
    ┃ input (embed the training set)
    v
∘┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄∘
┆             transformer             ┆
┆                                     ┆
┆ ∘━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━∘ ┆
┆ ┃  encoders: stack of N encoders  ┃ ┆
┆ ∘━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━∘ ┆
┆   ┃                                 ┆
┆   ┃                                 ┆
┆   v                                 ┆
┆ ∘━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━∘ ┆
┆ ┃  decoders: stack of N decoders  ┃ ┆
┆ ∘━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━∘ ┆
┆                                     ┆
∘┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄∘
    ┃
    ┃ output
    v
  ∘━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━∘
  ┃          your language          ┃
  ∘━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━∘
</code></pre></div><h4 id="flowchart">Flowchart</h4>
<figure>
    <img src="https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/img/transform20fps.gif"/> 
</figure>

<h3 id="a-look-inside-the-encoder-stack">A look inside the <code>encoder</code> stack</h3>
<p>Each stack contains <code>N</code> encoders.</p>
<dl>
<dt>list of 512D vectors</dt>
<dd>This is a list of words (initially, at least). It refers to either the input sentence or the output of an encoder.</dd>
</dl>
<p>Each encoder contains a <span class="underline">self-attention layer</span> and an <span class="underline">FFNN</span>:</p>
<div class="highlight"><div style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">
<table style="border-spacing:0;padding:0;margin:0;border:0;width:auto;overflow:auto;display:block;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">15
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">16
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">17
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">18
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">19
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">20
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">21
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">22
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">23
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">24
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">25
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">26
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">27
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">28
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">29
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fallback" data-lang="fallback">subgraph clusterEncoders {
    style = filled
    fillcolor = lightgrey
    node [style=filled,fillcolor=lightgrey,shape=circle];

    label = &#34;Set of encoders&#34;
    subgraph clusterEncoder1 {
        fillcolor = white
        label = &#34;Encoder 1&#34;;
        f1[label=&#34;FFNN&#34;]
        a1[label=&#34;Self-Attention layer&#34;]
        a1 -&gt; f1
    }
    subgraph clusterEncoder2 {
        fillcolor = white
        label = &#34;Encoder 2&#34;;
        f2[label=&#34;FFNN&#34;]
        a2[label=&#34;Self-Attention layer&#34;]
        a2 -&gt; f2
    }
    subgraph clusterEncoderN {
        fillcolor = white
        label = &#34;Encoder N&#34;;
        etc [label=&#34;...&#34;]
    }

    f1 -&gt; a2 [label=&#34;list of 512D vectors&#34;]
    f2 -&gt; etc [label=&#34;list of 512D vectors&#34;]
}</code></pre></td></tr></table>
</div>
</div>
<figure>
    <img src="https://mullikine.github.io/ox-hugo/self-attention-layer.png"/> 
</figure>

<h4 id="explanation-of-the-encoder-above">Explanation of the encoder above</h4>
<p>The first encoder receives one sentence [at a time]
as input.</p>
<p>Subsequent encoders receive the output of the
previous encoder (which is of the same
dimensionality).</p>
<p>Each word of the sentence is embedded into a
vector of size <code>512</code> and the length of the
embedded sentence is <span class="underline">the number of words in
the longest sentence</span> of our training dataset ×
<code>512</code>, the size of a word.</p>
<p>A <span class="underline">self-attention layer</span> helps the encoder
look at other words in the input sentence as
it encodes a specific word.</p>
<p>The exact same <code>FFNN</code> is independently applied
to each position (i.e. each vector flows
through it separately).</p>
<dl>
<dt>My thoughts on this&hellip;</dt>
<dd>If you imagine an encoder is a
<code>CNN</code>, you can think of the self-attention layer
as being the sliding window but it has context of
the entire input text (the sentence), not merely the few words around it.
The <code>FFNN</code> takes the tensor of attention values as its input.</dd>
<dt>A key property of the transformer</dt>
<dd>The word in each position flows through its own path in the encoder.
There are dependencies between
these paths in the self-attention
layer. The feed-forward layer
does not have those dependencies,
however, and thus the various
paths can be executed in parallel
while flowing through the
feed-forward layer.</dd>
</dl>
<h3 id="a-look-inside-the-decoder-stack">A look inside the <code>decoder</code> stack</h3>
<p>Each decoder is the same as an <span class="underline">encoder</span> except with an <span class="underline">encoder-decoder attention</span> layer in between.</p>
<div class="highlight"><div style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">
<table style="border-spacing:0;padding:0;margin:0;border:0;width:auto;overflow:auto;display:block;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">15
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">16
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">17
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">18
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-org" data-lang="org"><span style="color:#75715e">#+BEGIN_SRC </span><span style="color:#75715e">graphviz-dot</span><span style="color:#75715e"> -n :filter dot-digraph-ascii-lr :async :results verbatim code
</span><span style="color:#75715e"></span>  subgraph Decoder1 {
      f1[label=&#34;FFNN&#34;]
      eda1[label=&#34;Encoder-Decoder-Attention layer&#34;] -&gt; f1
      a1[label=&#34;Self-Attention layer&#34;] -&gt; eda1
  }
  subgraph Decoder2 {
      f2[label=&#34;FFNN&#34;]
      eda2[label=&#34;Encoder-Decoder-Attention layer&#34;] -&gt; f2
      a2[label=&#34;Self-Attention layer&#34;] -&gt; eda2
  }
  subgraph DecoderN {
      etc [label=&#34;...&#34;]
  }

  f1 -&gt; a2
  f2 -&gt; etc [label=&#34;...&#34;]
<span style="color:#75715e">#+END_SRC</span></code></pre></td></tr></table>
</div>
</div>
<div class="highlight"><div style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">
<table style="border-spacing:0;padding:0;margin:0;border:0;width:auto;overflow:auto;display:block;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">4
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">5
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">6
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">7
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">8
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fallback" data-lang="fallback">∘┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄∘     ∘ ┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄∘        ∘┄┄┄┄┄┄┄┄┄┄┄┄∘
┆                                           Decoder1                            ┆     ┆                                           Decoder2                            ┆        ┆  DecoderN  ┆
┆                                                                               ┆     ┆                                                                               ┆        ┆            ┆
┆ ∘━━━━━━━━━━━━━━━━━━━━━━∘     ∘━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━∘     ∘━━━━━━∘ ┆     ┆ ∘━━━━━━━━━━━━━━━━━━━━━━∘     ∘━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━∘     ∘━━━━━━∘ ┆  ...   ┆ ∘━━━━━━━━∘ ┆
┆ ┃ Self-Attention layer ┃ ━━&gt; ┃ Encoder-Decoder-Attention layer ┃ ━━&gt; ┃ FFNN ┃ ┆ ━━&gt; ┆ ┃ Self-Attention layer ┃ ━━&gt; ┃ Encoder-Decoder-Attention layer ┃ ━━&gt; ┃ FFNN ┃ ┆ ━━━━-&gt; ┆ ┃  ...   ┃ ┆
┆ ∘━━━━━━━━━━━━━━━━━━━━━━∘     ∘━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━∘     ∘━━━━━━∘ ┆     ┆ ∘━━━━━━━━━━━━━━━━━━━━━━∘     ∘━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━∘     ∘━━━━━━∘ ┆        ┆ ∘━━━━━━━━∘ ┆
┆                                                                               ┆     ┆                                                                               ┆        ┆            ┆
∘┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄∘     ∘ ┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄∘        ∘┄┄┄┄┄┄┄┄┄┄┄┄∘</code></pre></td></tr></table>
</div>
</div>
<h4 id="explanation-of-the-decoder-above">Explanation of the decoder above</h4>
<p>The <span class="underline">encoder-decoder attention layer</span> helps
the decoder focus on relevant parts of the
input sentence (similar what attention does in
<span class="underline">seq2seq</span> models).</p>
<dl>
<dt>My thoughts on this&hellip;</dt>
<dd>It seems as though the decoder&rsquo;s extra attention layer
learns a more macroscopic attention than the self-attention layer.</dd>
<dt>The intuition of self-attention&hellip;</dt>
<dd>Reflects on its own position/context within a greater whole.
Each \(512D\) row reflects on its own position/context
within the entire matrix.  If you’re familiar with RNNs, think of how maintaining a
hidden state allows an RNN to incorporate its representation of previous
words/vectors it has processed with the current one it’s processing. Self-attention
is the method the Transformer uses to bake the “understanding” of other relevant
words into the one we’re currently processing.</dd>
</dl>
<h2 id="self-attention-in-detail">Self-attention in detail</h2>
<p>The <span class="underline">query</span>, <span class="underline">key</span>, and <span class="underline">value</span> vectors are
abstractions that are useful for calculating
and thinking about attention.</p>
<p>The self-attention must be calculated for each word.</p>
<h3 id="step-1-create-a--q----k--and--v--vector-for-each-word--512d--input-vector-dot">Step 1: Create a \(q\), \(k\) and \(v\) vector for each word / \(512D\) input vector.</h3>
<div class="highlight"><div style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">
<table style="border-spacing:0;padding:0;margin:0;border:0;width:auto;overflow:auto;display:block;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fallback" data-lang="fallback">&#34;the first word of the input sentence&#34; -&gt; &#34;the first vector of the input list&#34; [label=&#34;is analogous to&#34;]</code></pre></td></tr></table>
</div>
</div>
<div class="highlight"><div style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">
<table style="border-spacing:0;padding:0;margin:0;border:0;width:auto;overflow:auto;display:block;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">4
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">5
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">6
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">7
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">8
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">9
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fallback" data-lang="fallback">∘━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━∘
┃ the first word of the input sentence ┃
∘━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━∘
  ┃
  ┃ is analogous to
  v
∘━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━∘
┃  the first vector of the input list  ┃
∘━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━∘</code></pre></td></tr></table>
</div>
</div>
<table>
<thead>
<tr>
<th>relations</th>
</tr>
</thead>
<tbody>
<tr>
<td>word ↦ vector ↦ word embedding</td>
</tr>
<tr>
<td>sentence ↦ input of encoder ↦ list of vectors ↦ list of embeddings</td>
</tr>
</tbody>
</table>
<p>This is the path of one word of the input sentence through an encoder.</p>
<p>Firstly we need the weight matrices.</p>
<p>We trained these during the training process. When was that? This is explained later, maybe.</p>
<table>
<thead>
<tr>
<th>matrix</th>
<th>long name</th>
</tr>
</thead>
<tbody>
<tr>
<td>\(W^Q\)</td>
<td>WQ weight matrix</td>
</tr>
<tr>
<td>\(W^K\)</td>
<td>WK weight matrix</td>
</tr>
<tr>
<td>\(W^V\)</td>
<td>WV weight matrix</td>
</tr>
</tbody>
</table>
<p>Then we compute \(q\), \(k\) and \(v\) from the weight matrices.</p>
<h4 id="e-dot-g-dot-make--q-1----k-1--and--v-1--from--x-1----the-first-word-of-the-input-sentence">e.g. make \(q_1\), \(k_1\) and \(v_1\) from \(X_1\) (the first word of the input sentence)</h4>
<table>
<thead>
<tr>
<th>vector</th>
<th>long name</th>
<th>size</th>
<th>created by</th>
<th>equation</th>
</tr>
</thead>
<tbody>
<tr>
<td>\(X_1\)</td>
<td>first word embedding</td>
<td>512 (input word)</td>
<td>embed a single word from input sentence</td>
<td></td>
</tr>
<tr>
<td>\(q_1\)</td>
<td><span class="underline">Query</span> vector</td>
<td>64 (&lt; the input word)</td>
<td>multiply embedding by weight</td>
<td>\(X_1\cdotp W^Q\)</td>
</tr>
<tr>
<td>\(k_1\)</td>
<td><span class="underline">Key</span> vector</td>
<td>64</td>
<td>multiply embedding by weight</td>
<td>\(X_1\cdotp W^K\)</td>
</tr>
<tr>
<td>\(v_1\)</td>
<td><span class="underline">Value</span> vector</td>
<td>64</td>
<td>multiply embedding by weight</td>
<td>\(X_1\cdotp W^V\)</td>
</tr>
</tbody>
</table>
<dl>
<dt><code>64</code></dt>
<dd>The <code>q,k,v</code> vectors don’t <strong>have</strong> to be smaller. This is an architecture choice to make the computation
of <code>multiheaded attention</code> (mostly) constant.</dd>
</dl>
<h3 id="step-2-calculate--n-times-n---scores--n--for-each-word-as-we-compare-each-word-with-every-other-word">Step 2: calculate (\(n\times n\)) scores, \(n\) for each word as we compare each word with every other word</h3>
<dl>
<dt>The score</dt>
<dd>determines how much focus to place
on other parts of the input sentence as we
encode a word at a certain position.</dd>
</dl>
<p>Take the dot product of the <span class="underline">query vector</span>
with the <span class="underline">key vector</span> of the respective word
we&rsquo;re scoring.</p>
<p>\(\mathit{score}_\mathit{i,j} = q_i \cdot k_j\)</p>
<h4 id="e-dot-g-dot-calculate-the-for-the-1st-word-in-the-sentence">e.g. calculate the <span class="underline">self-attention</span> for the 1st word in the sentence</h4>
<p>Score each word of the input sentence against
the 1st word.</p>
<p>If we’re processing the self-attention for the
word in position <code>#1</code>, the <span class="underline"><strong>1st</strong></span> score would be
the dot product of \(q_1\) and \(k_1\).</p>
<p>The <span class="underline"><strong>2nd</strong></span> score would be the dot product of \(q_1\)
and \(k_2\).</p>
<h3 id="step-3-divide-the-scores-by-8">Step 3: divide the scores by 8</h3>
<p><code>8</code> is obtained by taking the square root of
the dimension of the key vectors used in the
paper.</p>
<p>See <code>64</code> earlier in this document.</p>
<p>Dividing the scores leads to more stable gradients.</p>
<h3 id="step-4-pass-the-scores-through-softmax">Step 4: pass the scores through softmax</h3>
<p>Softmax normalizes the scores so they’re all
positive and add up to 1.</p>
<dl>
<dt>softmax score</dt>
<dd>determines how much how much
each word will be expressed at this position.</dd>
</dl>
<h3 id="step-5-multiply-each-value-vector-by-the-softmax-score">Step 5: multiply each value vector by the softmax score</h3>
<p>This is in preparation to sum them up.</p>
<dl>
<dt>the intuition</dt>
<dd>To keep intact the values of the word or words
we want to focus on, and drown-out irrelevant
words (by multiplying them by tiny numbers
like 0.001, for example).</dd>
</dl>
<h3 id="step-6-sum-up-the-weighted-value-vectors">Step 6: sum up the weighted value vectors</h3>
<p>This produces the output of the self-attention
layer at this position (for the 1st word).</p>
<h3 id="in-conclusion-of-explaining-the-self-attention-calculation">In conclusion of explaining the self-attention calculation</h3>
<div class="highlight"><div style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">
<table style="border-spacing:0;padding:0;margin:0;border:0;width:auto;overflow:auto;display:block;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">15
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-latex" data-lang="latex"><span style="color:#66d9ef">\Function</span>{Self-attention}{<span style="color:#e6db74">$</span>{word}_i<span style="color:#e6db74">$</span>}
<span style="color:#66d9ef">\State</span> <span style="color:#e6db74">$</span>X_i\gets embed<span style="color:#f92672">(</span>{word}_i<span style="color:#f92672">)</span><span style="color:#e6db74">$</span>
<span style="color:#66d9ef">\State</span> <span style="color:#e6db74">$</span>q_i\gets X_i\cdotp W^Q<span style="color:#e6db74">$</span>
<span style="color:#66d9ef">\State</span> <span style="color:#e6db74">$</span>k_i\gets X_i\cdotp W^K<span style="color:#e6db74">$</span>
<span style="color:#66d9ef">\State</span> <span style="color:#e6db74">$</span>v_i\gets X_i\cdotp W^V<span style="color:#e6db74">$</span>
<span style="color:#66d9ef">\For</span>{<span style="color:#e6db74">$</span>\textrm{every other word }j<span style="color:#e6db74">$</span>}
<span style="color:#66d9ef">\State</span> <span style="color:#e6db74">$</span>\mathit{score}_\mathit{i,j}\gets q_i \cdot k_j<span style="color:#e6db74">$</span>
<span style="color:#66d9ef">\EndFor</span>
<span style="color:#66d9ef">\EndFunction</span>
<span style="color:#66d9ef">\newline</span>
<span style="color:#66d9ef">\Function</span>{Self-attention-layer}{<span style="color:#e6db74">$</span>sentence<span style="color:#e6db74">$</span>}
<span style="color:#66d9ef">\For</span>{<span style="color:#e6db74">$</span>\textrm{each } word_i \textrm{ in } sentence<span style="color:#e6db74">$</span>}
<span style="color:#66d9ef">\State</span> <span style="color:#e6db74">$</span>\mathit{self\char`_attention}_\mathit{i}\gets \verb|Self<span style="color:#f92672">-</span>attention|<span style="color:#f92672">(</span>{word}_i<span style="color:#f92672">)</span><span style="color:#e6db74">$</span>
<span style="color:#66d9ef">\EndFor</span>
<span style="color:#66d9ef">\EndFunction</span></code></pre></td></tr></table>
</div>
</div>
<figure>
    <img src="https://mullikine.github.io/ox-hugo/latex-self-attention.svg"/> 
</figure>

<h2 id="calculating-self-attention-with-matrices">Calculating self-attention with matrices</h2>
<h3 id="first-step-calculate--q----k--and--v">First step - calculate \(Q\), \(K\) and \(V\)</h3>
<p>The 1st step is to calculate the Query, Key,
and Value matrices.</p>
<p>We do that by packing our embeddings into a
matrix \(X\), and multiplying it by the weight
matrices we’ve trained (\(W^Q\), \(W^K\), \(W^V\)).</p>
<figure>
    <img src="https://jalammar.github.io/images/t/self-attention-matrix-calculation.png"/> 
</figure>


    </div>
    <div class="post-footer">
        <p>
        <a href="https://mullikine.github.io/about/">About this weblog</a>
        &nbsp;
        ⚔
        &nbsp;
        <a href="https://asciinema.org/~mullikine">Asciinema recordings</a>
        &nbsp;
        ⚔
        &nbsp;
        <a href="https://mullikine.github.io/posts/blogs-and-vlogs/">Blogs and Vlogs</a>
        &nbsp;
        ⚔
        &nbsp;
        <a href="https://mullikine.github.io/tags/">Site Map</a>
        </p>
    </div>
  </article>

    </main>
  </body>
</html>