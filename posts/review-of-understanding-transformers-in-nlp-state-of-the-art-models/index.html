<!doctype html>
<html lang="en-us">
  <head>
    <title>Rewrite of &#39;Understanding Transformers in NLP: State-of-the-Art Models&#39; // Bodacious Blog</title>
    <meta charset="utf-8" />
    <meta name="generator" content="Hugo 0.71.0" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="author" content="Shane Mulligan" />
    <meta name="description" content="" />
    <link rel="stylesheet" href="https://mullikine.github.io/css/main.min.c93a97e54f16df99439e5c4acf79edb18fcb9e5384f1edc008a7a4639844b254.css" />

    
    <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Rewrite of &#39;Understanding Transformers in NLP: State-of-the-Art Models&#39;"/>
<meta name="twitter:description" content="Original article Understanding Transformers in NLP: State-of-the-Art Models  Table of Contents  Sequence-to-Sequence Models &ndash; A Backdrop  RNN based Sequence-to-Sequence Model Challenges   Introduction to the Transformer in NLP  Understanding the Model Architecture Grokking Self-Attention Calculation of Self-Attention Limitations of the Transformer   Understanding Transformer-XL  Using Transformer for Language Modeling Using Transformer-XL for Language Modeling   The New Sensation in NLP: Google&rsquo;s BERT  Model Architecture BERT Pre-Training Tasks    Sequence-to-Sequence Models &ndash; A Backdrop seq2seq models in NLP are used to convert sequences of Type A to sequences of Type B."/>

    <meta property="og:title" content="Rewrite of &#39;Understanding Transformers in NLP: State-of-the-Art Models&#39;" />
<meta property="og:description" content="Original article Understanding Transformers in NLP: State-of-the-Art Models  Table of Contents  Sequence-to-Sequence Models &ndash; A Backdrop  RNN based Sequence-to-Sequence Model Challenges   Introduction to the Transformer in NLP  Understanding the Model Architecture Grokking Self-Attention Calculation of Self-Attention Limitations of the Transformer   Understanding Transformer-XL  Using Transformer for Language Modeling Using Transformer-XL for Language Modeling   The New Sensation in NLP: Google&rsquo;s BERT  Model Architecture BERT Pre-Training Tasks    Sequence-to-Sequence Models &ndash; A Backdrop seq2seq models in NLP are used to convert sequences of Type A to sequences of Type B." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://mullikine.github.io/posts/review-of-understanding-transformers-in-nlp-state-of-the-art-models/" />
<meta property="article:published_time" content="2020-06-10T00:00:00+08:00" />
<meta property="article:modified_time" content="2020-06-10T00:00:00+08:00" /><meta property="og:site_name" content="Bodacious Blog" />


  </head>
  <body>
    <header class="app-header">

<a href="https://mullikine.github.io"><img class="app-header-avatar" src="http://mullikine.github.io/fievel.png" alt="Shane Mulligan" /></a>
      <h1>Bodacious Blog</h1>
      
      <div class="app-header-social">
        
          <a target="_blank" href="https://github.com/mullikine"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-github">
  <title>github</title>
  <path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22"></path>
</svg></a>
        
          <a target="_blank" href="https://linkedin.com/in/shane-mulligan-811b942b/"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-linkedin">
  <title>linkedin</title>
  <path d="M16 8a6 6 0 0 1 6 6v7h-4v-7a2 2 0 0 0-2-2 2 2 0 0 0-2 2v7h-4v-7a6 6 0 0 1 6-6z"></path><rect x="2" y="9" width="4" height="12"></rect><circle cx="4" cy="4" r="2"></circle>
</svg></a>
        
          <a target="_blank" href="https://gitlab.com/mullikine"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-gitlab">
  <title>gitlab</title>
  <path d="M22.65 14.39L12 22.13 1.35 14.39a.84.84 0 0 1-.3-.94l1.22-3.78 2.44-7.51A.42.42 0 0 1 4.82 2a.43.43 0 0 1 .58 0 .42.42 0 0 1 .11.18l2.44 7.49h8.1l2.44-7.51A.42.42 0 0 1 18.6 2a.43.43 0 0 1 .58 0 .42.42 0 0 1 .11.18l2.44 7.51L23 13.45a.84.84 0 0 1-.35.94z"></path>
</svg></a>
        
          <a target="_blank" href="https://twitter.com/mullikine"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-twitter">
  <title>twitter</title>
  <path d="M23 3a10.9 10.9 0 0 1-3.14 1.53 4.48 4.48 0 0 0-7.86 3v1A10.66 10.66 0 0 1 3 4s-4 9 5 13a11.64 11.64 0 0 1-7 2c9 5 20 0 20-11.5a4.5 4.5 0 0 0-.08-.83A7.72 7.72 0 0 0 23 3z"></path>
</svg></a>
        
        <a href="https://mullikine.github.io/cv/">CV</a>
      </div>
<div class="highlights">
<a class="big" href="http://otagoai.com/blog/generating-poetry-with-gpt-2/">TakaheAI Blog</a>
<br />
<a href="https://mullikine.github.io/posts/practical-macros-in-racket-and-how-to-work-with-them/">Practical macros in Racket</a>
<br />
<a href="https://mullikine.github.io/posts/github-search-with-bigquery/">Searching GitHub with BigQuery</a>

<br />
<a href="https://mullikine.github.io/posts/arbitrary-interpreters-for-babel/">Arbitrary interpreters for Babel</a>
<br />
<a href="https://mullikine.github.io/posts/dwarf-fortress-macros-with-emacs-and-tmux/">Automating Dwarf Fortress</a>
<br />
<a href="https://mullikine.github.io/posts/review-of-the-illustrated-transformer/">The Illustrated Transformer</a>
<br />
<a href="https://mullikine.github.io/glossary.html">Glossary A-Z (199 topics)</a>
<br />
<a href="https://mullikine.github.io/codelingo-vs-linters/summary/">CodeLingo vs Linters</a>
<br />
<div class="taglist">

<a class="tag" href="https://mullikine.github.io/tags/biosemiotics/">biosemiotics</a>
<a class="tag" href="https://mullikine.github.io/tags/xenolinguistics/">xenolinguistics</a>
<a class="tag big" href="https://mullikine.github.io/tags/emacs/">emacs</a>
<a class="tag" href="https://mullikine.github.io/tags/gpt/">GPT (Generative Pre-Training)</a>
<a class="tag" href="https://mullikine.github.io/tags/elisp/">elisp</a>
<a class="tag" href="https://mullikine.github.io/tags/racket/">racket</a>
<a class="tag" href="https://mullikine.github.io/tags/haskell/">haskell</a>
<a class="tag" href="https://mullikine.github.io/tags/nlp/">NLP</a>
<a class="tag" href="https://mullikine.github.io/tags/docker/">docker</a>
<a class="tag" href="https://mullikine.github.io/tags/feature-engineering/">feature-engineering</a>
<a class="tag big" href="https://mullikine.github.io/tags/ir/">IR</a>
<a class="tag" href="https://mullikine.github.io/tags/games/">games</a>
<a class="tag" href="https://mullikine.github.io/tags/data/">data</a>
<a class="tag" href="https://mullikine.github.io/tags/info/">info theory</a>
<a class="tag" href="https://mullikine.github.io/tags/probability/">probability</a>
<a class="tag" href="https://mullikine.github.io/tags/problog/">problog</a>
<a class="tag big" href="https://mullikine.github.io/tags/shell/">shell</a>
<a class="tag" href="https://mullikine.github.io/tags/gcp/">GCP</a>
<a class="tag big" href="https://mullikine.github.io/tags/github/">GitHub</a>
<a class="tag" href="https://mullikine.github.io/tags/parsers/">parsers</a>
<a class="tag" href="https://mullikine.github.io/tags/rust/">rust</a>
<a class="tag" href="https://mullikine.github.io/tags/cpp/">c++</a>
<a class="tag" href="https://mullikine.github.io/tags/review/">review</a>
<a class="tag" href="https://mullikine.github.io/tags/kaggle/">kaggle</a>
<a class="tag" href="https://mullikine.github.io/tags/dl/">deep learning</a>
<a class="tag" href="https://mullikine.github.io/tags/dsl/">DSL</a>
<a class="tag" href="https://mullikine.github.io/tags/df/">dwarf fortress</a>
<a class="tag" href="https://mullikine.github.io/tags/spacy/">spacy</a>
<a class="tag" href="https://mullikine.github.io/tags/latex/">latex</a>
<a class="tag" href="https://mullikine.github.io/tags/nix/">Nix</a>
<a class="tag" href="https://mullikine.github.io/tags/diagrams/">diagrams</a>
<a class="tag" href="https://mullikine.github.io/tags/python/">python</a>
<a class="tag" href="https://mullikine.github.io/tags/golang/">golang</a>
<a class="tag" href="https://mullikine.github.io/tags/codelingo/">codelingo</a>
<a class="tag" href="https://mullikine.github.io/tags/perl/">perl</a>
<a class="tag big" href="https://mullikine.github.io/tags/vim/">vim</a>
<a class="tag" href="https://mullikine.github.io/tags/telco/">telco</a>
<a class="tag" href="https://mullikine.github.io/tags/automation/">automation</a>
<a class="tag" href="https://mullikine.github.io/tags/terminals/">terminals</a>
<a class="tag" href="https://mullikine.github.io/tags/transformer/">transformer</a>
<a class="tag big" href="https://mullikine.github.io/tags/codegen/">code-gen</a>
<a class="tag" href="https://mullikine.github.io/tags/optimisation/">optimisation</a>
<a class="tag" href="https://mullikine.github.io/tags/release/">release</a>
<a class="tag" href="https://mullikine.github.io/tags/dotnet/">.NET</a>
<a class="tag" href="https://mullikine.github.io/tags/csharp/">csharp</a>
<a class="tag" href="https://mullikine.github.io/tags/tooling/">tooling</a>
<a class="tag" href="https://mullikine.github.io/tags/iac/">IaC</a>
<a class="tag big" href="https://mullikine.github.io/tags/facebook/">Facebook</a>
<a class="tag" href="https://mullikine.github.io/tags/wfh/">WFH</a>


<a class="tag" href="https://mullikine.github.io/tags/babel/">babel</a>
<a class="tag big" href="https://mullikine.github.io/tags/uber/">Uber</a>
<a class="tag" href="https://mullikine.github.io/tags/math/">math</a>
<a class="tag" href="https://mullikine.github.io/tags/microsoft/">Microsoft</a>
<a class="tag big" href="https://mullikine.github.io/tags/openai/">OpenAI</a>
<a class="tag" href="https://mullikine.github.io/tags/rosie/">rosie</a>
<a class="tag" href="https://mullikine.github.io/tags/terraform/">Terraform</a>
</div>
</div>



</div>
<div>
    </header>
    <main class="app-container">
      
  <article class="post">
    <header class="post-header">
      <h1 class ="post-title">Rewrite of &#39;Understanding Transformers in NLP: State-of-the-Art Models&#39;</h1>
      <div class="post-meta">
        <div>
          <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-calendar">
  <title>calendar</title>
  <rect x="3" y="4" width="18" height="18" rx="2" ry="2"></rect><line x1="16" y1="2" x2="16" y2="6"></line><line x1="8" y1="2" x2="8" y2="6"></line><line x1="3" y1="10" x2="21" y2="10"></line>
</svg>
          Jun 10, 2020
        </div>
        <div>
          <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-clock">
  <title>clock</title>
  <circle cx="12" cy="12" r="10"></circle><polyline points="12 6 12 12 16 14"></polyline>
</svg>
          17 min read
        </div><div>
          <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tag">
  <title>tag</title>
  <path d="M20.59 13.41l-7.17 7.17a2 2 0 0 1-2.83 0L2 12V2h10l8.59 8.59a2 2 0 0 1 0 2.82z"></path><line x1="7" y1="7" x2="7" y2="7"></line>
</svg>
          <a class="tag" href="https://mullikine.github.io/tags/nlp/">NLP</a><a class="tag" href="http://mullikine.github.io/tags/transformer/">transformer</a><a class="tag" href="http://mullikine.github.io/tags/openai/">openai</a></div></div>
    </header>
    

<link rel="stylesheet" type="text/css" href="https://mullikine.github.io/css/magit.css"/>

<script src="https://mullikine.github.io/js/mathjax-config.js"></script>
 
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_HTML"></script>


    <div class="post-content">
      <dl>
<dt>Original article</dt>
<dd><a href="https://www.analyticsvidhya.com/blog/2019/06/understanding-transformers-nlp-state-of-the-art-models/">Understanding Transformers in NLP: State-of-the-Art Models</a></dd>
</dl>
<h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li>Sequence-to-Sequence Models &ndash; A Backdrop
<ol>
<li>RNN based Sequence-to-Sequence Model</li>
<li>Challenges</li>
</ol>
</li>
<li>Introduction to the Transformer in NLP
<ol>
<li>Understanding the Model Architecture</li>
<li>Grokking Self-Attention</li>
<li>Calculation of Self-Attention</li>
<li>Limitations of the Transformer</li>
</ol>
</li>
<li>Understanding Transformer-XL
<ol>
<li>Using Transformer for Language Modeling</li>
<li>Using Transformer-XL for Language Modeling</li>
</ol>
</li>
<li>The New Sensation in NLP: Google&rsquo;s BERT
<ol>
<li>Model Architecture</li>
<li>BERT Pre-Training Tasks</li>
</ol>
</li>
</ol>
<h2 id="sequence-to-sequence-models-a-backdrop">Sequence-to-Sequence Models &ndash; A Backdrop</h2>
<p><code>seq2seq</code> models in NLP are used to convert
sequences of Type A to sequences of Type B.</p>
<p>For example, translation of English sentences
to German sentences is a seq2seq task.</p>
<p><code>RNN-based seq2seq</code> have garnered a lot of
traction ever since they were introduced in 2014.</p>
<p>Most of the data in the current world are in
the form of sequences &ndash; it can be a number
sequence, text sequence, a video frame
sequence or an audio sequence.</p>
<p>The performance of these <code>seq2seq</code> models was further enhanced with the
addition of the <code>Attention Mechanism</code> in 2015.</p>
<p><code>seq2seq</code> models are used in a variety of NLP
tasks, such as:</p>
<ul>
<li>Machine Translation</li>
<li>Text Summarization</li>
<li>Speech Recognition</li>
<li>Question-Answering System, and so on</li>
</ul>
<h3 id="rnn-based-sequence-to-sequence-model">RNN based Sequence-to-Sequence Model</h3>
<p>Let&rsquo;s take a simple example of a seq2seq
model.</p>
<figure>
    <img src="https://mullikine.github.io/ox-hugo/seq2seq.gif"/> 
</figure>

<p>Breakdown:</p>
<ul>
<li>Both <strong>Encoder</strong> and <strong>Decoder</strong> are RNNs</li>
<li>At every time step in the Encoder, the RNN takes a word vector (<code>x_i</code>)
from the input sequence and a hidden state (<code>H_i</code>) from the previous
time step.</li>
<li>The hidden state is updated at each time step.</li>
<li>The hidden state from the last unit is known as the <code>context vector</code>.
This contains information about the input sequence.</li>
<li>This context vector is then passed to the decoder and it is then used
to generate the target sequence (English phrase)</li>
<li>If we use the <strong>Attention mechanism</strong>, then the weighted sum of the
hidden states are passed as the context vector to the decoder</li>
</ul>
<!--listend-->
<div class="highlight"><div style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">
<table style="border-spacing:0;padding:0;margin:0;border:0;width:auto;overflow:auto;display:block;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">15
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">16
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">17
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">18
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">19
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">20
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">21
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">22
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">23
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">24
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">25
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">26
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">27
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">28
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">29
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">30
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">31
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">32
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">33
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">34
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">35
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">36
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">37
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">38
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">39
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">40
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">41
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">42
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">43
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">44
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">45
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">Recurrent Neural Network
RNN
    It models sequences.

    The de facto go-to type of network to make
    predictions with time series data
    (including including all variants of
    recurrent neural networks including
    residual networks)

    Applyies the same set of weights
    recursively to the state of the aggregator
    at time t and input at time t.

    Pure RNNs are rarely used now, but its
    analogs, for example, LSTM and GRU, are
    the most up-to-date in most sequence
    modeling problems.

    LSTM, which is used instead of a simple
    dense layer in pure RNN.

    Then what‘s wrong with RNNs?
    1.  The first flaw of RNN is its
        sequential nature.

        It means that each hidden state
        depends on the output of the previous
        hidden state.

        This becomes a huge problem for GPUs.

        As they have huge a computational
        power, they resent having to wait for
        data from the network to become
        available.

        This makes RNN unfit even with
        technologies like CuDNN which slow
        down the whole process for GPU.

    2.  The second is the long-range
        dependencies.

    https://medium.com/saarthi-ai/transformers-attention-based-seq2seq-machine-translation-a28940aaa4fe</code></pre></td></tr></table>
</div>
</div>
<div class="highlight"><div style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">
<table style="border-spacing:0;padding:0;margin:0;border:0;width:auto;overflow:auto;display:block;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">15
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">16
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">17
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">18
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">19
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">20
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">21
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">22
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">23
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">24
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">25
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">26
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">27
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">28
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">29
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">30
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">31
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">32
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">33
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">34
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">35
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">36
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">37
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">38
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">39
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">40
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">41
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">42
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">43
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">44
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">45
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">46
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">47
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">48
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">49
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">50
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">51
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">52
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">53
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">54
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">55
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">56
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">57
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">58
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">59
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">60
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">61
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">62
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">63
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">64
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">65
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">66
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">67
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">68
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">69
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">70
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">71
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">72
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">73
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">74
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">context vector
    [word vector embedding]

    The hidden state from the last unit is
    known as the &#39;context vector&#39;.

    This contains information about the input
    sequence.

    See &#34;word vector&#34;:
        vim +/&#34;word vector&#34; &#34;$NOTES/glossary.txt&#34;

word vector
word vector embedding
    Obtained using two methods (both involving
    Neural Networks):
    - Skip Gram, and
    - Common Bag Of Words (CBOW)

    Limitations:
    - they presume incorrectly that a word’s
      meaning is relatively stable across
      sentences.

      Polysemy abounds, and we must beware of
      massive differences in meaning for a
      single word:
        e.g.
        - lit (an adjective that describes
        something burning) and lit (an
        abbreviation for literature); or get
        (a verb for obtaining) and get (an
        animal’s offspring).

    The optimal dimensionality of word
    embeddings is mostly task-dependent:
    - a smaller dimensionality works better
      for more syntactic tasks such as named
      entity recognition or part-of-speech
      (POS) tagging, while
    - a larger dimensionality is more useful
      for more semantic tasks such as
      sentiment analysis.

    Fixed-length vector representations.

    Useful for document retrieval and word
    sense disambiguation.

    Motivated by four goals:
    - Capture “similarity of use” among words
      “car” is similar to “auto”, but not
      similar to “hippopotamus”.
    - Quickly find constituent objects
      eg., documents that contain specified words.
    - Generate context vectors automatically
      from an unlabeled corpus.
    - Use context vectors as input to standard
      learning algorithms.

    Lack, however, a natural way to represent
    syntax, discourse, or logic.

    Accommodating all these capabilities into
    a “Grand Unified Representation” is, we
    maintain, a prerequisite for solving the
    most difficult problems in Artificial
    Intelligence, including natural language
    understanding.

    The dot product of the CV for “car” with
    CV’s of documents containing “car” to be
    larger than the dot product of the CV for
    “car”.</code></pre></td></tr></table>
</div>
</div>
<h3 id="limitations-of-seq2seq-with-attention">Limitations of <code>seq2seq with attention</code></h3>
<ul>
<li>Long-range dependencies is still challenging</li>
<li>The sequential nature of the model architecture prevents
parallelization. These challenges are addressed by Google Brain&rsquo;s
Transformer concept.</li>
</ul>
<h2 id="the-transformer">The <code>Transformer</code></h2>
<ul>
<li>proposed in the paper <a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a></li>
</ul>
<!--listend-->
<div class="highlight"><div style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">
<table style="border-spacing:0;padding:0;margin:0;border:0;width:auto;overflow:auto;display:block;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">15
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">16
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">17
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">18
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">19
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">20
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">21
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">22
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">23
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">24
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">25
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">26
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">27
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">28
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">29
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">transduction
transductive inference
    [#statistical inference]

    Reasoning from observed, specific
    (training) cases to specific (test) cases.

    vs inductive inference:
        The distinction is most interesting in
        cases where the predictions of the
        transductive model are not achievable
        by any inductive model.

    In contrast to induction, it has no
    intermediate step of learning rules /
    generalising.

transduction problems
    Approximating a mapping function from data
    and using it to make a prediction.

    Example:
    - language modeling

neural sequence transduction model
sequence transduction model
transduction model
    Examples:
    - The Transformer</code></pre></td></tr></table>
</div>
</div>
<h2 id="an-aside">An aside</h2>
<div class="highlight"><div style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">
<table style="border-spacing:0;padding:0;margin:0;border:0;width:auto;overflow:auto;display:block;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">15
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">16
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">17
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">18
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">19
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">20
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">21
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">22
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">23
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">24
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">25
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">26
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">Induction of regular languages
grammar induction
grammatical inference
    [#deep learning]
    [process]
    [task]

    Learn a grammar from a given set of
    example strings / observations.

    Not every regular language can be learned
    this way if the provided examples are
    specifically chosen to mislead (see
    language identification in the limit),
    approaches have been investigated for a
    variety of subclasses.

    Output:
        A model which accounts for the
        characteristics of the observed
        objects.

    That branch of machine learning where the
    instance space consists of discrete
    combinatorial objects such as strings,
    trees and graphs.</code></pre></td></tr></table>
</div>
</div>
<h3 id="another-aside-grex">Another aside, <code>grex</code></h3>
<dl>
<dt>grex</dt>
<dd><a href="https://mullikine.github.io/posts/review-of-grex-rust/">Review of &lsquo;grex - Rust&rsquo; - generate regex from test cases // Bodacious Blog</a> <br />
No statistical induction is done here.</dd>
</dl>
<!--listend-->
<p><a id="code-snippet--grex-input"></a>
<div class="highlight"><div style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">
<table style="border-spacing:0;padding:0;margin:0;border:0;width:auto;overflow:auto;display:block;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">Thomas Bergersen - Avalanche (Female Vocals) - YouTube
Thomas Bergersen - Our Destiny (Sun) - YouTube
Thomas Bergersen - So Small - YouTube</code></pre></td></tr></table>
</div>
</div></p>
<div class="highlight"><div style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">
<table style="border-spacing:0;padding:0;margin:0;border:0;width:auto;overflow:auto;display:block;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">grex</code></pre></td></tr></table>
</div>
</div>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">^Thomas Bergersen <span style="color:#ae81ff">\-</span> <span style="color:#f92672">(</span>?:Avalanche <span style="color:#ae81ff">\(</span>Female Vocals<span style="color:#ae81ff">\)</span>|<span style="color:#f92672">(</span>?:Our Destiny <span style="color:#ae81ff">\(</span>Sun<span style="color:#ae81ff">\)</span>|So Small<span style="color:#f92672">))</span> <span style="color:#ae81ff">\-</span> YouTube$
</code></pre></div><p><a title="asciinema recording" href="https://asciinema.org/a/folYKguzGQiEBS838hc1DTdRR" target="_blank"><img alt="asciinema recording" src="https://asciinema.org/a/folYKguzGQiEBS838hc1DTdRR.svg" /></a></p>
<h2 id="continuing-with-the-transformer">Continuing with the <code>Transformer</code></h2>
<p>Quoting from the paper:</p>
<blockquote>
<p>The Transformer is the first transduction
model relying entirely on self-attention to
compute representations of its input and
output without using sequence-aligned RNNs or
convolution.</p>
</blockquote>
<dl>
<dt>transduction</dt>
<dd>the conversion of input sequences into output sequences.</dd>
</dl>
<p>The idea behind Transformer is to handle the dependencies
between input and output with <strong>attention</strong> and recurrence completely.</p>
<p>Let&rsquo;s take a look at the architecture of the
Transformer below.</p>
<p>It might look intimidating but don&rsquo;t worry, we
will break it down and understand it block by
block.</p>
<h3 id="understanding-transformer-s-model-architecture">Understanding Transformer&rsquo;s Model Architecture</h3>
<p><span class="underline"><strong>Explanation of the <code>Encoder</code> and <code>Decoder</code> parts only</strong></span></p>
<div class="highlight"><div style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">
<table style="border-spacing:0;padding:0;margin:0;border:0;width:auto;overflow:auto;display:block;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">15
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">16
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">17
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">18
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">19
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">20
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">21
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">22
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">23
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">24
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">25
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">26
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">27
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">28
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">29
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">30
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">31
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">32
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">33
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">34
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">35
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">36
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">37
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">38
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">39
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">40
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">41
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">42
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">43
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">Multi-Head Attention block
    [component of the transformer]

    We are now familiar with RNN’s shortcoming
    is that it is not well versed in handling
    dependencies between input or output
    tokens themselves.

    To handle this flaw, the Transformer just
    allows the encoder and decoder to see the
    entire input sequence all at once,
    directly modelling these dependencies
    using self-attention.

    This fundamental idea of transformer is
    implemented by its vital component, the
    Multi-Head Attention block.

transformer
    [architecture]

    https://rubikscode.net/2019/07/29/introduction-to-transformers-architecture/

    Transformer architectures are not designed
    to operate efficiently on CPU, so we
    recommend you have a GPU available for
    both training and usage.

    https://explosion.ai/blog/spacy-pytorch-transformers

    Let’s roll back a bit to understand the
    basic concept of Transformers.

    Components
    - Multi-Head Attention block.

    https://jalammar.github.io/illustrated-transformer/

    Types of attention (in the Transformer)
    https://youtu.be/BhlOGGzC0Q0?t=569
    - encoder-decoder attention
    - encoder self-attention
    - decoder self-attention</code></pre></td></tr></table>
</div>
</div>
<p>The Encoder block has 1 layer of a <code>Multi-Head Attention</code> followed by another layer of <code>FFNN</code>.</p>
<p>The decoder, on the other hand, has an extra <strong>Masked</strong> <strong>Multi-Head Attention.</strong></p>
<p><strong>The encoder and decoder blocks are actually multiple identical encoders
and decoders stacked on top of each other.</strong> Both the encoder stack and
the decoder stack have the same number of units.</p>
<p>The number of encoder and decoder units is a hyperparameter. In the
paper, 6 encoders and decoders have been used.</p>
<p>Let&rsquo;s see how this setup of the encoder and the decoder stack works:</p>
<ul>
<li>The word embeddings of the input sequence are passed to the first
encoder</li>
<li>These are then transformed and propagated to the next encoder</li>
<li>The output from the last encoder in the encoder-stack is passed to
all the decoders in the decoder-stack as shown in the figure below:</li>
</ul>
<p>An important thing to note here &ndash; in addition to the <strong>self-attention</strong>
and feed-forward layers, the decoders also have one more layer of
Encoder-Decoder Attention layer. This helps the decoder focus on the
appropriate parts of the input sequence.</p>
<p>You might be thinking &ndash; what exactly does this “Self-Attention” layer
do in the Transformer? Excellent question! This is arguably the most
crucial component in the entire setup so let&rsquo;s understand this concept.</p>
<h3 id="grokking-self-attention">Grokking Self-Attention</h3>
<p>According to the paper:</p>
<blockquote>
<p>“Self-attention, sometimes called intra-attention, is an attention
mechanism relating different positions of a single sequence in order
to compute a representation of the sequence.”</p>
</blockquote>
<p>Take a look at the above image. Can you figure out what the term <strong>“it”</strong>
in this sentence refers to?</p>
<p>Is it referring to the street or to the animal? It&rsquo;s a simple question
for us but not for an algorithm. When the model is processing the word
“it”, self-attention tries to associate “it” with <strong>“animal”</strong> in the same
sentence.</p>
<p>Self-attention allows the model to look at the other words in the input
sequence to get a better understanding of a certain word in the
sequence. Now, let&rsquo;s see how we can calculate self-attention.</p>
<p> </p>
<h3 id="calculating-self-attention">Calculating Self-Attention</h3>
<p>I have divided this section into various steps for ease of
understanding.</p>
<ol>
<li>First, we need to create three vectors from each of the encoder&rsquo;s</li>
</ol>
<p>input vectors:</p>
<ol>
<li>
<ol>
<li>Query Vector</li>
<li>Key Vector</li>
<li>Value Vector.</li>
</ol>
</li>
</ol>
<p>These vectors are trained and updated during the training process. We&rsquo;ll
know more about their roles once we are done with this section</p>
<ol>
<li>Next, we will calculate self-attention for every word in the input</li>
</ol>
<p>sequence</p>
<ol>
<li>Consider this phrase &ndash; “Action gets results”. To calculate the</li>
</ol>
<p>self-attention for the first word “Action”, we will calculate scores for
all the words in the phrase with respect to “Action”. This score
determines the importance of other words when we are encoding a certain
word in an input sequence</p>
<ol>
<li>
<ol>
<li>The score for the first word is calculated by taking the dot
product of the Query vector (q1) with the keys vectors (k1, k2,
k3) of all the
words.</li>
<li>Then, these scores are divided by 8 which is the square root of
the dimension of the key
vector.</li>
<li>Next, these scores are normalized using the softmax activation
function.</li>
<li>These normalized scores are then multiplied by the value vectors
(v1, v2, v3) and sum up the resultant vectors to arrive at the
final vector (z1). This is the output of the self-attention layer.
It is then passed on to the feed-forward network as
input.</li>
</ol>
</li>
</ol>
<p>So, z1 is the self-attention vector for the first word of the input
sequence “Action gets results”. We can get the vectors for the rest of
the words in the input sequence in the same fashion:</p>
<p>Self-attention is computed not once but multiple times in the
Transformer&rsquo;s architecture, in parallel and independently. It is
therefore referred to as <strong>Multi-head Attention</strong>. The outputs are
concatenated and linearly transformed as shown in the figure below:</p>
<p>According to the paper “Attention Is All You Need”:</p>
<blockquote>
<p>“Multi-head attention allows the model to jointly attend to
information from different representation subspaces at different
positions.”</p>
</blockquote>
<p>You can access the code to implement Transformer <a href="https://paperswithcode.com/paper/attention-is-all-you-need">here</a>.</p>
<h3 id="limitations-of-the-transformer">Limitations of the Transformer</h3>
<p>Transformer is undoubtedly a huge improvement over the RNN based seq2seq
models. But it comes with its own share of limitations:</p>
<ul>
<li>Attention can only deal with fixed-length text strings. The text has
to be split into a certain number of segments or chunks before being
fed into the system as input</li>
<li>This chunking of text causes <strong>context fragmentation</strong>. For example, if
a sentence is split from the middle, then a significant amount of
context is lost. In other words, the text is split without respecting
the sentence or any other semantic boundary</li>
</ul>
<p>So how do we deal with these pretty major issues? That&rsquo;s the question
folks who worked with Transformer asked. And out of this came
Transformer-XL.</p>
<h2 id="understanding-transformer-xl">Understanding Transformer-XL</h2>
<p>Transformer architectures can learn longer-term dependency. However,
they can&rsquo;t stretch beyond a certain level due to the use of fixed-length
context (input text segments). A new architecture was proposed to
overcome this shortcoming in the paper &ndash;
<a href="https://arxiv.org/pdf/1901.02860.pdf">Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context</a>.</p>
<p>In this architecture, the hidden states obtained in previous segments
are reused as a source of information for the current segment. It
enables modeling longer-term dependency as the information can flow from
one segment to the next.</p>
<h3 id="using-transformer-for-language-modeling">Using Transformer for Language Modeling*\\</h3>
<blockquote>
<p>Think of language modeling as a process of estimating the probability
of the next word given the previous words.</p>
</blockquote>
<p><a href="https://arxiv.org/abs/1808.04444">Al-Rfou et al. (2018)</a> proposed the
idea of <strong>applying the Transformer model for language modeling</strong>. As per
the paper, the entire corpus can be split into fixed-length segments of
manageable sizes. Then, we train the Transformer model on the segments
independently, ignoring all contextual information from previous
segments:</p>
<p><em>Transformer Model with a segment length of 4 (Source: <a href="https://arxiv.org/abs/1901.02860">https://arxiv.org/abs/1901.02860</a>)</em></p>
<p>This architecture doesn&rsquo;t suffer from the problem of vanishing
gradients. But the context fragmentation limits its longer-term
dependency learning. During the evaluation phase, the segment is shifted
to the right by only one position. The new segment has to be processed
entirely from scratch. This evaluation method is unfortunately quite
compute-intensive.</p>
<h3 id="using-transformer-xl-for-language-modeling">Using Transformer-XL for Language Modeling</h3>
<p>During the training phase in Transformer-XL, the hidden state computed
for the previous state is used as an additional context for the current
segment. This recurrence mechanism of Transformer-XL takes care of the
limitations of using a fixed-length context.</p>
<p><em>Transformer XL Model with a segment length of 4</em></p>
<p>During the evaluation phase, the representations from the previous
segments can be reused instead of being computed from scratch (as is the
case of the Transformer model). This, of course, increases the
computation speed manifold.</p>
<p>You can access the code to implement Transformer-XL <a href="https://paperswithcode.com/paper/transformer-xl-attentive-language-models">here</a>.</p>
<h2 id="the-new-sensation-in-nlp-google-s-bert--bidirectional-encoder-representations-from-transformers"><strong>The New Sensation in NLP: Google&rsquo;s BERT (Bidirectional Encoder Representations from Transformers)</strong></h2>
<p>We all know how significant <strong>transfer learning</strong> has been in the field of
computer vision. For instance, a pre-trained deep learning model could
be fine-tuned for a new task on the ImageNet dataset and still give
decent results on a relatively small labeled dataset.</p>
<ul>
<li>Language model pre-training similarly has been quite effective for improving many natural language processing tasks:
<ul>
<li><a href="https://openai.com/blog/language-unsupervised/">https://paperswithcode.com/paper/transformer-xl-attentive-language-models</a></li>
<li><a href="https://arxiv.org/abs/1801.06146">https://paperswithcode.com/paper/transformer-xl-attentive-language-models</a>).</li>
</ul>
</li>
</ul>
<p>The BERT framework, a new language representation model from Google AI,
uses pre-training and fine-tuning to create state-of-the-art models for
a wide range of tasks. These tasks include question answering systems,
sentiment analysis, and language inference.</p>
<h3 id="bert-s-model-architecture">BERT&rsquo;s Model Architecture</h3>
<p>BERT uses a multi-layer bidirectional Transformer encoder. Its
self-attention layer performs self-attention in both directions. Google
has released two variants of the model:</p>
<ol>
<li><strong>BERT Base</strong>: Number of Transformers layers = 12, Total Parameters =
110M</li>
<li><strong>BERT Large</strong>: Number of Transformers layers = 24, Total Parameters =
340M</li>
</ol>
<p>BERT uses bidirectionality by pre-training on a couple of tasks &mdash;
<strong>Masked Language Model</strong> and <strong>Next Sentence Prediction</strong>. Let&rsquo;s discuss
these two tasks in detail.</p>
<h3 id="bert-pre-training-tasks">BERT Pre-Training Tasks</h3>
<p>BERT is pre-trained using the following two unsupervised prediction
tasks.</p>
<h4 id="1-dot-masked-language-modeling--mlm">1. Masked Language Modeling (MLM)</h4>
<p>According to the <a href="https://arxiv.org/pdf/1810.04805.pdf">paper</a>:</p>
<blockquote>
<p>“The masked language model randomly masks some of the tokens from the
input, and the objective is to predict the original vocabulary id of
the masked word based only on its context. Unlike left-to-right
language model pre-training, the MLM objective allows the
representation to fuse the left and the right context, which allows us
to pre-train a deep bidirectional Transformer.”</p>
</blockquote>
<p>The Google AI researchers masked 15% of the words in each sequence at
random. The task? To predict these masked words. A caveat here &ndash; the
masked words were not always replaced by the masked tokens [MASK]
because the [MASK] token would never appear during fine-tuning.</p>
<p>So, the researchers used the below technique:</p>
<ul>
<li>80% of the time the words were replaced with the masked token [MASK]</li>
<li>10% of the time the words were replaced with random words</li>
<li>10% of the time the words were left unchanged</li>
</ul>
<h4 id="2-dot-next-sentence-prediction">2. Next Sentence Prediction</h4>
<p>Generally, language models do not capture the relationship between
consecutive sentences. BERT was pre-trained on this task as well.</p>
<p>For language model pre-training, BERT uses pairs of sentences as its
training data. The selection of sentences for each pair is quite
interesting. Let&rsquo;s try to understand it with the help of an example.</p>
<p>Imagine we have a text dataset of 100,000 sentences and we want to
pre-train a BERT language model using this dataset. So, there will be
50,000 training examples or pairs of sentences as the training data.</p>
<ul>
<li>For 50% of the pairs, the second sentence would actually be the next
sentence to the first sentence</li>
<li>For the remaining 50% of the pairs, the second sentence would be a
random sentence from the corpus</li>
<li>The labels for the first case would be <em><strong>‘IsNext&rsquo;</strong></em> and <em><strong>‘NotNext&rsquo;</strong></em>
for the second case</li>
</ul>
<p>Architectures like BERT demonstrate that unsupervised learning
(pre-training and fine-tuning) is going to be a key element in many
language understanding systems. Low resource tasks especially can reap
huge benefits from these deep <em>bidirectional</em> architectures.</p>
<p>Below is a snapshot of a few NLP tasks where BERT plays an important
role:</p>
<p><em>Source: <a href="https://arxiv.org/abs/1810.04805">https://arxiv.org/abs/1810.04805</a></em></p>
<h2 id="end-notes">End Notes</h2>
<p>We should really consider ourselves lucky as so many state-of-the-art
advancements are happening in NLP at such a rapid pace. Architectures
like Transformers and BERT are paving the way for even more advanced
breakthroughs to happen in the coming years.</p>
<p>I encourage you to implement these models and share your work in the
comments section below. And if you have any feedback on this article or
any doubts/queries, then do let me know and I will get back to you.</p>
<p>You can also take the below course to learn or brush up your NLP skills:</p>
<ul>
<li><a href="https://courses.analyticsvidhya.com/courses/natural-language-processing-nlp??utm%5Fsource=blog&amp;utm%5Fmedium=understanding-transformers-nlp-state-of-the-art-models">Natural Language Processing (NLP) using Python</a></li>
</ul>
<h2 id="intuition-about-the-transformer">Intuition about the transformer</h2>
<p><a href="https://www.youtube.com/watch?v=S27pHKBEp30">YouTube</a></p>
<p>You need to somehow take your document and
represent it as a fixed-size vector.</p>
<p>&ldquo;I&rsquo;m not aware of any linear algebra that
works on vectors of variable dimensionality&rdquo;.</p>
<p>The challenge with this is that documents are
of variable length.</p>
<p>You have to come up with some way of taking
that document and meaningfully encoding it
into a fixed size vector.</p>
<p>The classic way of doing this is the bag of
words right where you have one dimension per
unique word in your vocabulary.</p>
<h3 id="evolution-of-sequence-modelling">Evolution of sequence modelling</h3>
<div class="highlight"><div style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">
<table style="border-spacing:0;padding:0;margin:0;border:0;width:auto;overflow:auto;display:block;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fallback" data-lang="fallback">&#34;bag of words&#34; -&gt; RNN -&gt; LSTM -&gt; Transformer</code></pre></td></tr></table>
</div>
</div>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fallback" data-lang="fallback">+--------------+     +-----+     +------+     +-------------+
| bag of words | --&gt; | RNN | --&gt; | LSTM | --&gt; | Transformer |
+--------------+     +-----+     +------+     +-------------+
</code></pre></div><ul>
<li>BoW Bag of Words
<ul>
<li>One dimension per word in vocabulary</li>
<li>d = 100,000</li>
<li>Almost all values are zero</li>
<li>Leads to sparse data
<ul>
<li>Don&rsquo;t actually store the zeros. You store lists
of position-value tuples. &ndash; skip list?</li>
</ul>
</li>
<li>Key limitation:
<ul>
<li>Order of words matters and BoW doesn&rsquo;t take order into account.</li>
<li>One solution: n-grams and bigrams.
<ul>
<li>Problem: English trigrams: 10^15 dimensions. Impractical.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>RNN: Solution to BoW
<ul>
<li>Recursively define the output at any stage
as a function of inputs at the previous
stages in the previous output.</li>
<li>For the purpose of supervised learning,
the final output is just the final hidden
state.</li>
</ul>
</li>
</ul>

    </div>
    <div class="post-footer">
        <p>
        <a href="https://mullikine.github.io/about/">About this weblog</a>
        &nbsp;
        ⚔
        &nbsp;
        <a href="https://asciinema.org/~mullikine">Asciinema recordings</a>
        &nbsp;
        ⚔
        &nbsp;
        <a href="https://mullikine.github.io/posts/blogs-and-vlogs/">Blogs and Vlogs</a>
        &nbsp;
        ⚔
        &nbsp;
        <a href="https://mullikine.github.io/tags/">Site Map</a>
        </p>
    </div>
  </article>

    </main>
  </body>
</html>