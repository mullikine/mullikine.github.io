https://www.aclweb.org/anthology/N18-2075/

* Latex
https://www.overleaf.com/learn/latex/Aligning%20equations%20with%20amsmath

* Rust
https://yoric.github.io/post/rust-typestate/

Typestates are an anachronism.

* NLP
https://medium.com/syncedreview/chineseglue-new-nlu-benchmark-for-chinese-nlp-models-bfcccc0b6c86
https://medium.com/syncedreview/baidus-ernie-2-0-beats-bert-and-xlnet-on-nlp-benchmarks-51a8c21aa433

* Interesting
https://venturebeat.com/2019/10/24/google-achieves-state-of-the-art-nlp-performance-with-an-enormous-language-model-and-data-set/
** Blog about bert for search
https://www.theverge.com/2019/10/25/20931657/google-bert-search-context-algorithm-change-10-percent-langauge
https://www.searchenginejournal.com/google-bert-update/332161/amp/

* I should automatically remove the fbclid thing when pasting into emacs
https://arxiv.org/abs/1909.01066

#+BEGIN_SRC sh :async :results verbatim drawer
  arxiv-summary -u https://arxiv.org/abs/1909.01066
#+END_SRC

#+RESULTS:
:RESULTS:
Recent progress in pretraining LMs on large
textual corpora led to a surge of improvements
for downstream NLP tasks.

Whilst learning linguistic knowledge, these
models may also be storing relational
knowledge present in the training data, and
may be able to answer queries structured as
"fill-in-the-blank" cloze statements.

LMs have many advantages over structured KBs:
they require no schema engineering, allow
practitioners to query about an open class of
relations, are easy to extend to more data,
and require no human supervision to train.

We present an in-depth analysis of the
relational knowledge already present (without
fine-tuning) in a wide range of SOTA
pretrained LMs.

We find that (i) without fine-tuning, BERT
contains relational knowledge competitive with
traditional NLP methods that have some access
to oracle knowledge, (ii) BERT also does
remarkably well on open-domain QA against a
supervised baseline, and (iii) certain types
of factual knowledge are learned much more
readily than others by standard LM pretraining
approaches.

The surprisingly strong ability of these
models to recall factual knowledge without any
fine-tuning demonstrates their potential as
unsupervised open-domain QA systems.

The code to reproduce our analysis is
available at this https URL.
:END:

* NLP
** TODO [#A] UniLM
https://venturebeat.com/2019/10/16/microsofts-unilm-ai-achieves-state-of-the-art-performance-on-summarization-and-language-generation/

UniLM AI achieves state-of-the-art performance
on summarization and language generation.

* Relationship without intimacy
https://icppd.com/can-a-relationship-survive-without-intimacy/

* Study this website
https://www.gwern.net/index

** Read
https://www.gwern.net/Language

* [#A] Read this in =eww=. The website when viewed with a gui browser, although impressive, is distracting
#+BEGIN_SRC sh :async :results verbatim drawer
  eww "https://www.gwern.net/GPT-2"
#+END_SRC

* Read this
https://elastisys.com/2015/09/10/scalability-design-principles/

* DONE Go to Megan's thing
https://www.facebook.com/events/469665216962238/

* Transformer explanation
https://jalammar.github.io/illustrated-transformer/

* Get this going
$NOTES/ws/utils/fuzzy.txt

* =writup.ai=
https://senrigan.io/blog/how-writeupai-runs-behind-the-scenes/

* Text generator with Keras
https://www.thepythoncode.com/article/text-generation-keras-python

* Keyword extraction
https://medium.com/analytics-vidhya/automated-keyword-extraction-from-articles-using-nlp-bfd864f41b34
https://towardsdatascience.com/textrank-for-keyword-extraction-by-python-c0bae21bcec0

Will this do what I want?
I can use spaCy.

https://medium.com/reputation-com-datascience-blog/keywords-extraction-with-ngram-and-modified-skip-gram-based-on-spacy-14e5625fce23

* A great paper - A Comparative Study of Programming Languages in Rosetta Code
https://arxiv.org/pdf/1409.0252.pdf

* Read this -- How to Write Fast(er) Emacs Lisp
https://nullprogram.com/blog/2017/01/30/

** lexical scope
#+BEGIN_SRC sh :async :results verbatim drawer
  ewwlinks +/"(1) Use lexical scope" "https://nullprogram.com/blog/2017/01/30/"
#+END_SRC

* ansiterm, shell or external
https://www.reddit.com/r/emacs/comments/dg6kvx/do_you_use_term_ansiterm_shell_or_external/

* Abstractive summariser
https://hackernoon.com/build-an-abstractive-text-summarizer-in-94-lines-of-tensorflow-tutorial-6-f0e1b4d88b55

* ngram NLG
https://medium.com/@b.terryjack/natural-language-generation-nlg-fe8844db6f01

* emacs
** comint
https://www.masteringemacs.org/article/shell-comint-secrets-history-commands

** eshell
*** expansion
https://www.gnu.org/software/emacs/manual/html_mono/eshell.html#Expansion

* This is actually some good practical advice. At least develop this mentality as a base
https://psgraphics.blogspot.com/2019/09/how-to-succeed-as-poor-programmer.html?m=1

* Plant intelligence
https://www.theparisreview.org/blog/2019/09/26/the-intelligence-of-plants/

* Hierarchical Decision Making by Generating and Following Natural Language Instructions
https://arxiv.org/abs/1906.00744

:summary:
We explore using latent natural language
instructions as an expressive and
compositional representation of complex
actions for hierarchical decision making.

Rather than directly selecting micro-actions,
our agent first generates a latent plan in
natural language, which is then executed by a
separate model.

We introduce a challenging real-time strategy
game environment in which the actions of a
large number of units must be coordinated
across long time scales.

We gather a dataset of 76 thousand pairs of
instructions and executions from human play,
and train instructor and executor models.

Experiments show that models using natural
language as a latent variable significantly
outperform models that directly imitate human
actions.

The compositional structure of language proves
crucial to its effectiveness for action
representation.

We also release our code, models and data.
:END:

* The Shack
$MYGIT/mullikine/reading/The Shack - William P. Young.txt
$MYGIT/mullikine/reading/Thinking, Fast and Slow - Kahneman, Daniel.txt

* This is an important place to check fairly regularly
https://www.reddit.com/r/haskell/

* [#A] problog -- this is really good
http://csci431.artifice.cc/notes/problog.html
http://csci431.artifice.cc/notes/naive-bayesian.html

* This whole site is good
http://csci431.artifice.cc/

* Read - This Simple Structure Unites All Human Languages
http://nautil.us/issue/76/language/this-simple-structure-unites-all-human-languages

* [#A] Functor, applicative, and monad
https://typeslogicscats.gitlab.io/posts/functor-applicative-monad.html

* NLP
$NOTES/ws/gpt-2/examples/links.org

* fp and deep learning
https://www.welcometothejungle.co/fr/articles/btc-deep-learning-clojure-haskell

* What Kind of Language Is Hard to Language-Model?
https://arxiv.org/abs/1906.04726

#+BEGIN_SRC sh :async :results verbatim drawer
  arxiv-summary "https://arxiv.org/abs/1906.04726"
#+END_SRC

* Productionizing NLP Models (Beyond modelling)
https://towardsdatascience.com/productionizing-nlp-models-9a2b8a0c7d14

* Natural Language in Python using spaCy: An Introduction
https://blog.dominodatalab.com/natural-language-in-python-using-spacy/?r=1

* List of fallacies
https://en.wikipedia.org/wiki/List_of_fallacies

* NLG libraries
https://medium.com/sciforce/a-comprehensive-guide-to-natural-language-generation-dd63a4b6e548

* NLP
** Universal Adversarial Triggers for Attacking and Analyzing NLP
We create short phrases that cause a specific
model prediction when concatenated to ð˜¢ð˜¯ð˜º
input from a dataset.

Triggers cause:
- GPT-2 to spew racism
- SQuAD models to answer "to kill american people" for 72% of questions asking "Why..."
- Classification models to drop from 90% accuracy to 1%

http://www.ericswallace.com/triggers

* Topic modelling
https://medium.com/@souravboss.bose/comprehensive-topic-modelling-with-nmf-lsa-plsa-lda-lda2vec-part-1-20002a8e03ae

* [#A] Easy haskell parsing tutorial
https://two-wrongs.com/parser-combinators-parsing-for-haskell-beginners.html

+ grok the article [/]
  - [-] Create a playground or use ghci for testing the functions

* Some easy haskells
https://wiki.haskell.org/Cookbook/Lists_and_strings

* BERT
https://towardsdatascience.com/breaking-bert-down-430461f60efb

* zipfs law and NLP
https://medium.com/@_init_/using-zipfs-law-to-improve-neural-language-models-4c3d66e6d2f6

* [#A] StrangeLoop talk transcript on BitFunnel core ideas
http://bitfunnel.org/strangeloop/

* [#A] NLP - Age of Transformers
https://blog.scaleway.com/2019/building-a-machine-reading-comprehension-system-using-the-latest-advances-in-deep-learning-for-nlp/

* haskell
#+BEGIN_SRC sh :async :results verbatim drawer
  vim +/"Haskell: Simon Peyton-Jones" "$DUMP$NOTES/ws/programming-languages/az.txt"
#+END_SRC

* prolog
http://www.amzi.com/AdventureInProlog/advtop.php

* Read
https://medium.com/@aqsakausar30/nlp-in-tensorflow-all-you-need-for-a-kickstart-3293d7d2630e

* This looks good - code generation / IR?
https://blog.sigplan.org/2019/08/22/from-programs-to-deep-models-part-1/

* TODO Read this -- checn linked me the github code
http://www.peterbloem.nl/blog/transformers
https://github.com/pbloem/former
** Try to become better at spacy and the huggingface transformers
https://explosion.ai/blog/spacy-pytorch-transformers

* [#A] NLP
https://blog.dominodatalab.com/deep-learning-illustrated-building-natural-language-processing-models/

* Haskell syntax sugar
https://en.wikibooks.org/wiki/Haskell/Syntactic_sugar

* [#A] Read about the state of transfer learning for NLP 2019
http://ruder.io/state-of-transfer-learning-in-nlp/

Take notes.

Use NLP to take notes.

* Best programming languages
http://matt.might.net/articles/best-programming-languages/

* Rust vs Python
https://ngoldbaum.github.io/posts/python-vs-rust-nn/

* Go over this -- do some learning
https://medium.com/datadriveninvestor/deep-learning-techniques-for-text-classification-9392ca9492c7

** TODO [#A] Learn reinforcement learning

This looks like a really good read.

[[https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html][A (Long) Peek into Reinforcement Learning]]

* [#A] Dependent haskell
https://serokell.io/blog/why-dependent-haskell

* [#A] Read -- I will learn more about practical NLP
https://spacy.io/usage/rule-based-matching

* Read this stuff -- make snippets
https://dev.to/deepu105/7-easy-functional-programming-techniques-in-go-3idp

* monads as a programming pattern
https://samgrayson.me/2019-08-06-monads-as-a-programming-pattern/

* Read from Andrew Ng
https://medium.com/@mohamedalihabib7/advice-on-building-a-machine-learning-career-and-reading-research-papers-by-prof-andrew-ng-f90ac99a0182

* You Only Need Attention to Traverse Trees
https://aclweb.org/anthology/papers/P/P19/P19-1030/

In recent NLP research, a topic of
interest is universal sentence encoding,
sentence representations that can be used
in any supervised task.

At the word sequence level, fully
attention-based models suffer from two
problems:
- a quadratic increase in memory
consumption with respect to the sentence
length, and
- an inability to capture and use
syntactic information.

Recursive neural nets can extract very
good syntactic information by traversing a
tree structure.

To this end, we propose Tree Transformer,
a model that captures phrase level syntax
for constituency trees as well as word-
level dependencies for dependency trees by
doing recursive traversal only with
attention.

Evaluation of this model on four tasks
gets noteworthy results compared to the
standard transformer and LSTM-based models
as well as tree-structured LSTMs.

Ablation studies to find whether
positional information is inherently
encoded in the trees and which type of
attention is suitable for doing the
recursive traversal are provided.

** Glossary
#+BEGIN_SRC text :async :results verbatim drawer
  ablation study
      [procedure]
      
      Certain parts of the network are removed,
      in order to gain a better understanding of
      the networkâ€™s behaviour.
#+END_SRC

* haskell
** language extensions
https://www.schoolofhaskell.com/school/to-infinity-and-beyond/pick-of-the-week/guide-to-ghc-extensions/basic-syntax-extensions

** basics
https://argumatronic.com/posts/1970-01-01-beginners.html

* NLG + NLU
https://medium.com/towards-artificial-intelligence/unified-language-model-pre-training-for-natural-language-understanding-and-generation-f87dc226aa2

* regex
https://gabebw.com/blog/2015/10/11/regular-expressions-in-haskell

* Abstract algebra
https://argumatronic.com/posts/2019-06-21-algebra-cheatsheet.html

* Catching a Unicorn with GLTR: A tool to detect automatically generated text
http://gltr.io/

* Clojure
https://purelyfunctional.tv/mini-guide/how-to-read-clojure-code-like-an-expert/

* transformers
https://huggingface.co/pytorch-transformers/examples.html

https://rubikscode.net/2019/07/29/introduction-to-transformers-architecture/

* Read the spaCy blog
https://explosion.ai/blog/spacy-v2-1

I should try to stay on top of this project.

* Arxiv summaries
#+BEGIN_SRC sh
  as 1803.03453
#+END_SRC

* Keras Sequential Model
#+BEGIN_SRC sh
  eww "https://keras.io/getting-started/sequential-model-guide/"
#+END_SRC

* [#A] The Morning Paper -- A paper a day
https://blog.acolyer.org/

** [#A] Meta-learning neural Bloom filters
https://blog.acolyer.org/2019/07/19/meta-learning-neural-bloom-filters/

They even talk about the paper I read.
[[https://blog.acolyer.org/2018/01/08/the-case-for-learned-index-structures-part-i/][The case for learned index structures - part I - the morning paper]]
[[https://blog.acolyer.org/2018/01/09/the-case-for-learned-index-structures-part-ii/][The case for learned index structures - Part II - the morning paper]]

https://blog.acolyer.org/2019/01/16/sagedb-a-learned-database-system/

* Haskell TensorFlow
$DUMP$NOTES/ws/haskell-tensorflow/haskell-tensorflow-guide.txt

* racket reflection
https://docs.racket-lang.org/guide/reflection.html

* idiomatic python
https://docs.python-guide.org/

* python postgresql
https://hakibenita.com/fast-load-data-python-postgresql

* c/cpp common error messages
https://latedev.wordpress.com/2014/04/22/common-c-error-messages-2-unresolved-reference/

* Haskell iterating over arrays / vectors
#+BEGIN_SRC sh
  ewwlinks +/"Numeric Haskell: A Vector Tutorial" "https://wiki.haskell.org/Numeric_Haskell:_A_Vector_Tutorial#Indexing_vectors"
#+END_SRC

* BERT TensorFlow Search Engine
https://towardsdatascience.com/building-a-search-engine-with-bert-and-tensorflow-c6fdc0186c8a

* Transformer
https://blog.exxactcorp.com/examining-the-transformer-architecture-part-3-training-a-transformer-network-from-scratch-in-docker/

* [#A] Read this - WHAT I WISH I KNEW WHEN LEARNING HASKELL
It's very good.

http://dev.stephendiehl.com/hask/

* Rasa
https://blog.rasa.com/rasa-nlu-in-depth-part-1-intent-classification/

* [#A] Good read
https://pair-code.github.io/interpretability/bert-tree/

* Haskell
https://www.haskell.org/tutorial/index.html

* Extremely good blog
https://www.adhiraiyan.org/

* tensorflow probability
https://www.tensorflow.org/probability

* Chapter 3: Probability and Information Theory with Tensorflow Probability
https://www.adhiraiyan.org/deeplearning/03.00-Probability-and-Information-Theory

* haskell
** keywords
#+BEGIN_SRC sh
  eww "https://wiki.haskell.org/Keywords#type"
#+END_SRC

** simple examples
#+BEGIN_SRC sh
  eww "https://www.schoolofhaskell.com/school/to-infinity-and-beyond/pick-of-the-week/Simple%20examples"
#+END_SRC

* Read this
$MYGIT/mullikine/mullikine.github.io/practical-macros-in-racket-and-how-to-work-with-them.org

The eww version is better.
#+BEGIN_SRC sh
  eww "https://mullikine.github.io/practical-macros-in-racket-and-how-to-work-with-them.html"
#+END_SRC

* Learn to automate emacs interactive functions
The problem is that for most cli programs, I'm able to automate them.
But the more interactive they get the harder they get to automate.

* TODO new emacs binding -- open racket doc immediately
- immediate
- in eww
- in spv split
- use traditional region selection map override method

* meta-learning
https://blog.floydhub.com/meta-rl/

* [#A] TODO Read all of this
http://frnsys.com/ai_notes/machine_learning/natural_language_processing.html

* Read up on this
https://dtai.cs.kuleuven.be/problog/natural_language/

* [#A] Read this
http://vrici.digitalkingdom.org/~rlpowell/software/spath/README

Learning this CL library will help me to dig into sexp objects.

* [#A] I should read with eww
https://medium.com/huggingface/the-best-and-most-current-of-modern-natural-language-processing-5055f409a1d1

* Learn haskell -- do it with 'eww'
#+BEGIN_SRC sh
  eww "http://learnyouahaskell.com/chapters"
#+END_SRC

* Spacy course (markdown)
This course has been designed by one of the core developers.

This will make me a legit NLP developer.
But then I need to take it to the next level and do some Transformer / GPT-2

Because spacy is very real-language-specific.

$MYGIT/ines/spacy-course/slides

$MYGIT/ines/spacy-course/slides/chapter1_01_introduction-to-spacy.md
$MYGIT/ines/spacy-course/slides/chapter1_02_statistical-models.md
$MYGIT/ines/spacy-course/slides/chapter1_03_rule-based-matching.md
$MYGIT/ines/spacy-course/slides/chapter2_01_data-structures-1.md
$MYGIT/ines/spacy-course/slides/chapter2_02_data-structures-2.md
$MYGIT/ines/spacy-course/slides/chapter2_03_word-vectors-similarity.md
$MYGIT/ines/spacy-course/slides/chapter2_04_models-rules.md
$MYGIT/ines/spacy-course/slides/chapter3_01_processing-pipelines.md
$MYGIT/ines/spacy-course/slides/chapter3_02_custom-pipeline-components.md
$MYGIT/ines/spacy-course/slides/chapter3_03_extension-attributes.md
$MYGIT/ines/spacy-course/slides/chapter3_04_scaling-performance.md
$MYGIT/ines/spacy-course/slides/chapter4_01_training-updating-models.md
$MYGIT/ines/spacy-course/slides/chapter4_02_training-loop.md
$MYGIT/ines/spacy-course/slides/chapter4_03_training-best-practices.md
$MYGIT/ines/spacy-course/slides/chapter4_04_wrapping-up.md

* Read
https://blog.floydhub.com/gpt2/

* 
https://medium.com/towards-artificial-intelligence/attention-please-document-classification-7be927e758a

* Read this
http://echo.rsmw.net/n00bfaq.html

* Read this abstract
$DUMP/tmp/scratchZu26Lb.txt

* Haskell
** 24 Days of GHC Extensions: Deriving
https://ocharles.org.uk/guest-posts/2014-12-15-deriving.html

* TODO make a vimlinks goto binding for eww-mode
vimlinks +/"Decision Tree Algorithm" "https://www.saedsayad.com/decision_tree_reg.htm"

* Make a new glossary system
Tree-based.
This is important for my own learning.
Otherwise, I am waiting for the NLP model to become perfected.
I should be thinking in the world in which the NLP model is perfected
and computers have perfect language understanding.

Word vectors are severely limited.

Spend lots of time reading towards data science and medium.
https://towardsdatascience.com/sentence-classification-using-bi-lstm-b74151ffa565

* Good overview of machine learning basics
https://xgboost.readthedocs.io/en/latest/tutorials/model.html

* 
https://uxplanet.org/beautiful-and-functional-interfaces-f293ea2b367f

* GPT-2
https://towardsdatascience.com/too-powerful-nlp-model-generative-pre-training-2-4cc6afb6655
https://towardsdatascience.com/openais-gpt-2-the-model-the-hype-and-the-controversy-1109f4bfd5e8

* Attention
https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf

* eamcs
| kb    | f            |
|-------+--------------|
| C-c O | tmux-edit-vs |

* Examples should be my first port of call
** References should be for trying to understand examples
https://github.com/google/re2/wiki/Syntax

Before memorising this, ensure I can search for examples.

* LOL
https://www.reddit.com/r/explainlikeimfive/comments/1brymm/eli5_what_is_a_hypercube_and_what_use_does_it/

* Add to my glossary
https://towardsdatascience.com/ai-search-algorithms-every-data-scientist-should-know-ed0968a43a7a?fbclid=IwAR0znoVpxxN8oriY1c8QlmWfNyBfio2QjBJ5U5U1a3C3_fGyy7ZG8_3pRH8

* haskellbyexample
https://lotz84.github.io/haskellbyexample/

* sequencing
https://docs.racket-lang.org/guide/begin.html

* Text classification algorithms
$DUMP$HOME/notes2018/ws/nlp-natural-language-processing/reading/information-10-00150-v2.txt

#+BEGIN_SRC sh
  egr racket sequence functions
  egr haskell sequence functions
  
  cbn egr "racket\nhaskell" sequence functions
#+END_SRC

* This is actually a great place to learn the intuition behind monads
https://wiki.haskell.org/Blow_your_mind#Monad_magic

* Learn
$MYGIT/NervanaSystems/nlp-architect/examples/chunker/README.md
$MYGIT/NervanaSystems/nlp-architect/examples/crosslingembs/README.md
$MYGIT/NervanaSystems/nlp-architect/examples/intent_extraction/README.md
$MYGIT/NervanaSystems/nlp-architect/examples/memn2n_dialogue/README.md
$MYGIT/NervanaSystems/nlp-architect/examples/most_common_word_sense/README.md
$MYGIT/NervanaSystems/nlp-architect/examples/ner/README.md
$MYGIT/NervanaSystems/nlp-architect/examples/np2vec/README.md
$MYGIT/NervanaSystems/nlp-architect/examples/np_semantic_segmentation/README.md
$MYGIT/NervanaSystems/nlp-architect/examples/reading_comprehension/README.md
$MYGIT/NervanaSystems/nlp-architect/examples/sparse_gnmt/README.md
$MYGIT/NervanaSystems/nlp-architect/examples/supervised_sentiment/README.md
$MYGIT/NervanaSystems/nlp-architect/examples/word_language_model_with_tcn/README.md

* Racket News
https://racket-news.com/

* Keep reading this until I understand it all
$MYGIT/mullikine/mullikine.github.io/practical-macros-in-racket-and-how-to-work-with-them.org

* code in per regex
https://perldoc.perl.org/perlretut.html#A-bit-of-magic%253a-executing-Perl-code-in-a-regular-expression

* racket
#+BEGIN_SRC sh
  nvt x -cd "$(pwd)" -sh "racket -iI racket" -e ">" -s "(permutations '(1 2))" -c m -i
#+END_SRC

* This is most likely worth reading
https://stackoverflow.com/questions/1132042/in-perl-how-can-i-get-the-matched-substring-from-a-regex

* This looks like a good into to problog
https://problog.readthedocs.io/en/latest/modeling_basic.html

* This is good to read because it's a counter argument to pipes
http://metamodular.com/Common-Lisp/lispos.html

* Easy racket macro reading
$HOME/notes2018/ws/racket/examples/racket-macros.rkt
Read this.
Move onto more complicated racket macros / transformers.

* Read this to understand it
https://docs.racket-lang.org/reference/syntax-model.html#%28tech._syntax._transformer%29

* This is very interesting
https://treeregexlib.github.io/

* Log probability
https://en.wikipedia.org/wiki/Log_probability

* Clojure
** Consider learning this
https://purelyfunctional.tv/guide/clojure-concurrency/

* Characterizing secret leakage in public GitHub repositories
https://blog.acolyer.org/2019/04/08/how-bad-can-it-git-characterizing-secret-leakage-in-public-github-repositories/?fbclid=IwAR22sVqSeJq6_hTcRK2-Kwsnz-FkFWxn9GqL-GCRYCD5M6O8fNXq1j2Tuq0

* Read
https://towardsdatascience.com/how-to-create-data-products-that-are-magical-using-sequence-to-sequence-models-703f86a231f8
https://towardsdatascience.com/semantic-code-search-3cd6d244a39c

** High Dimensional Probability
A typical graduate probability course is no longer sufficient to acquire
the level of mathematical sophistication that is expected from a
beginning researcher in data sciences today. The proposed book intends
to partially cover this gap. It presents some of the key probabilistic
methods and results that should form an essential toolbox for a
mathematical data scientist. This book can be used as a textbook for a
basic second course in probability w

$DUMP$HOME/notes2018/ws/probability/HDP-book.pdf

* 
http://news.mit.edu/2019/teaching-machines-to-reason-about-what-they-see-0402

* [#A] This is really good
http://www.happylearnhaskelltutorial.com/1/types_jigsaw.html#s3

* [#B] Really good read on probabilitic inference
http://deepdive.stanford.edu/inference
* 
https://en.wikipedia.org/wiki/Probabilistic_context-free_grammar

https://medium.com/@giacaglia/transformers-141e32e69591?fbclid=IwAR2XGP_CaBRh7_FnlMPYsOMDlLQ--xHtjtpuaZV7Qr7fWYkBSyEfTtE600I

* Haskell one-liners
https://wiki.haskell.org/Blow_your_mind

* Haskell Classes
https://www.haskell.org/tutorial/classes.html

* Learning Explanatory Rules from Noisy Data
$HOME/notes2018/ws/inductive-logic-programming-ilp/1711.04574.txt

* Read
https://skymind.ai/wiki/attention-mechanism-memory-network
This is a good read.

https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html#attention-mechanisms

* Read
** Haskell
http://www.happylearnhaskelltutorial.com/

** Racket
http://www.realmofracket.com/

* This is cool. Read it
http://blog.fogus.me/2011/06/03/10-haskell-one-liners-to-impress-your-friends/

* PyTorch
https://towardsdatascience.com/pytorch-vs-tensorflow-spotting-the-difference-25c75777377b

* NLP common tasks
https://www.analyticsvidhya.com/blog/2017/10/essential-nlp-guide-data-scientists-top-10-nlp-tasks/

* [#A] Very good blog on Deep Learning concepts
http://ruder.io/optimizing-gradient-descent/index.html#adam

If I get this knowledge in me then I should be able to do deep learning with PyTorch.

* Read
https://code.fb.com/developer-tools/getafix-how-facebook-tools-learn-to-fix-bugs-automatically/
https://medium.com/@martin.monperrus/human-competitive-patches-in-automatic-program-repair-with-repairnator-359042e00f6a

* From 2015
https://devblogs.nvidia.com/understanding-natural-language-deep-neural-networks-using-torch/

* This is a really good resource
http://zvon.org/other/haskell/Outputprelude/map_f.html

* Learn more jq
https://unix.stackexchange.com/questions/433678/merging-many-json-files-into-one

* download repos with graphql
https://stackoverflow.com/questions/47458143/how-to-download-github-repositories-via-graphql-api-search?rq=1

* Startup scripts -- great read
https://blog.flowblok.id.au/2013-02/shell-startup-scripts.html

* Perl filter capture group through shell command
https://perlmaven.com/regex-superpowers-execute-code-in-substitution

But how do I filter every capture group through a single shell command?

I probably have to use awk instead.

11:21 < Grinnz> see https://perldoc.pl/perlop#qx/STRING/

* Read to improve my understanding of how to use DL on NLP
https://nlpoverview.com/

* IMPORTANT Read this
https://help.github.com/articles/searching-code/

* [#A] Preprocessing for NLP
https://mlwhiz.com/blog/2019/01/17/deeplearning_nlp_preprocess/

* Know all of these techniques
Take notes.

#+BEGIN_SRC sh
  eww "https://heartbeat.fritz.ai/the-7-nlp-techniques-that-will-change-how-you-communicate-in-the-future-part-ii-636ab06da258?fbclid=IwAR3-lO2fKPURk7jOks5On6hgjHxl1hnQcmvbN15uvOlHukqQPZK1OTdSxlw"
#+END_SRC

* Reading now
https://blog.adamretter.org.uk/haskell-io-and-xpath/

* Hacker's guide to Neural Networks
http://karpathy.github.io/neuralnets/

* Read
https://wiki.haskell.org/Hitchhikers_guide_to_Haskell

* Just read this
https://github.com/MaximAbramchuck/awesome-interview-questions#c

* Perl
** Default variable
https://perlmaven.com/the-default-variable-of-perl

** Gold mine
https://blogs.oracle.com/linux/the-top-10-tricks-of-perl-one-liners-v2

** Perl 5 and 6
https://perlgeek.de/en/article/5-to-6

* Language design
http://lampwww.epfl.ch/~doeraene/thesis/doeraene-thesis-2018-cross-platform-language-design.pdf

If I want to become a professional language designer, read this.

* Read this
https://haskell-lang.org/library/http-client

* Amazing!
$MYGIT/tech-srl/code2vec

* Definitely read this!
https://developer.ibm.com/code/2016/12/08/rosie-pattern-language-qa/

** And this
$HOME/source/git/rosie-pattern-language/rosie/doc/i-know-regex.md

* This is important to read 
https://wiki.haskell.org/How_to_read_Haskell

* awk records
https://stackoverflow.com/questions/40952412/awk-printing-the-second-to-last-record-of-a-file

* continuation
https://docs.racket-lang.org/guide/conts.html

* Read the crash course
https://docs.racket-lang.org/syntax-parse-example/index.html

* Read this -- I need automatic keyphrase extraction functions set up
$HOME/notes2018/ws/nlp-natural-language-processing/reading/ed3book.txt

** Information extraction
#+BEGIN_SRC sh
  vim +/"I am the very model of a modern Major-General," "$HOME/notes2018/ws/nlp-natural-language-processing/reading/ed3book.txt"
#+END_SRC

I should definitely set up some things. There must be extisting environments set up for this.

[[gr:nlp development environment]]

* [#A] Read
eww -x "https://erikbern.com/2018/10/08/the-hackers-guide-to-uncertainty-estimates.html"

* Williams-Dissertation.pdf
$HOME/notes2018/ws/william-palet/Williams-Dissertation.pdf

* Read
https://gluon.mxnet.io/chapter09_natural-language-processing/tree-lstm.html

* TODO
[[/home/shane/dump/home/shane/notes2018/projects/ir-assignment-2/neural-information-retrieval.pdf][ir-assignment-2/neural-information-retrieval.pdf]]

* TODO
http://blog.aylien.com/a-review-of-the-recent-history-of-natural-language-processing/

* Graph convolutional networks
http://tkipf.github.io/graph-convolutional-networks/

* read
http://lambda-the-ultimate.org/node/5517

* Definitely read this
https://github.com/hemanth/functional-programming-jargon

* 
[[http://www.offconvex.org/2018/09/18/alacarte/][Simple and efficient semantic embeddings for rare words, n-grams, and language features - Off the convex path]]

* Must read
https://githubengineering.com/towards-natural-language-semantic-code-search/

* Mining massive datasets
http://infolab.stanford.edu/~ullman/mmds/book.pdf

* python
https://realpython.com/python-f-strings/#old-school-string-formatting-in-python

http://gorefactor.org/doc.html

* read -- emacs stuff
https://stackoverflow.com/questions/22837135/how-to-search-for-the-nth-occurence-of-a-pattern-in-emacs

* racket blogs
https://blog.jverkamp.com/2015/05/07/tuppers-self-referential-formula/

* haskell
https://wiki.haskell.org/Memoization

* TODO [#A] This looks like a good book
https://beautifulracket.com/

* Learn to extend languages
[[https://beautifulracket.com/jsonic/the-tokenizer.html][Beautiful Racket: Extend a data format: jsonic]]

* Animal language
https://en.wikipedia.org/wiki/Animal_language

* TODO [#A] Lots of important tabs here
[[/home/shane/notes2018/ws/tabs/13.09.18.txt][tabs/13.09.18.txt]]

* TODO [#A] Definitely read this
https://wiki.haskell.org/All_About_Monads

** Looks like a good article
[[https://cacm.acm.org/magazines/2018/3/225475-a-programmable-programming-language/fulltext][A Programmable Programming Language | March 2018 | Communications of the ACM]]

* cpp

** CPP
[[https://www.fluentcpp.com/2018/08/31/modern-cpp-fake-it-until-you-have-it/][Modern C++: 7 Ways to Fake It Until You Have It - Fluent C++]]

* Is it worth reading this? PixelGAN

** Maybe I should just stick to quick youtube videos to get the gist of different architectures, and to learn the theory slowly, while solidly learning functional programming

One thing is for sure, I need to be building languages for the sake of creating editors.

On top of this, I have to be generating code. I'll probably generate code from pre-existing models and APIs that I find on the internet.

[[https://arxiv.org/abs/1706.00531][arxiv.org/abs/1706.00531]]

* Scheme
[[/home/shane/dump/home/shane/notes2018/ws/scheme/Harold-Abelson_-Gerald-Jay-Sussman_-Julie-Sussman-Structure-and-interpretation-of-computer-programs-MIT-1996.pdf][Structure and Interpretation of Computer Programs pdf]]

* [#A] racket
[[https://docs.racket-lang.org/guide/languages.html][17 Creating Languages]]

Go through this slowly and do not stop reading it until I understand it all.

* NLP
Force myself to read through some of these. NLP and IR are literally the future of programming so I should maximise study on high-quality articles.
The rest of my learning should be towards racket.

[[https://hanxiao.github.io/2018/01/10/Build-Cross-Lingual-End-to-End-Product-Search-using-Tensorflow/#symbolic-vs-neural-ir-system][Building Cross-Lingual End-to-End Product Search with Tensorflow  Han Xiao Tech Blog - Deep Learning, Tensorflow, Machine Learning and more!]]

* Keep wondering through the racket documentation, clicking on syntactic forms and making a note of what they do
[[/home/shane/notes2018/ws/racket/blog.org][racket/blog.org]]

[[/home/shane/dump/home/shane/notes2018/ws/query-compiler/tahboub-sigmod18.pdf][query-compiler/tahboub-sigmod18.pdf]]

* TODO [#A] Read this entire blog
[[http://www.pinksquirrellabs.com/blog/2017/01/14/programming-in-racket-1/][Programming in Racket #1]]

** TODO [#A] Well, it's written from doing this, so I should do this
[[http://docs.racket-lang.org/aoc-racket/][Advent of Code: solutions & explanations]]

*** Read this to -- nice description of how the functions were chosen to solve the problems
[[/home/shane/var/smulliga/source/git/ruliana/racket-advent-of-code-2017/README.md][racket-advent-of-code-2017/README.md]]

* Need to make sure I know how to write amazing snippets
[[/home/shane/notes2018/ws/yasnippet/yasnippet-writing-snippets.txt][yasnippet/yasnippet-writing-snippets.txt]]

* It's extremely important for me to be reading this documentation
Hurry up.
[[https://docs.racket-lang.org/htdp-langs/beginner.html#%28part._beginner._.Pre-.Defined._.Variables%29][1 Beginning Student]]

* 3d graphics
http://www.codinglabs.net/article_world_view_projection_matrix.aspx

* kefin
https://kevin.stravers.net/2017/11/practical-macros-in-racket-and-how-to-work-with-them.html

* 
https://www.scientificamerican.com/article/this-ultrahot-exoplanet-has-metallic-skies/
https://medium.com/@jcowles/siggraph-2018-papers-ee2bb1be9050

* racket
https://docs.racket-lang.org/guide/scripts.html
http://docs.racket-lang.org/guide/parameterize.html?q=parameter#%28tech._parameter%29

* haskell
[[/home/shane/notes2018/ws/haskell/techniques.org][haskell/techniques.org]]

* racket
** mocking
[[https://docs.racket-lang.org/mock/mock-guide.html#%28part._.Introduction_to_.Mocks%29][1 The Mock Guide]]

* scripty scribble
[[/var/smulliga/source/git/mullikine/scripty/scripty-doc/scribblings/scripty.scrbl][scribblings/scripty.scrbl]]

* emacs syntax highlighting
http://www.modernemacs.com/post/advanced-syntax/

* Learn racket from this
https://artyom.me/learning-racket-1

* Wow
** this is cool
[[/home/shane/notes2018/ws/racket/read/languages-the-racket-way.txt][read/languages-the-racket-way.txt]]
[[/home/shane/dump/home/shane/notes2018/ws/racket/read/languages-the-racket-way.pdf][read/languages-the-racket-way.pdf]]
[[http://users.eecs.northwestern.edu/~stamourv/papers/languages-the-racket-way.pdf][users.eecs.northwestern.edu/~stamourv/papers/languages-the-racket-way.pdf]]

* This is a book I am currently going through
[[https://beautifulracket.com/][Beautiful Racket by Matthew Butterick]]

* ANCHORTOP
** TODO [#A] This is an excellent and easy to read blog explaining how Alexia built hackett. I have to read this
https://lexi-lambda.github.io/blog/2018/04/15/reimplementing-hackett-s-type-language-expanding-to-custom-core-forms-in-racket/

** Recursion schemes
[[https://blog.sumtypeofway.com/an-introduction-to-recursion-schemes/][An Introduction to Recursion Schemes]]

** Infographics on differences between languages
[[http://hyperpolyglot.org/lisp][Lisp: Common Lisp, Racket, Clojure, Emacs Lisp - Hyperpolyglot]]
[[http://hyperpolyglot.org/data][Relational Data Tools: SQL, Awk, Pig - Hyperpolyglot]]

** TODO [#A] amazing -- my first racket langauge
http://belph.github.io/racket/programming/guides/2015/05/07/racket-lang-for-idiots.html

** Read this
I trust that if Alexia King wrote this, it will be a no-bs, concise read and I should try to learn it all.
http://docs.racket-lang.org/hackett/index.html

** 
[[https://github.com/mullikine/autodidact][GitHub - mullikine/autodidact: A pedagogical implementation of Autograd]]

** advanced sed
https://stackoverflow.com/questions/12833714/the-concept-of-hold-space-and-pattern-space-in-sed
https://www.gnu.org/software/sed/manual/html_node/Multiline-techniques.html#Multiline-techniques

** ANCHORTOP 
*** The emacs manual is actually, very much worth learning
[[https://www.gnu.org/software/emacs/manual/html_node/elisp/Mapping-Functions.html][Mapping Functions - GNU Emacs Lisp Reference Manual]]

Think of how I remembered the let* when I needed it.

*** 1 hour a day do some reading from the
[[/home/shane/notes2018/ws/plans/reading.org][plans/reading.org]]

*** [[/home/shane/notes2018/reading-list.org][notes2018/reading-list.org]]

*** YouTube subs are good to read
[[/home/shane/dump/home/shane/notes2018/ws/youtube/subs][youtube/subs]]

*** Racket macros
http://www.greghendershott.com/fear-of-macros/

** Read people talking about how they are applying tensorflow

#+BEGIN_SRC sh
  yt-subs.sh "https://www.youtube.com/watch?v=F_uuqfgdZZw&t=1392s" | ca | v
#+END_SRC

** uni
[[/home/shane/dump/home/shane/notes2018/uni/cosc/412/resources/Discrete_Probability.pdf][resources/Discrete_Probability.pdf]]

** [#A] IMPORTANT
*** [[/home/shane/dump/home/shane/notes2018/ws/machine-learning/reading/machine-learning-cheat-sheet.pdf][reading/machine-learning-cheat-sheet.pdf]]

*** [#A] 
[[http://matt.might.net/articles/higher-order-list-operations/][Higher-order list operations in Racket and Haskell]]

** 
http://pachyderm.readthedocs.io/en/stable/getting_started/beginner_tutorial.html

** Read about all the neural networks in my wallpaper

** parlai

** TODO [#A] read
[[https://markkarpov.com/post/lisp-and-haskell.html][Lisp and Haskell]]

** Books
*** [[/home/shane/dump/home/shane/notes2018/ws/nlp/goldberg_2017_book_draft_20170123.pdf][nlp/goldberg_2017_book_draft_20170123.pdf]]

*** [[http://www.paulgraham.com/onlisp.html][On Lisp]]
[[http://www.paulgraham.com/onlisptext.html][Download]]

** Understand the math
https://stats.stackexchange.com/questions/253244/gradients-for-skipgram-word2vec

** [#A] Read this thoroughly

[[http://www.asimovinstitute.org/neural-network-zoo/][The Neural Network Zoo - The Asimov Institute]]

Be able to talk about all of these neural networks.

*** misc

https://github.com/philkuz/Neural-Network-Zoo

** fast.ai
I should read this daily.

Blog about the things I have read.

http://www.fast.ai/

** good read
[[https://jamesmccaffrey.wordpress.com/2013/11/05/why-you-should-use-cross-entropy-error-instead-of-classification-error-or-mean-squared-error-for-neural-network-classifier-training/][Why You Should Use Cross-Entropy Error Instead Of Classification Error Or Mean Squared Error For Neural Network Classifier Training | James D. McCaffrey]]

** TODO [#A] This is not the first time I have found really good articles from this site
This guy is very good at emacs. To get to the level I need to get to, I have to follow this guy's blog.

*** Holy cow, did he create scimax? -- Yes, it's the same guy. John Kitchin.
https://github.com/jkitchin/jmax
https://github.com/jkitchin/scimax

*** https://kitchingroup.cheme.cmu.edu/blog/2015/09/27/Upping-my-Emacs-navigation-game/
*** http://kitchingroup.cheme.cmu.edu/blog/2015/07/28/A-highlight-annotation-mode-for-Emacs-using-font-lock/
*** http://kitchingroup.cheme.cmu.edu/blog/category/hylang/2/
*** http://kitchingroup.cheme.cmu.edu/blog/2014/09/28/Editing-org-mode-python-source-blocks-in-an-external-editor-Canopy/
*** http://kitchingroup.cheme.cmu.edu/blog/2016/03/31/More-on-Hy-and-why-I-think-it-is-a-big-deal/
*** [#A] http://kitchingroup.cheme.cmu.edu/blog/2015/01/24/Anatomy-of-a-helm-source/
This is amazing! I should definitely read more.
*** http://kitchingroup.cheme.cmu.edu/blog/2015/02/02/helm-actions-when-there-is-no-match/
*** http://kitchingroup.cheme.cmu.edu/blog/2018/05/14/f-strings-in-emacs-lisp/
*** http://kitchingroup.cheme.cmu.edu/blog/2015/01/04/Redirecting-stderr-in-org-mode-shell-blocks/
*** https://kitchingroup.cheme.cmu.edu/blog/category/helm/

** [[http://craftinginterpreters.com/][Crafting Interpreters]]

https://github.com/munificent/craftinginterpreters
[[/var/smulliga/source/git/munificent/craftinginterpreters][munificent/craftinginterpreters]]

Read this. Learn to create my own language.
This will also teach me c.

http://craftinginterpreters.com/a-map-of-the-territory.html

[[/var/smulliga/source/git/munificent/craftinginterpreters/c/][craftinginterpreters/c]]

** I should be looking for terminal programs to help me find definitions of arbitrary things from the internet

Can I get emacs to show me the definitions of things as I am reading?

** Read through everything in here, creating definitions

[[/home/shane/notes2017/ws/two-minute-papers][ws/two-minute-papers]]

** [#A] I should follow this blog until I know moa inside out
https://moa.cms.waikato.ac.nz/blog/

This should also be on the front page of my blog somewhere.

* evil mode
[[https://write.as/notmyfirslanguage/the-road-to-emacs][The Road to Emacs  notmyfirslanguage]]

* Keras
https://github.com/Microsoft/computerscience/blob/master/Labs/Content%20in%20draft/Keras/Keras.md

* blog about convolutional neural networks
[[https://mohitjain.me/2018/06/09/googlenet/][Paper Explanation: Going Deeper with Convolutions (GoogLeNet)  Mohit Jain]]

* Nice academic paper about why lisp is great

[[https://arxiv.org/abs/1608.02621][The Machine that Builds Itself: How the Strengths of Lisp Family Languages Facilitate Building Complex and Flexible Bioinformatic Models]]

We address the need for expanding the presence of the Lisp family of programming languages in bioinformatics and computational biology research. Languages of this family, like Common Lisp, Scheme, or Clojure, facilitate the creation of powerful and flexible software models that are required for complex and rapidly evolving domains like biology. We will point out several important key features that distinguish languages of the Lisp family from other programming languages and we will explain how these features can aid researchers in becoming more productive and creating better code. We will also show how these features make these languages ideal tools for artificial intelligence and machine learning applications. We will specifically stress the advantages of domain-specific languages (DSL): languages which are specialized to a particular area and thus not only facilitate easier research problem formulation, but also aid in the establishment of standards and best programming practices as applied to the specific research field at hand. DSLs are particularly easy to build in Common Lisp, the most comprehensive Lisp dialect, which is commonly referred to as the "programmable programming language." We are convinced that Lisp grants programmers unprecedented power to build increasingly sophisticated artificial intelligence systems that may ultimately transform machine learning and AI research in bioinformatics and computational biology.

* 
[[/home/shane/notes2018/ws/big-data/learn.org][big-data/learn.org]]

* 
https://m.techxplore.com/news/2018-06-breakthrough-algorithm-exponentially-faster-previous.html

* When I have the time
[[/home/shane/notes2018/ws/apache-spark/learn.org][apache-spark/learn.org]]

* OpenAI
** [[https://blog.openai.com/language-unsupervised/][Improving  Language Understanding with Unsupervised Learning]]

See also:
- Elmo

** [[https://blog.openai.com/learning-montezumas-revenge-from-a-single-demonstration/][Learning Montezuma's Revenge from a Single Demonstration]]

*** Why Exploration is Difficult
Model-free RL methods like policy gradients and Q-learning explore by taking actions randomly. If, by chance, the random actions lead to a reward, they are reinforced, and the agent becomes more likely to take these beneficial actions in the future. This works well if rewards are dense enough for random actions to lead to a reward with reasonable probability. However, many of the more complicated games require long sequences of very specific actions to experience any reward, and such sequences are extremely unlikely to occur randomly.

* 
https://einstein.ai/research/the-natural-language-decathlon

* [#A] [[/home/shane/notes2018/ws/hy/Writing-hy-code-from-hy-code.org][Writing-hy-code-from-hy-code.org]]

* [#A] [[https://cs.uwaterloo.ca/~plragde/flaneries/FDS/][Functional Data Structures]]

* https://ntirawen.blogspot.com/2018/06/linear-regression.html

* [[https://blog.acolyer.org/2018/06/19/debugging-with-intelligence-via-probabilistic-inference/][Debugging with intelligence via probabilistic inference | the morning paper]]

* Absolutely excellent blog
http://planet.emacsen.org/

* Get to know OAuth2

http://www.bubblecode.net/en/2016/01/22/understanding-oauth2/

Then I can claim I know it.

Start a reading list which others can see.
Maybe this is simply a blog.

* [#A] Read this!!

Awesome guide, so much information. Read!

[[http://www.r2d3.us/visual-intro-to-machine-learning-part-1/][A VISUAL INTRODUCTION TO MACHINE LEARNINGâ€”PART I]]
[[http://www.r2d3.us/visual-intro-to-machine-learning-part-2/][A VISUAL INTRODUCTION TO MACHINE LEARNINGâ€”PART II]]

* Learn haskell
https://learnxinyminutes.com/docs/haskell/

* LFE Lisp Flavored Erlang

Set this up.

https://lfe.gitbooks.io/quick-start/content/index.html

* [#A] Algorithm complexity analysis

http://discrete.gr/complexity/

* TODO [#A] Elisp - I should gobble all of this up
[[http://ergoemacs.org/emacs/elisp.html][Practical Emacs Lisp]]

* [#A] Intermediate -- great resource
[[/var/smulliga/source/git/ClojureBridge/intermediate-clojure][intermediate-clojure]]

* Read

Not only is it possible. It's very well supported.

[[http://kieranbrowne.com/research/clojure-tensorflow-interop/][Running TensorFlow in Clojure  Kieran Browne]]

** [#A] In fact, I should start learning it now
I can learn tensorflow with clojure as I am learning clojure.

* Learn all the functions in here
** Correct place
+ Inside the .jar file

/home/shane/.m2/repository/org/clojure/clojure/1.8.0/clojure-1.8.0.jar
  /clojure/core.clj

** Incorrect place

#+BEGIN_SRC emacs-lisp
  ;; Run this in spacemacs repl
  (cider-doc-lookup "clojure.core")
#+END_SRC

#+BEGIN_SRC sh
  sp -e "(cider-doc-lookup \"clojure.core\")"
#+END_SRC

* Probability
https://rdipietro.github.io/friendly-intro-to-cross-entropy-loss/

* Reinforcement learning

This covers some good theory. It's a great way to learn.

https://worldmodels.github.io/

* Haskell -- Learn to read it
https://wiki.haskell.org/How_to_read_Haskell#General_advice

* 
https://docs.google.com/document/d/1x_REsTCQPL3K0N-qrwASaXPBAN92KQFuc8VtTwDfWYc/edit
https://docs.google.com/document/d/1iDmlZl0g9WVzbIErR40pjo6pFw9BOtj3r7CdojelRpo/edit

* Study for exam
$HOME/notes2018/uni/cosc/420_Neural Networks_S1/lectures/L1.org
$HOME/notes2018/uni/cosc/420_Neural Networks_S1/lectures/L3.org
$HOME/notes2018/uni/cosc/420_Neural Networks_S1/lectures/L4.org
$HOME/notes2018/uni/cosc/420_Neural Networks_S1/lectures/L5.org
$HOME/notes2018/uni/cosc/420_Neural Networks_S1/lectures/L6.org
$HOME/notes2018/uni/cosc/420_Neural Networks_S1/lectures/L7.org
$HOME/notes2018/uni/cosc/420_Neural Networks_S1/lectures/L8.org
$HOME/notes2018/uni/cosc/420_Neural Networks_S1/lectures/chap-10.org

* Lisp
$DUMP$HOME/notes2018/ws/lisp/books/Land of Lisp - Barski M.D., Conrad.pdf
$DUMP$HOME/notes2018/ws/lisp/books/Practical-COMMON-LISP.pdf

* Clojure

[[https://bsima.me/clog/robust-clojure-nil.html][Robust Clojure: The best way to handle nil : bsima.me]]

* Research papers
** Learning to Optimize Tensor Programs
$DUMP$HOME/notes2018/ws/tensor-programming/papers/1805.08166.pdf

If I used hacker news then I can review them really well because I can look at the comments.

I should then search for papers that have appeared on hacker news.

https://news.ycombinator.com/item?id=17128055

* [#A] Read these kinds of things
https://developers.google.com/machine-learning/rules-of-ml/

The thing about getting my reading from Google is I will learn the correct terminology for things, that have been discovered by Google's algorithms to be the correct terminology.
Until we have blockchain for language, this is the best way.
* AST transformations

** Learn
#+BEGIN_EXAMPLE
evil-smartparens
#+END_EXAMPLE

* [#A] Excellent books to read. Read them all
$DUMP$HOME/notes2018/ws/machine-learning/books

* 
[[http://norvig.com/python-lisp.html][Python for Lisp Programmers]]

* Read on emacs lisp
** [#A] Good, short read
*** Multiple optional arguments as a list using &rest
https://writequit.org/denver-emacs/presentations/2016-01-26-elisp-projectile-eshell.html

*** Multiple optional arguments as individual arguments following a single &optional
http://ergoemacs.org/emacs/elisp_optional_params.html

** 
https://caiorss.github.io/Emacs-Elisp-Programming/Elisp_Snippets.html

* [#A] Tensor programming
$HOME/notes2018/ws/tensor-programming/links.org

* [#A] Debugging lisp
http://malisper.me/debugging-lisp-part-1-recompilation/

* [#A] Design a language using Racket (Scheme)
https://docs.racket-lang.org/guide/languages.html

* [#A] Important to read for learing to optimise my indexer
http://leto.net/docs/C-optimization.php

* [#B] Read
$MYGIT/soupi/rfc/Todo.hs
$MYGIT/soupi/rfc/getting-started-haskell-stack.md
$MYGIT/soupi/rfc/reading_haskell.md
$MYGIT/soupi/rfc/writing_simple_haskell.md

* [#A] Easy/Simplified Reference
** This is amazing, study it
http://jtra.cz/stuff/lisp/sclr/index.html
http://jtra.cz/stuff/lisp/sclr/allprint.html

* Everything in the terminal

If I can do more in emacs then I will feel less crazy deciding on things
to learn, such as on youtube. I don't really want to watch so many
partial videos. I want to complete more videos. That is my theory.

* Execellent and simple slides on quantum cryptography by Artur Ekert
[[$DUMP$HOME/notes2018/ws/quantum-cryptography/quantum_zak_bertrand.pdf][quantum_zak_bertrand.pdf]]

Amazing list of references at the end.

* Artur Ekert
A British-Polish professor of quantum physics at the Mathematical Institute, University of Oxford