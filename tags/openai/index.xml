<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>openai on Bodacious Blog</title>
    <link>https://mullikine.github.io/tags/openai/</link>
    <description>Recent content in openai on Bodacious Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 14 Apr 2021 00:00:00 +1200</lastBuildDate><atom:link href="https://mullikine.github.io/tags/openai/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>GPT-3 is The Thing</title>
      <link>https://mullikine.github.io/posts/gpt-3-like-the-thing/</link>
      <pubDate>Wed, 14 Apr 2021 00:00:00 +1200</pubDate>
      
      <guid>https://mullikine.github.io/posts/gpt-3-like-the-thing/</guid>
      <description>Summary The Thing is a lot like GPT-3 due to it being extremely capable, but limited to the queries you give it.
It&amp;rsquo;s as powerful as your understanding on how to talk to it.
The Thing is a black box computer in the nomes series by Terry Pratchett.
I make reference to the first book in the series Truckers.
This book is freely available on the Internet Archive.
 Truckers by Terry Pratchett https://archive.</description>
    </item>
    
    <item>
      <title>Imaginary programming with GPT-3</title>
      <link>https://mullikine.github.io/posts/imaginary-programming-with-gpt-3/</link>
      <pubDate>Thu, 08 Apr 2021 00:00:00 +1200</pubDate>
      
      <guid>https://mullikine.github.io/posts/imaginary-programming-with-gpt-3/</guid>
      <description>Code https://github.com/semiosis/pen.el Prompts https://github.com/semiosis/prompts/ Disclaimer Please contribute as this is an open source project! It&amp;rsquo;s very hard to find free prompts online currently and that&amp;rsquo;s because everyone is out for themselves. Please support open source. Thank you.  Summary This is a demonstration of an imaginary programming environment. There may be nothing else like it in the world today.
What does it mean to be imaginary? Several of the components of a normal programming environment are replaced by functions that infer rather than evaluate.</description>
    </item>
    
    <item>
      <title>Fictional statements of remorse with GPT-3 in the 1st and 3rd person</title>
      <link>https://mullikine.github.io/posts/fictional-statements-of-remorse-with-gpt-3/</link>
      <pubDate>Wed, 07 Apr 2021 00:00:00 +1200</pubDate>
      
      <guid>https://mullikine.github.io/posts/fictional-statements-of-remorse-with-gpt-3/</guid>
      <description>Summary I use GPT-3 to generate fictional statements of remorse.
It should be noted that this is only one such way that GPT-3 will upheave legal process.
RemorseBot (in the 1st person)  --  RemorseBot (in the 3rd person)  --  Prompts 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108  title: &amp;#34;Statement of remorse&amp;#34; # future-titles: &amp;#34;&amp;#34; # aims: |+ # - More abstractive rewording doc: &amp;#34;&amp;#34; # aims: |+ # - More abstractive rewording prompt-version: 1 # &amp;lt;:pp&amp;gt; defines a point where the following # text is concatenated before the postprocessor # is run.</description>
    </item>
    
    <item>
      <title>Translating with GPT-3 and Emacs</title>
      <link>https://mullikine.github.io/posts/translating-with-gpt-3-and-emacs/</link>
      <pubDate>Tue, 06 Apr 2021 00:00:00 +1200</pubDate>
      
      <guid>https://mullikine.github.io/posts/translating-with-gpt-3-and-emacs/</guid>
      <description>Summary Sorry for the lazy blog post today. I just ask GPT3 for some subtopics of ancient roman law as I am looking for a cool word to use. I would like to know what these words mean, so I use GPT3 for that too.
Subtopics of Ancient Roman Law These were generated by GPT-3.
1 2 3 4 5  aedilitas advocatus auctoritas augur auspex caupona cena clientela contio domus ius ludos ministra mos ora otium praetor quaestio res mancipi sacerdos status suovetaurilia tabella tribunus plebis via vir    --  GPT-3 Language detection and translation  language Latin  English translation:</description>
    </item>
    
    <item>
      <title>GPT-3 mind maps with an AI tutor for any topic</title>
      <link>https://mullikine.github.io/posts/gpt-3-for-building-mind-maps-with-an-ai-tutor-for-any-topic/</link>
      <pubDate>Mon, 29 Mar 2021 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/gpt-3-for-building-mind-maps-with-an-ai-tutor-for-any-topic/</guid>
      <description>Code http://github.com/semiosis/pen.el Prompts http://github.com/semiosis/prompts  Summary I combine GPT-3 with org-brain to expand on topics, suggesting subtopics and providing an interactive tutor for any topic.
Demonstration Subtopic generation I demonstrate how to explore arbitrary topics with GPT-3 by automatically generating subtopics, and then allowing you to invoke the GPT-3 tutor to answer questions within that context.
 --  Tutor demonstration  Rolling conversation is a work in progress, but on its way.</description>
    </item>
    
    <item>
      <title>Generating pickup lines with GPT-3</title>
      <link>https://mullikine.github.io/posts/generating-pickup-lines-with-gpt-3/</link>
      <pubDate>Sat, 27 Mar 2021 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/generating-pickup-lines-with-gpt-3/</guid>
      <description>Latest version of the pick up lines prompt http://github.com/semiosis/prompts/blob/master/prompts/very-witty-pick-up-lines.prompt  Summary I create a prompt in my prompt description format and use it to generate some pickup lines.
Demonstration (v2 with emacs counsel integration) New results are fed into a fuzzy finder as they are generated. I can stop and select at any time.
 --  Demonstration (version 2)  --  Pick up lines with the topic &amp;ldquo;SETTLERS OF CATAN&amp;rdquo; 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28  When playing Settlers of Catan, the shortest route is a straight line to my heart.</description>
    </item>
    
    <item>
      <title>Autocompleting anything with GPT-3 in emacs</title>
      <link>https://mullikine.github.io/posts/autocompleting-anything-with-gpt-3-in-emacs/</link>
      <pubDate>Tue, 16 Mar 2021 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/autocompleting-anything-with-gpt-3-in-emacs/</guid>
      <description>Emacs package http://github.com/mullikine/emacs/blob/master/config/pen.el Prompt http://github.com/semiosis/prompts/blob/master/prompts/generic-file-type-completion.prompt  Summary I make a prompt for the OpenAI API which completes given a file type and some preceding text.
I then make a company-mode completion function for it, and then demo its usage.
This gives me a generic completion mechanism when dealing with any type of document.
Demonstration This is GPT-3 completing some text for me.
I can type a few characters and then GPT-3 will complete the rest of the text.</description>
    </item>
    
    <item>
      <title>A tour of Ryan Ong&#39;s - NLP 365</title>
      <link>https://mullikine.github.io/posts/ryan-ong-nlp-365/</link>
      <pubDate>Sat, 13 Mar 2021 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/ryan-ong-nlp-365/</guid>
      <description>Glossaries nlp-natural-language-processing.txt information-retrieval.txt fasttext.txt transformer.txt nmt-neural-machine-translation.txt information-theory.txt  Summary I go over Ryan Ong&amp;rsquo;s series on NLP, adding terms to my glossaries and reproducing what he&amp;rsquo;s done.
 Finished reading &amp;lt;2021-03-13 Sat&amp;gt;
 Day 1: What is Natural Language Processing - Ryan Ong Day 2: Damerau-Levenshtein Distance - Ryan Ong Day 3: Word Embeddings - Ryan Ong  Terms in the green text have added to my glossary.
  This is the project that introduced me to GloVe a few years ago.</description>
    </item>
    
    <item>
      <title>Look for my friend&#39;s wagon using OpenAI CLIP</title>
      <link>https://mullikine.github.io/posts/look-for-my-friend-s-wagon-using-openai-clip/</link>
      <pubDate>Fri, 12 Mar 2021 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/look-for-my-friend-s-wagon-using-openai-clip/</guid>
      <description>Full demonstration I show you how easy it is to search for an arbitrary thing inside of an arbitrary youtube video.
I am blogging and recording as I am demonstrating the technology.
Skip to 3 minutes to see the magic.

Summary I am looking for my friend&amp;rsquo;s wagon in a youtube video.
 This is the video Here&amp;rsquo;s What We REALLY Think Of Your Cars 5 - YouTube  The images of various wagons should appear after I run the command which I have bound in emacs.</description>
    </item>
    
    <item>
      <title>Reading about DALL-E</title>
      <link>https://mullikine.github.io/posts/reading-about-dall-e/</link>
      <pubDate>Fri, 12 Mar 2021 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/reading-about-dall-e/</guid>
      <description>Article https://openai.com/blog/dall-e/ Original paper https://arxiv.org/abs/2102.12092.pdf  Authors of Paper - Aditya Ramesh - Mikhail Pavlov - Gabriel Goh - Scott Gray - Chelsea Voss - Alec Radford - Mark Chen - Ilya Sutskever
Summary of DALL-E from Arxiv 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  Text-to-image generation has traditionally focused on finding better modeling assumptions for training on a fixed dataset.</description>
    </item>
    
    <item>
      <title>Context menus based on GPT-3</title>
      <link>https://mullikine.github.io/posts/context-menus-based-on-gpt-3/</link>
      <pubDate>Tue, 09 Mar 2021 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/context-menus-based-on-gpt-3/</guid>
      <description>Summary I create a GPT-3 prompt for testing to see if code is Haskell and use it as a test inside emacs to suggest further functions.
 Prompt file http://github.com/semiosis/prompts/blob/master/prompts/text-is-haskell.prompt  Demonstration As you can see, GPT-3 is able to detect the language and I can use that as a test in my emacs to provide further functions. The suggested function was yet another GPT-3 prompt function for translating Haskell into Clojure.</description>
    </item>
    
    <item>
      <title>An operating system based on GPT-3</title>
      <link>https://mullikine.github.io/posts/an-operating-system-based-on-gpt-3/</link>
      <pubDate>Mon, 08 Mar 2021 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/an-operating-system-based-on-gpt-3/</guid>
      <description>Reference http://github.com/semiosis/examplary  Summary I assume that GPT-3 or some descendant of it will become the primary interface to the computer.
I create a configuration option to enable / disable GPT-3.
When disabled, the environment will resort to alternative means of performing tasks.
Configuration 1  vim +/&amp;#34;use_gpt3: on&amp;#34; &amp;#34;$NOTES/myrc.yaml&amp;#34;     1  vim +/&amp;#34;summarize) {&amp;#34; &amp;#34;$SCRIPTS/s&amp;#34;   If gpt3 is enabled, filter through OpenAI API abstractive summarizer Otherwise, use sumy.</description>
    </item>
    
    <item>
      <title>A natural language database using a single GPT prompt</title>
      <link>https://mullikine.github.io/posts/a-natural-language-database-using-a-single-gpt-prompt/</link>
      <pubDate>Sun, 07 Mar 2021 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/a-natural-language-database-using-a-single-gpt-prompt/</guid>
      <description>Original article https://www.gwern.net/GPT-3#the-database-prompt  Summary A single prompt describes transactions to and from a database.
GPT-3 is able to answer questions about the transactions that have taken place.
GPT-3 isn&amp;rsquo;t actually a database.
The LM simply understands language so well that describing the transactions that have taken place would naturally lead to the GPT-3 response.
The prompt 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55  title: &amp;#34;database example&amp;#34; doc: &amp;#34;GPT-3 as a NL interface for semantically querying logic in prose&amp;#34; prompt: |+The database begins knowing nothing.</description>
    </item>
    
    <item>
      <title>Translating Haskell to Clojure with GPT-3</title>
      <link>https://mullikine.github.io/posts/translating-haskell-to-clojure-with-gpt-3/</link>
      <pubDate>Sun, 07 Mar 2021 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/translating-haskell-to-clojure-with-gpt-3/</guid>
      <description>Relevant material https://hyperpolyglot.org/ml  Summary Who needs hyperpolyglot when you have GPT-3?
I translate Haskell into Clojure using the following prompt.
haskell-to-clojure.prompt 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50  title: &amp;#34;Translate Haskell to Clojure&amp;#34; prompt: |+Haskell: zip (map show [1,5,9]) [&amp;#34;a&amp;#34;,&amp;#34;b&amp;#34;,&amp;#34;c&amp;#34;] Clojure: (println (map vector &amp;#39;(1 2 3) &amp;#39;(4 5 6))) Haskell: map toUpper &amp;#34;MiXeD cAsE&amp;#34; Clojure: (clojure.</description>
    </item>
    
    <item>
      <title>Fine-tuning GPT-3</title>
      <link>https://mullikine.github.io/posts/fine-tuning-gpt-3/</link>
      <pubDate>Sun, 21 Feb 2021 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/fine-tuning-gpt-3/</guid>
      <description>Notes on GPT-3 fine-tuning http://github.com/mullikine/fine-tuning-gpt-3  Fine-tuning GPT-3 to generate puns Aims Train GPT-3 to continue on sequences of puns Train GPT-3 to speak in puns Training Data    format     jsonl       Sources
 https://github.com/taivop/joke-dataset       Naive approach
1 2 3  [{&amp;#34;data&amp;#34; : &amp;#34;joke set 1&amp;#34;}, {&amp;#34;data&amp;#34;: &amp;#34;joke set 2&amp;#34;}         Better approach</description>
    </item>
    
    <item>
      <title>Creating a playground for GPT-3 in emacs</title>
      <link>https://mullikine.github.io/posts/creating-a-playground-for-gpt-3-in-emacs/</link>
      <pubDate>Thu, 18 Feb 2021 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/creating-a-playground-for-gpt-3-in-emacs/</guid>
      <description>Code https://github.com/mullikine/pen.el Prompts https://github.com/mullikine/prompts meetup.com event https://www.meetup.com/Code-Craft-Dunedin/events/276407816/ Slides http://github.com/mullikine/presentation-prompt-engineering-in-emacs/blob/master/presentation.pdf  Demonstration 
Prompt-Engineering Part 1: Building an environment Summary of talk I received a key for the OpenAI API 2 weeks ago, just a couple of days before my birthday.
I had applied around 5 to 10 times in the last year and I had finally been given a key.
I decided since the hour is late I would work on productivity tools that anyone can use, to facilitate programming in the new paradigm.</description>
    </item>
    
    <item>
      <title>Review of &#39;Search inside YouTube videos using natural language&#39;</title>
      <link>https://mullikine.github.io/posts/review-of-github-haltakov-natural-language-youtube-search-search-inside-youtube-videos-using-natural-language/</link>
      <pubDate>Sun, 14 Feb 2021 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/review-of-github-haltakov-natural-language-youtube-search-search-inside-youtube-videos-using-natural-language/</guid>
      <description>Source code GitHub - haltakov/natural-language-youtube-search: Search inside YouTube videos using natural language Algorithm https://github.com/openai/CLIP.git  Requirements  CPU  Non-requirements  The OpenAI API key is not required to run this.  Summary Takes 10 seconds without a GPU to find a stop sign in a 3 min music video.
Results   The vid Chemical Hearts {2020} Soundtrack.Take Care - BEACH HOUSE - YouTube
Code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135  #!</description>
    </item>
    
    <item>
      <title>OpenAI API for NLP</title>
      <link>https://mullikine.github.io/posts/openai-api-for-nlp/</link>
      <pubDate>Fri, 12 Jun 2020 00:00:00 +1200</pubDate>
      
      <guid>https://mullikine.github.io/posts/openai-api-for-nlp/</guid>
      <description>OpenAI API https://beta.openai.com/  Summary I&amp;rsquo;d like to add some OpenAI support to emacs. GPT-3 is easy to integrate because it is a very general-purpose transformer. In a later article, I&amp;rsquo;d like to integrate more specialised huggingface transformers.
Results Here are some of the things I had managed to do with emacs and GPT-3 so far.
 Imaginary programming with GPT-3 // Bodacious Blog  Here I assemble an imaginary programming environment in GPT-3.</description>
    </item>
    
    <item>
      <title>Rewrite of &#39;Understanding Transformers in NLP: State-of-the-Art Models&#39;</title>
      <link>https://mullikine.github.io/posts/review-of-understanding-transformers-in-nlp-state-of-the-art-models/</link>
      <pubDate>Wed, 10 Jun 2020 00:00:00 +0800</pubDate>
      
      <guid>https://mullikine.github.io/posts/review-of-understanding-transformers-in-nlp-state-of-the-art-models/</guid>
      <description>Original article Understanding Transformers in NLP: State-of-the-Art Models  Table of Contents  Sequence-to-Sequence Models &amp;ndash; A Backdrop  RNN based Sequence-to-Sequence Model Challenges   Introduction to the Transformer in NLP  Understanding the Model Architecture Grokking Self-Attention Calculation of Self-Attention Limitations of the Transformer   Understanding Transformer-XL  Using Transformer for Language Modeling Using Transformer-XL for Language Modeling   The New Sensation in NLP: Google&amp;rsquo;s BERT  Model Architecture BERT Pre-Training Tasks    Sequence-to-Sequence Models &amp;ndash; A Backdrop seq2seq models in NLP are used to convert sequences of Type A to sequences of Type B.</description>
    </item>
    
    <item>
      <title>Review of &#39;Microsoft: &#34;The future of tech, with Kevin Scott and guests // Microsoft Build&#34;&#39;</title>
      <link>https://mullikine.github.io/posts/review-of-microsoft-the-future-of-tech-with-kevin-scott-and-guests-microsoft-build/</link>
      <pubDate>Tue, 12 May 2020 00:00:00 +0800</pubDate>
      
      <guid>https://mullikine.github.io/posts/review-of-microsoft-the-future-of-tech-with-kevin-scott-and-guests-microsoft-build/</guid>
      <description>Original video Microsoft: &amp;ldquo;The future of tech, with Kevin Scott and guests // Microsoft Build&amp;rdquo; Original article News OpenAI Model Generates Python Code - YouTube Original video from microsoft https://www.youtube.com/watch?v=fZSFNUT6iY8  Summary At 29 min you can see a demo of code generation from comments.
This is similar to what deep tabnine currently does, though probably uses a more powerful language model.
GitHub CodeSpaces and VSCode will probably have this built-in.</description>
    </item>
    
    <item>
      <title>(WIP) Notes on &#34;The Illustrated GPT-2&#34;</title>
      <link>https://mullikine.github.io/posts/notes-on-illustrated-gpt2/</link>
      <pubDate>Thu, 24 Oct 2019 00:00:00 +0800</pubDate>
      
      <guid>https://mullikine.github.io/posts/notes-on-illustrated-gpt2/</guid>
      <description>Original article The Illustrated GPT-2 (Visualizing Transformer Language Models) Jay Alammar Visualizing machine learning one concept at a time  Prereading Overview of The Illustrated Transformer // Bodacious Blog
Parameters When an article talks about the number of parameters, this is what it&amp;rsquo;s referring to.
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  Parameters Single Transformer block Conv1d attn/c_attn w 768 2304 1769472 b 2304 2304 attn/c_proj w 768 768 589824 b 768 768 mlp/c_fc w 768 3072 2359296 b 768 768 mlp/c_proj w 3072 3072 2359296 b 768 768 Norm ln_1 g 768 768 b 768 768 ln_2 g 768 768 b 768 768 total 7085568 per block X 12 blocks 85026816 In all blocks Embeddings 50257 768 38597376 Positional Embeddings 2024 768 786432 Grand Total 124410624   Goal Supplement The Illustrated Transformer with more visuals explaining the inner-workings of transformers, and how they’ve evolved since the original paper.</description>
    </item>
    
    <item>
      <title>writeup.ai</title>
      <link>https://mullikine.github.io/posts/review-of-writeup-ai/</link>
      <pubDate>Wed, 16 Oct 2019 00:00:00 +0800</pubDate>
      
      <guid>https://mullikine.github.io/posts/review-of-writeup-ai/</guid>
      <description>Original article https://senrigan.io/blog/how-writeupai-runs-behind-the-scenes/#h%5F6068056784021570782144062  Glossary 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26  cross entropy loss (between two probability distributions) (and over the same underlying set of events) Higher loss is bad. Measures the performance of a classification model whose output is a probability value between 0 and 1. Measures the average number of bits needed to identify an event from the set.</description>
    </item>
    
  </channel>
</rss>
