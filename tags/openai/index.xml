<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>openai on Bodacious Blog</title>
    <link>https://mullikine.github.io/tags/openai/</link>
    <description>Recent content in openai on Bodacious Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 12 Mar 2021 00:00:00 +1300</lastBuildDate><atom:link href="https://mullikine.github.io/tags/openai/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Look for my friend&#39;s wagon using OpenAI CLIP</title>
      <link>https://mullikine.github.io/posts/look-for-my-friend-s-wagon-using-openai-clip/</link>
      <pubDate>Fri, 12 Mar 2021 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/look-for-my-friend-s-wagon-using-openai-clip/</guid>
      <description>Full demonstration 
Summary I am looking for my friend&amp;rsquo;s wagon in a youtube video.
 This is the video Here&amp;rsquo;s What We REALLY Think Of Your Cars 5 - YouTube  The images of various wagons should appear after I run the command which I have bound in emacs.
   kb f      M-l / V find-in-youtube global-map    It&amp;rsquo;s based on code from this github repository.</description>
    </item>
    
    <item>
      <title>Reading about DALL-E</title>
      <link>https://mullikine.github.io/posts/reading-about-dall-e/</link>
      <pubDate>Fri, 12 Mar 2021 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/reading-about-dall-e/</guid>
      <description>Article https://openai.com/blog/dall-e/ Original paper https://arxiv.org/abs/2102.12092.pdf  Authors of Paper - Aditya Ramesh - Mikhail Pavlov - Gabriel Goh - Scott Gray - Chelsea Voss - Alec Radford - Mark Chen - Ilya Sutskever
Summary of DALL-E from Arxiv 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  Text-to-image generation has traditionally focused on finding better modeling assumptions for training on a fixed dataset.</description>
    </item>
    
    <item>
      <title>Context menus based on GPT-3</title>
      <link>https://mullikine.github.io/posts/context-menus-based-on-gpt-3/</link>
      <pubDate>Tue, 09 Mar 2021 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/context-menus-based-on-gpt-3/</guid>
      <description>Summary I create a GPT-3 prompt for testing to see if code is Haskell and use it as a test inside emacs to suggest further functions.
 Prompt file http://github.com/semiosis/prompts/blob/master/prompts/text-is-haskell.prompt  Demonstration As you can see, GPT-3 is able to detect the language and I can use that as a test in my emacs to provide further functions. The suggested function was yet another GPT-3 prompt function for translating Haskell into Clojure.</description>
    </item>
    
    <item>
      <title>Fine-tuning GPT-3</title>
      <link>https://mullikine.github.io/posts/fine-tuning-gpt-3/</link>
      <pubDate>Sun, 21 Feb 2021 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/fine-tuning-gpt-3/</guid>
      <description>Notes on GPT-3 fine-tuning http://github.com/mullikine/fine-tuning-gpt-3  Fine-tuning GPT-3 to generate puns Aims Train GPT-3 to continue on sequences of puns Train GPT-3 to speak in puns Training Data    format     jsonl       Sources
 https://github.com/taivop/joke-dataset       Naive approach
1 2 3  [{&amp;#34;data&amp;#34; : &amp;#34;joke set 1&amp;#34;}, {&amp;#34;data&amp;#34;: &amp;#34;joke set 2&amp;#34;}         Better approach</description>
    </item>
    
    <item>
      <title>Creating a playground for GPT-3 in emacs</title>
      <link>https://mullikine.github.io/posts/creating-a-playground-for-gpt-3-in-emacs/</link>
      <pubDate>Thu, 18 Feb 2021 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/creating-a-playground-for-gpt-3-in-emacs/</guid>
      <description>Code https://github.com/mullikine/pen.el Prompts https://github.com/mullikine/prompts meetup.com event https://www.meetup.com/Code-Craft-Dunedin/events/276407816/ Slides http://github.com/mullikine/presentation-prompt-engineering-in-emacs/blob/master/presentation.pdf  Demonstration 
Prompt-Engineering Part 1: Building an environment Summary of talk I received a key for the OpenAI API 2 weeks ago, just a couple of days before my birthday.
I had applied around 5 to 10 times in the last year and I had finally been given a key.
I decided since the hour is late I would work on productivity tools that anyone can use, to facilitate programming in the new paradigm.</description>
    </item>
    
    <item>
      <title>Review of &#39;Search inside YouTube videos using natural language&#39;</title>
      <link>https://mullikine.github.io/posts/review-of-github-haltakov-natural-language-youtube-search-search-inside-youtube-videos-using-natural-language/</link>
      <pubDate>Sun, 14 Feb 2021 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/review-of-github-haltakov-natural-language-youtube-search-search-inside-youtube-videos-using-natural-language/</guid>
      <description>Source code GitHub - haltakov/natural-language-youtube-search: Search inside YouTube videos using natural language Algorithm https://github.com/openai/CLIP.git  Requirements  CPU  Non-requirements  The OpenAI API key is not required to run this.  Summary Takes 10 seconds without a GPU to find a stop sign in a 3 min music video.
Results   The vid Chemical Hearts {2020} Soundtrack.Take Care - BEACH HOUSE - YouTube
Code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135  #!</description>
    </item>
    
    <item>
      <title>OpenAI API for NLP</title>
      <link>https://mullikine.github.io/posts/openai-api-for-nlp/</link>
      <pubDate>Fri, 12 Jun 2020 00:00:00 +1200</pubDate>
      
      <guid>https://mullikine.github.io/posts/openai-api-for-nlp/</guid>
      <description>OpenAI API https://beta.openai.com/  Related articles I&amp;rsquo;d like to add some OpenAI support to emacs.
  Context menus based on GPT-3 // Bodacious Blog
  crontab.guru in emacs and making a prompt with GPT-3 to copy it // Bodacious Blog
  Translating Haskell to Clojure with GPT-3 // Bodacious Blog
  Creating a playground for GPT-3 in emacs // Bodacious Blog
  API  Apply the API to any language task  semantic search, summarization, sentiment analysis, content generation, translation, and more&amp;hellip;    Use only a few examples or by specifying your task in English.</description>
    </item>
    
    <item>
      <title>Rewrite of &#39;Understanding Transformers in NLP: State-of-the-Art Models&#39;</title>
      <link>https://mullikine.github.io/posts/review-of-understanding-transformers-in-nlp-state-of-the-art-models/</link>
      <pubDate>Wed, 10 Jun 2020 00:00:00 +0800</pubDate>
      
      <guid>https://mullikine.github.io/posts/review-of-understanding-transformers-in-nlp-state-of-the-art-models/</guid>
      <description>Original article Understanding Transformers in NLP: State-of-the-Art Models  Table of Contents  Sequence-to-Sequence Models &amp;ndash; A Backdrop  RNN based Sequence-to-Sequence Model Challenges   Introduction to the Transformer in NLP  Understanding the Model Architecture Grokking Self-Attention Calculation of Self-Attention Limitations of the Transformer   Understanding Transformer-XL  Using Transformer for Language Modeling Using Transformer-XL for Language Modeling   The New Sensation in NLP: Google&amp;rsquo;s BERT  Model Architecture BERT Pre-Training Tasks    Sequence-to-Sequence Models &amp;ndash; A Backdrop seq2seq models in NLP are used to convert sequences of Type A to sequences of Type B.</description>
    </item>
    
    <item>
      <title>Review of &#39;Microsoft: &#34;The future of tech, with Kevin Scott and guests // Microsoft Build&#34;&#39;</title>
      <link>https://mullikine.github.io/posts/review-of-microsoft-the-future-of-tech-with-kevin-scott-and-guests-microsoft-build/</link>
      <pubDate>Tue, 12 May 2020 00:00:00 +0800</pubDate>
      
      <guid>https://mullikine.github.io/posts/review-of-microsoft-the-future-of-tech-with-kevin-scott-and-guests-microsoft-build/</guid>
      <description>Original video Microsoft: &amp;ldquo;The future of tech, with Kevin Scott and guests // Microsoft Build&amp;rdquo; Original article News OpenAI Model Generates Python Code - YouTube Original video from microsoft https://www.youtube.com/watch?v=fZSFNUT6iY8  Summary At 29 min you can see a demo of code generation from comments.
This is similar to what deep tabnine currently does, though probably uses a more powerful language model.
GitHub CodeSpaces and VSCode will probably have this built-in.</description>
    </item>
    
    <item>
      <title>(WIP) Notes on &#34;The Illustrated GPT-2&#34;</title>
      <link>https://mullikine.github.io/posts/notes-on-illustrated-gpt2/</link>
      <pubDate>Thu, 24 Oct 2019 00:00:00 +0800</pubDate>
      
      <guid>https://mullikine.github.io/posts/notes-on-illustrated-gpt2/</guid>
      <description>Original article The Illustrated GPT-2 (Visualizing Transformer Language Models) Jay Alammar Visualizing machine learning one concept at a time  Prereading Overview of The Illustrated Transformer // Bodacious Blog
Parameters When an article talks about the number of parameters, this is what it&amp;rsquo;s referring to.
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  Parameters Single Transformer block Conv1d attn/c_attn w 768 2304 1769472 b 2304 2304 attn/c_proj w 768 768 589824 b 768 768 mlp/c_fc w 768 3072 2359296 b 768 768 mlp/c_proj w 3072 3072 2359296 b 768 768 Norm ln_1 g 768 768 b 768 768 ln_2 g 768 768 b 768 768 total 7085568 per block X 12 blocks 85026816 In all blocks Embeddings 50257 768 38597376 Positional Embeddings 2024 768 786432 Grand Total 124410624   Goal Supplement The Illustrated Transformer with more visuals explaining the inner-workings of transformers, and how they’ve evolved since the original paper.</description>
    </item>
    
    <item>
      <title>writeup.ai</title>
      <link>https://mullikine.github.io/posts/review-of-writeup-ai/</link>
      <pubDate>Wed, 16 Oct 2019 00:00:00 +0800</pubDate>
      
      <guid>https://mullikine.github.io/posts/review-of-writeup-ai/</guid>
      <description>Original article https://senrigan.io/blog/how-writeupai-runs-behind-the-scenes/#h%5F6068056784021570782144062  Glossary 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26  cross entropy loss (between two probability distributions) (and over the same underlying set of events) Higher loss is bad. Measures the performance of a classification model whose output is a probability value between 0 and 1. Measures the average number of bits needed to identify an event from the set.</description>
    </item>
    
  </channel>
</rss>
