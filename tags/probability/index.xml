<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Probability on Bodacious Blog</title>
    <link>http://mullikine.github.io/tags/probability/</link>
    <description>Recent content in Probability on Bodacious Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 24 Oct 2019 00:00:00 +1300</lastBuildDate><atom:link href="http://mullikine.github.io/tags/probability/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Notes on &#34;Math4IQB Hopfield Networks&#34;</title>
      <link>http://mullikine.github.io/posts/notes-on-math4iqb-hopfield-networks/</link>
      <pubDate>Thu, 24 Oct 2019 00:00:00 +1300</pubDate>
      
      <guid>http://mullikine.github.io/posts/notes-on-math4iqb-hopfield-networks/</guid>
      <description>Original video https://www.youtube.com/watch?v=gfPUWwBkXZY  Glossary information gain [data mining] The amount of information that&amp;#39;s gained by knowing the value of the attribute, which is the entropy of the distribution before the split minus the entropy of the distribution after it. The largest information gain is equivalent to the smallest entropy. vim +/&amp;#34;mutual information&amp;#34; &amp;#34;$NOTES/ws/glossaries/information-theory.txt&amp;#34; information gain ratio [#decision tree learning] Ratio of information gain to the intrinsic information. It was proposed by Ross Quinlan, to reduce a bias towards multi-valued attributes by taking the number and size of branches into account when choosing an attribute.</description>
    </item>
    
  </channel>
</rss>
