<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>dl on Bodacious Blog</title>
    <link>https://mullikine.github.io/tags/dl/</link>
    <description>Recent content in dl on Bodacious Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 23 Dec 2019 00:00:00 +1300</lastBuildDate><atom:link href="https://mullikine.github.io/tags/dl/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Reading 23.12.19</title>
      <link>https://mullikine.github.io/posts/reading-23.12.19/</link>
      <pubDate>Mon, 23 Dec 2019 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/reading-23.12.19/</guid>
      <description>AI NLP  Language Engineering; Harnessing the Power of Language Google AI Blog: ALBERT: A Lite BERT for Self-Supervised Learning of Language Representations  Computer Vision  ObjectNet  Automated testing  Cucumber | Tools &amp;amp; techniques that elevate teams to greatness Writing Features - Gherkin Language - Behat 2.5.3 documentation  Information Retrieval  Why databases use ordered indexes but programming uses hash tables (evanjones.ca)  Programming languages haskell parsers  An introduction to parsing text in Haskell with Parsec - Nick.</description>
    </item>
    
    <item>
      <title>Review of &#39;Deep Learning breakthrough made by Rice University scientists | Ars Technica&#39;</title>
      <link>https://mullikine.github.io/posts/review-of-deep-learning-breakthrough-made-by-rice-university-scientists-ars-technica/</link>
      <pubDate>Sun, 15 Dec 2019 00:00:00 +0800</pubDate>
      
      <guid>https://mullikine.github.io/posts/review-of-deep-learning-breakthrough-made-by-rice-university-scientists-ars-technica/</guid>
      <description>Original article Deep Learning breakthrough made by Rice University scientists | Ars Technica  Cloudy with a chance of neurons: The tools that make NNs work In an earlier DL article, we talked about how inference workloads—
the use of already-trained NNs to analyze data—can run on fairly cheap hardware, but running the training workload that the NN &amp;ldquo;learns&amp;rdquo; on is orders of magnitude more expensive.
In particular, the more potential inputs you have to an algorithm, the more out of control your scaling problem gets when analyzing its problem space.</description>
    </item>
    
    <item>
      <title>Notes on &#34;Generating Beatles’ Lyrics with Machine Learning&#34;</title>
      <link>https://mullikine.github.io/posts/notes-on-generating-beatles-lyrics-with-machine-learning/</link>
      <pubDate>Thu, 24 Oct 2019 00:00:00 +0800</pubDate>
      
      <guid>https://mullikine.github.io/posts/notes-on-generating-beatles-lyrics-with-machine-learning/</guid>
      <description>Original article Generating Beatles Lyrics with Machine Learning - Towards Data Science  Apparatus     URL      code https://github.com/EugenHotaj/beatles    dataset http://toti.eu.com/beatles/index.asp github.com/EugenHotaj/beatles/blob/master/scraper.py   embeddings     algorithm      Hypothesis Aim Questions  How might might generate sentences from a language model?  Method Factoids Unigram model Ignores any conditioning and simply chooses the next word randomly from the training data.</description>
    </item>
    
    <item>
      <title>Notes on &#34;Identifying the right meaning of the words using BERT&#34;</title>
      <link>https://mullikine.github.io/posts/notes-on-identifying-the-right-meaning-of-the-words-using-bert/</link>
      <pubDate>Thu, 24 Oct 2019 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/notes-on-identifying-the-right-meaning-of-the-words-using-bert/</guid>
      <description>Original article Identifying the right meaning of the words using BERT  Glossary 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  Uncased [model] The text has been lowercased before WordPiece tokenization, e.g., John Smith becomes john smith. The Uncased model also strips out any accent markers. Cased [model] The true case and accent markers are preserved.</description>
    </item>
    
    <item>
      <title>Notes on &#34;Math4IQB Hopfield Networks&#34;</title>
      <link>https://mullikine.github.io/posts/notes-on-math4iqb-hopfield-networks/</link>
      <pubDate>Thu, 24 Oct 2019 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/notes-on-math4iqb-hopfield-networks/</guid>
      <description>Original video https://www.youtube.com/watch?v=gfPUWwBkXZY  Glossary information gain [data mining] The amount of information that&amp;#39;s gained by knowing the value of the attribute, which is the entropy of the distribution before the split minus the entropy of the distribution after it. The largest information gain is equivalent to the smallest entropy. vim +/&amp;#34;mutual information&amp;#34; &amp;#34;$NOTES/ws/glossaries/information-theory.txt&amp;#34; information gain ratio [#decision tree learning] Ratio of information gain to the intrinsic information. It was proposed by Ross Quinlan, to reduce a bias towards multi-valued attributes by taking the number and size of branches into account when choosing an attribute.</description>
    </item>
    
    <item>
      <title>The Illustrated Transformer</title>
      <link>https://mullikine.github.io/posts/review-of-the-illustrated-transformer/</link>
      <pubDate>Sat, 19 Oct 2019 00:00:00 +0800</pubDate>
      
      <guid>https://mullikine.github.io/posts/review-of-the-illustrated-transformer/</guid>
      <description>Original article https://jalammar.github.io/illustrated-transformer/ Source https://github.com/tensorflow/tensor2tensor  Prereading https://jalammar.github.io/illustrated-word2vec/
Helpful glossary 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51  Multilayer perceptron Feed-Forward Neural Network FFNN Basically, these are multi-level logistic regression classifiers.</description>
    </item>
    
    <item>
      <title>gwern.net GPT-2</title>
      <link>https://mullikine.github.io/posts/review-of-gewn-gpt2/</link>
      <pubDate>Thu, 17 Oct 2019 00:00:00 +0800</pubDate>
      
      <guid>https://mullikine.github.io/posts/review-of-gewn-gpt2/</guid>
      <description>original article https://www.gwern.net/GPT-2  Read using eww The website when viewed with a GUI browser, although impressive, is distracting.
The website was built with hackyll, a static site generator employing haskell.
1  eww &amp;#34;https://www.gwern.net/GPT-2&amp;#34;   Ease of reading Straight-forward once you understand ML jargon.</description>
    </item>
    
    <item>
      <title>writeup.ai</title>
      <link>https://mullikine.github.io/posts/review-of-writeup-ai/</link>
      <pubDate>Wed, 16 Oct 2019 00:00:00 +0800</pubDate>
      
      <guid>https://mullikine.github.io/posts/review-of-writeup-ai/</guid>
      <description>Original article https://senrigan.io/blog/how-writeupai-runs-behind-the-scenes/#h%5F6068056784021570782144062  Glossary 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26  cross entropy loss (between two probability distributions) (and over the same underlying set of events) Higher loss is bad. Measures the performance of a classification model whose output is a probability value between 0 and 1. Measures the average number of bits needed to identify an event from the set.</description>
    </item>
    
    <item>
      <title>(WIP) Review of Language, trees, and geometry in neural networks</title>
      <link>https://mullikine.github.io/posts/language-trees-and-geometry-in-neural-networks/</link>
      <pubDate>Mon, 07 Oct 2019 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/language-trees-and-geometry-in-neural-networks/</guid>
      <description>1906.02715 Visualizing and Measuring the Geometry of BERT
https://pair-code.github.io/interpretability/bert-tree/
https://pair-code.github.io/interpretability/context-atlas/blogpost/
Existing representation: word embeddings Language is made of discrete structures, yet neural networks operate on continuous data: vectors in high-dimensional space.
A successful language-processing network must translate this symbolic information into some kind of geometric representation—but in what form?
Word embeddings provide two well-known examples: distance encodes semantic similarity, while certain directions correspond to polarities (e.g. male vs. female).
New representation A recent, fascinating discovery points to an entirely new type of representation.</description>
    </item>
    
    <item>
      <title>COSC420: Neural Networks - Assignment 2</title>
      <link>https://mullikine.github.io/posts/ir-assignment-2/</link>
      <pubDate>Fri, 25 May 2018 00:00:00 +1200</pubDate>
      
      <guid>https://mullikine.github.io/posts/ir-assignment-2/</guid>
      <description>Introduction / Background There are many activation and error functions available to choose from when designing a NN to solve a particular task. One could even design their own if they wanted.
There are rules of thumb for selecting functions that have been shown empiracally to give you better results. I&amp;rsquo;ve decided to explore the difference that choice can make on the time it takes to train my delta rule network.</description>
    </item>
    
  </channel>
</rss>
