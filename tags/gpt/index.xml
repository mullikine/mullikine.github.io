<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>gpt on Bodacious Blog</title>
    <link>https://mullikine.github.io/tags/gpt/</link>
    <description>Recent content in gpt on Bodacious Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 27 Aug 2020 00:00:00 +1200</lastBuildDate>
    
	<atom:link href="https://mullikine.github.io/tags/gpt/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>OpenAI API for NLP</title>
      <link>https://mullikine.github.io/posts/openai-api-for-nlp/</link>
      <pubDate>Fri, 12 Jun 2020 00:00:00 +0800</pubDate>
      
      <guid>https://mullikine.github.io/posts/openai-api-for-nlp/</guid>
      <description>OpenAI API https://beta.openai.com/  Related articles I&amp;rsquo;d like to add some OpenAI support to emacs.
 Emacs Websockets // Bodacious Blog  API  Apply the API to any language task  semantic search, summarization, sentiment analysis, content generation, translation, and more&amp;hellip;    Use only a few examples or by specifying your task in English.
Semantic search 1 2 3 4 5 6  Semantic Search [openai-api] Allows searching over documents based on the natural-language meaning of queries rather than keyword matching.</description>
    </item>
    
    <item>
      <title>Review of &#39;GPT-3: Language Models are Few-Shot Learners (Paper Explained) - YouTube&#39;</title>
      <link>https://mullikine.github.io/posts/review-of-gpt-3-language-models-are-few-shot-learners-paper-explained-youtube/</link>
      <pubDate>Sat, 30 May 2020 00:00:00 +0800</pubDate>
      
      <guid>https://mullikine.github.io/posts/review-of-gpt-3-language-models-are-few-shot-learners-paper-explained-youtube/</guid>
      <description>Work in progress
 Original video GPT-3: Language Models are Few-Shot Learners (Paper Explained) - YouTube  Demonstration of me reading this video and taking notes 
GPT-3 GPT 3 has 175 billion parameters which this is absolutely crazy is an order of magnitude higher than anything that ever existed.
GPT-2 parameters by comparison This is what people are talking about when they say parameters.
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  Parameters Single Transformer block Conv1d attn/c_attn w 768 2304 1769472 b 2304 2304 attn/c_proj w 768 768 589824 b 768 768 mlp/c_fc w 768 3072 2359296 b 768 768 mlp/c_proj w 3072 3072 2359296 b 768 768 Norm ln_1 g 768 768 b 768 768 ln_2 g 768 768 b 768 768 total 7085568 per block X 12 blocks 85026816 In all blocks Embeddings 50257 768 38597376 Positional Embeddings 2024 768 786432 Grand Total 124410624   What you can do with giant LMs?</description>
    </item>
    
    <item>
      <title>Review of &#39;Microsoft: &#34;The future of tech, with Kevin Scott and guests // Microsoft Build&#34;&#39;</title>
      <link>https://mullikine.github.io/posts/review-of-microsoft-the-future-of-tech-with-kevin-scott-and-guests-microsoft-build/</link>
      <pubDate>Tue, 12 May 2020 00:00:00 +0800</pubDate>
      
      <guid>https://mullikine.github.io/posts/review-of-microsoft-the-future-of-tech-with-kevin-scott-and-guests-microsoft-build/</guid>
      <description>Original video Microsoft: &amp;ldquo;The future of tech, with Kevin Scott and guests // Microsoft Build&amp;rdquo; Original article News OpenAI Model Generates Python Code - YouTube Original video from microsoft https://www.youtube.com/watch?v=fZSFNUT6iY8  Summary At 29 min you can see a demo of code generation from comments.
This is similar to what deep tabnine currently does, though probably uses a more powerful language model.
GitHub CodeSpaces and VSCode will probably have this built-in.</description>
    </item>
    
    <item>
      <title>Creating Infinitely Generated Text Adventures with DL LMs</title>
      <link>https://mullikine.github.io/posts/infinitely-generated-text-adventures/</link>
      <pubDate>Fri, 06 Dec 2019 00:00:00 +0800</pubDate>
      
      <guid>https://mullikine.github.io/posts/infinitely-generated-text-adventures/</guid>
      <description>Original article AI Dungeon 2: Creating Infinitely Generated Text Adventures with Deep Learning Language Models - Perception, Control, Cognition  For each action you type the model is fed the context sentence as well as the past N action- result pairs in its memory to generate the result.
We found N=8 to be a good amount of memory to feed the model.
Other times the model has difficulty keeping track of who is who, especially in dialogue.</description>
    </item>
    
    <item>
      <title>BERT</title>
      <link>https://mullikine.github.io/posts/bert/</link>
      <pubDate>Tue, 03 Dec 2019 00:00:00 +0800</pubDate>
      
      <guid>https://mullikine.github.io/posts/bert/</guid>
      <description> Original article BERT Explained: A Complete Guide with Theory and Tutorial - Towards Machine Learning  </description>
    </item>
    
    <item>
      <title>AI-assisted coding tools for emacs</title>
      <link>https://mullikine.github.io/posts/ai-assisted-coding-tools-emacs/</link>
      <pubDate>Wed, 27 Nov 2019 00:00:00 +0800</pubDate>
      
      <guid>https://mullikine.github.io/posts/ai-assisted-coding-tools-emacs/</guid>
      <description>language-detection Emacs Lisp library that automatically detects the programming language in a buffer or string. Implemented as a random forest classifier, trained in scikit-learn and deployed to Emacs Lisp.
https://github.com/andreasjansson/language-detection.el
Example of usage 1 2 3 4 5 6 7 8 9  (defun new-buffer-from-selection-detect-language () &amp;#34;Creates a new buffer from the selection and tries to set the mode&amp;#34; (interactive) (if (selected-p) (let* ((b (new-buffer-from-string (selection)))) (with-current-buffer b (switch-to-buffer b) (guess-major-mode))))) (defalias &amp;#39;detect-language-set-mode &amp;#39;guess-major-mode)   1 2  ;; This works great (my/truly-selective-binding &amp;#34;Y&amp;#34; #&amp;#39;new-buffer-from-selection-detect-language)   expanded the macro</description>
    </item>
    
    <item>
      <title>Coding faster with Deep TabNine</title>
      <link>https://mullikine.github.io/posts/using-deep-tabnine/</link>
      <pubDate>Thu, 14 Nov 2019 00:00:00 +0800</pubDate>
      
      <guid>https://mullikine.github.io/posts/using-deep-tabnine/</guid>
      <description>Example of using Deep TabNine to write with example code</description>
    </item>
    
    <item>
      <title>TensorFlow BERT</title>
      <link>https://mullikine.github.io/posts/tensorflow-bert-keras/</link>
      <pubDate>Thu, 31 Oct 2019 00:00:00 +0800</pubDate>
      
      <guid>https://mullikine.github.io/posts/tensorflow-bert-keras/</guid>
      <description>Original article Hugging Face: State-of-the-Art Natural Language Processing in ten lines of TensorFlow 2.0  A list of transformer architectures    architecture     BERT   RoBERTa   GPT-2   DistilBERT    pip&#39;s transformers library Builds on 3 main classes:  configuration class tokenizer class model class  configuration class Hosts relevant information concerning the model we will be using, such as:</description>
    </item>
    
    <item>
      <title>Deep TabNine and emacs</title>
      <link>https://mullikine.github.io/posts/emacs-deep-tab-nine/</link>
      <pubDate>Wed, 30 Oct 2019 00:00:00 +0800</pubDate>
      
      <guid>https://mullikine.github.io/posts/emacs-deep-tab-nine/</guid>
      <description>Deep TabNine can supplement your autocompletion needs by providing you with example code.
 plugin https://github.com/TommyX12/company-tabnine/  Objective Have two bindings; one for regular company complete and one for tabnine.
   kb f     M-` company-complete   M-Tab company-tabnine    1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25  (require &amp;#39;company-tabnine) ;; TODO Fix ;; While browsing the completions list, if I press &amp;lt;space&amp;gt; then company aborts.</description>
    </item>
    
    <item>
      <title>(WIP) Notes on &#34;The Illustrated GPT-2&#34;</title>
      <link>https://mullikine.github.io/posts/notes-on-illustrated-gpt2/</link>
      <pubDate>Thu, 24 Oct 2019 00:00:00 +0800</pubDate>
      
      <guid>https://mullikine.github.io/posts/notes-on-illustrated-gpt2/</guid>
      <description>Original article The Illustrated GPT-2 (Visualizing Transformer Language Models) Jay Alammar Visualizing machine learning one concept at a time  Prereading Overview of The Illustrated Transformer // Bodacious Blog
Parameters When an article talks about the number of parameters, this is what it&amp;rsquo;s referring to.
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  Parameters Single Transformer block Conv1d attn/c_attn w 768 2304 1769472 b 2304 2304 attn/c_proj w 768 768 589824 b 768 768 mlp/c_fc w 768 3072 2359296 b 768 768 mlp/c_proj w 3072 3072 2359296 b 768 768 Norm ln_1 g 768 768 b 768 768 ln_2 g 768 768 b 768 768 total 7085568 per block X 12 blocks 85026816 In all blocks Embeddings 50257 768 38597376 Positional Embeddings 2024 768 786432 Grand Total 124410624   Goal Supplement The Illustrated Transformer with more visuals explaining the inner-workings of transformers, and how they’ve evolved since the original paper.</description>
    </item>
    
    <item>
      <title>Notes on &#34;Generating Beatles’ Lyrics with Machine Learning&#34;</title>
      <link>https://mullikine.github.io/posts/notes-on-generating-beatles-lyrics-with-machine-learning/</link>
      <pubDate>Thu, 24 Oct 2019 00:00:00 +0800</pubDate>
      
      <guid>https://mullikine.github.io/posts/notes-on-generating-beatles-lyrics-with-machine-learning/</guid>
      <description>Original article Generating Beatles Lyrics with Machine Learning - Towards Data Science  Apparatus     URL      code https://github.com/EugenHotaj/beatles    dataset http://toti.eu.com/beatles/index.asp github.com/EugenHotaj/beatles/blob/master/scraper.py   embeddings     algorithm      Hypothesis Aim Questions  How might might generate sentences from a language model?  Method Factoids Unigram model Ignores any conditioning and simply chooses the next word randomly from the training data.</description>
    </item>
    
    <item>
      <title>The Illustrated Transformer</title>
      <link>https://mullikine.github.io/posts/review-of-the-illustrated-transformer/</link>
      <pubDate>Sat, 19 Oct 2019 00:00:00 +0800</pubDate>
      
      <guid>https://mullikine.github.io/posts/review-of-the-illustrated-transformer/</guid>
      <description>Original article https://jalammar.github.io/illustrated-transformer/ Source https://github.com/tensorflow/tensor2tensor  Prereading https://jalammar.github.io/illustrated-word2vec/
Helpful glossary 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51  Multilayer perceptron Feed-Forward Neural Network FFNN Basically, these are multi-level logistic regression classifiers.</description>
    </item>
    
    <item>
      <title>gwern.net GPT-2</title>
      <link>https://mullikine.github.io/posts/review-of-gewn-gpt2/</link>
      <pubDate>Thu, 17 Oct 2019 00:00:00 +0800</pubDate>
      
      <guid>https://mullikine.github.io/posts/review-of-gewn-gpt2/</guid>
      <description>original article https://www.gwern.net/GPT-2  Read using eww The website when viewed with a GUI browser, although impressive, is distracting.
The website was built with hackyll, a static site generator employing haskell.
1  eww &amp;#34;https://www.gwern.net/GPT-2&amp;#34;   Ease of reading Straight-forward once you understand ML jargon.</description>
    </item>
    
    <item>
      <title>Rewrite of gwern.net GPT-2 Neural Network Poetry</title>
      <link>https://mullikine.github.io/posts/rewrite-of-gwern-gpt2-poetry/</link>
      <pubDate>Thu, 17 Oct 2019 00:00:00 +0800</pubDate>
      
      <guid>https://mullikine.github.io/posts/rewrite-of-gwern-gpt2-poetry/</guid>
      <description>original article https://www.gwern.net/GPT-2  Read using eww The website when viewed with a GUI browser, although impressive, is distracting.
The website was built with hackyll, a static site generator employing haskell.
1  eww &amp;#34;https://www.gwern.net/GPT-2&amp;#34;   Ease of reading Straight-forward once you understand ML jargon.</description>
    </item>
    
    <item>
      <title>(WIP) Review of NLP tools</title>
      <link>https://mullikine.github.io/posts/review-of-nlp-tools/</link>
      <pubDate>Wed, 16 Oct 2019 00:00:00 +0800</pubDate>
      
      <guid>https://mullikine.github.io/posts/review-of-nlp-tools/</guid>
      <description>lm-explorer Interactive explorer for language models (currently only OpenAI GPT-2).</description>
    </item>
    
    <item>
      <title>writeup.ai</title>
      <link>https://mullikine.github.io/posts/review-of-writeup-ai/</link>
      <pubDate>Wed, 16 Oct 2019 00:00:00 +0800</pubDate>
      
      <guid>https://mullikine.github.io/posts/review-of-writeup-ai/</guid>
      <description>Original article https://senrigan.io/blog/how-writeupai-runs-behind-the-scenes/#h%5F6068056784021570782144062  Glossary 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26  cross entropy loss (between two probability distributions) (and over the same underlying set of events) Higher loss is bad. Measures the performance of a classification model whose output is a probability value between 0 and 1. Measures the average number of bits needed to identify an event from the set.</description>
    </item>
    
    <item>
      <title>PClean: A probabilistic scripting DSL</title>
      <link>https://mullikine.github.io/posts/pclean-gen-gpl/</link>
      <pubDate>Mon, 30 Sep 2019 00:00:00 +0800</pubDate>
      
      <guid>https://mullikine.github.io/posts/pclean-gen-gpl/</guid>
      <description>Links  MIT Probabilistic Computing Project Introduction | Gen GitHub - probcomp/Gen: A general-purpose probabilistic programming system with programmable inference  Other talks by MIT Probabilistic Computing Project: Videos, Talks, and Podcasts - MIT Probabilistic Computing Project
Tools Gen  a package for the Julia programming language. consists of multiple modeling languages that are implemented as DSLs in Julia and a Julia library for inference programming.  PClean  A probabilistic scripting DSL in the Gen package.</description>
    </item>
    
  </channel>
</rss>