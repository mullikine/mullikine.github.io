<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>gpt on Bodacious Blog</title>
    <link>https://mullikine.github.io/tags/gpt/</link>
    <description>Recent content in gpt on Bodacious Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 11 Jul 2021 00:00:00 +1200</lastBuildDate><atom:link href="https://mullikine.github.io/tags/gpt/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Pen Tutorial</title>
      <link>https://mullikine.github.io/posts/pen-tutorial/</link>
      <pubDate>Sun, 11 Jul 2021 00:00:00 +1200</pubDate>
      
      <guid>https://mullikine.github.io/posts/pen-tutorial/</guid>
      <description>Summary This is a tutorial for how to use pen.el.
Default Key bindings    kb f      H-TAB g pen-generate-prompt-functions pen-map   H-TAB r pen-run-prompt-function pen-map   SPC pen-run-prompt-function selected-map    H is the Hyper key, which works similar to Escape, Meta, Alt, Control or Shift.
pen.el emulates a Hyper key with C-M-\.
Usage Running pen-generate-prompt-functions will load all prompts from the prompts directory, which is typically located here: ~/.</description>
    </item>
    
    <item>
      <title>Pen (Prompt Engineering in Emacs)</title>
      <link>https://mullikine.github.io/posts/pen/</link>
      <pubDate>Fri, 02 Jul 2021 00:00:00 +1200</pubDate>
      
      <guid>https://mullikine.github.io/posts/pen/</guid>
      <description>Introducing Pen I start a GitHub project to deploy pen.el.
 GitHub project GitHub - semiosis/pen.el: pen.el is a package for prompt engineering in emacs. It facilitates the creation, ongoing development, discovery and usage of prompts to a language model such as OpenAI&amp;rsquo;s GPT-3 or EleutherAI&amp;rsquo;s GPT-j. Project timeline and objectives https://github.com/semiosis/pen.el/tree/master/docs Prompt README prompts/README.org at master semiosis/prompts GitHub Tutorial https://semiosis.github.io/posts/pen-tutorial/   1  docker run --rm -ti --entrypoint= semiosis/pen.</description>
    </item>
    
    <item>
      <title>TomatoBear Story</title>
      <link>https://mullikine.github.io/posts/tomatobear-story/</link>
      <pubDate>Tue, 29 Jun 2021 00:00:00 +1200</pubDate>
      
      <guid>https://mullikine.github.io/posts/tomatobear-story/</guid>
      <description>The unfolding story https://github.com/semiosis/tomato-bear-story Story origins https://github.com/semiosis/tomato-bear-story/blob/master/docs/28.06.21/story-origins.org  Perspective Stories will auto-complete one day By this, I mean, if I place an incomplete story on GitHub, this will autocomplete one day because an AGI will go looking for it, find it on GitHub and publish the completed version somewhere, but I can be working on it now, if I want.
Therefore, I must focus on connecting things to sources of immutable truth.</description>
    </item>
    
    <item>
      <title>Representing multiverses with Datomic</title>
      <link>https://mullikine.github.io/posts/representing-multiverses-with-datomic/</link>
      <pubDate>Sat, 26 Jun 2021 00:00:00 +1200</pubDate>
      
      <guid>https://mullikine.github.io/posts/representing-multiverses-with-datomic/</guid>
      <description>Related articles  Language models are multiverse generators :: Moire  |:ϝ∷¦ϝ&amp;rsquo;s blog post on exploring language models.   Datomic with Rich Hickey - YouTube Writing Datomic in Clojure - Rich Hickey - YouTube richhickey.md GitHub  Rich Hickey on becoming a better developer   Notes: https://github.com/semiosis/code-org-tidbits/blob/master/datomic/basic-query.org  Summary After reading |:ϝ∷¦ϝ&amp;rsquo;s blog article LMs are multiverse generators :: Moire, I decided to take a closer look into Datomic as a possible store for LM generations.</description>
    </item>
    
    <item>
      <title>explainshell with GPT-3</title>
      <link>https://mullikine.github.io/posts/explainshell-with-gpt-3/</link>
      <pubDate>Wed, 16 Jun 2021 00:00:00 +1200</pubDate>
      
      <guid>https://mullikine.github.io/posts/explainshell-with-gpt-3/</guid>
      <description>Summary I make a simple GPT-3 prompt to explain shell code while using emacs.
It mimics the functionality of explainshell but it&amp;rsquo;s able to also describe the purpose of commands with syntax and those that are semi-baked or pseudocode.
 Related https://mullikine.github.io/posts/crontab-guru-in-emacs/  Demo  --  Prompt 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135  in-development: no title: &amp;#34;explain a shell command&amp;#34; issues: design-patterns: # future-titles: &amp;#34;&amp;#34; # aims: doc: &amp;#34;Explain what a shell command does&amp;#34; # aims: |+ # - More abstractive rewording prompt-version: 1 # &amp;lt;:pp&amp;gt; defines a point where the following # text is concatenated before the postprocessor # is run.</description>
    </item>
    
    <item>
      <title>nlsh (Natural Language Shell) with GPT-3</title>
      <link>https://mullikine.github.io/posts/nlsh-natural-language-shell/</link>
      <pubDate>Wed, 16 Jun 2021 00:00:00 +1200</pubDate>
      
      <guid>https://mullikine.github.io/posts/nlsh-natural-language-shell/</guid>
      <description>Summary I extend my openai-complete script with REPL capabilities and use it to create parameterised nlsh REPLs for different Operating Systems.
I use comint, the emacs mode for managing REPLs and rlwrap to manage history and allow me to run the REPL without emacs.
I also generalise it within my prompt description format as &amp;ldquo;conversation mode&amp;rdquo; which enables me to have rolling conversations with a prompt.
So far, I have not implemented any kind of pseudo-memory system for rolling conversation.</description>
    </item>
    
    <item>
      <title>GPT-3 assistants for emacs modes</title>
      <link>https://mullikine.github.io/posts/gpt-3-assistants-for-emacs-modes/</link>
      <pubDate>Wed, 02 Jun 2021 00:00:00 +1200</pubDate>
      
      <guid>https://mullikine.github.io/posts/gpt-3-assistants-for-emacs-modes/</guid>
      <description>Summary In this article I will show how I transition from using shell script to emacs lisp with my &amp;lsquo;any topic&amp;rsquo; tutor in emacs lisp.
I am working on integrating GPT-3, GPT-j and more GPT completion engines into emacs, and connecting more and more emacs packages to GPT-3.
Some ideas I have:
 Correct spelling and grammar in conversations over erc and other emacs chat modes. Select error messages and ask what they mean.</description>
    </item>
    
    <item>
      <title>Exploring Neuralink concepts with GPT-3 (WIP)</title>
      <link>https://mullikine.github.io/posts/exploring-neuralink-concepts-with-gpt-3/</link>
      <pubDate>Wed, 14 Apr 2021 00:00:00 +1200</pubDate>
      
      <guid>https://mullikine.github.io/posts/exploring-neuralink-concepts-with-gpt-3/</guid>
      <description>This article is a work in progress.
Summary I create several org-brain repositories for exploratory learning and ideation.
1 2 3 4 5 6 7 8 9 10 11 12  billboard exploratory fungible ideation infogetics infrastructure-tooling neuralink open-source-alternatives reference thoughts tooling welfare-organisations   I then use GPT-3 to discuss topics and why they are important.
Mind terminal   Glossaries  http://github.com/mullikine/glossaries-gh/blob/master/biochemistry.txt http://github.com/mullikine/glossaries-gh/blob/master/brain-computer-interface-bci.txt http://github.com/mullikine/glossaries-gh/blob/master/neuralink.txt http://github.com/mullikine/glossaries-gh/blob/master/neuroscience.txt http://github.com/mullikine/glossaries-gh/blob/master/neural-engineering.txt http://github.com/mullikine/glossaries-gh/blob/master/gpt.txt http://github.com/mullikine/glossaries-gh/blob/master/ai-safety.txt http://github.</description>
    </item>
    
    <item>
      <title>GPT-3 is The Thing</title>
      <link>https://mullikine.github.io/posts/gpt-3-like-the-thing/</link>
      <pubDate>Wed, 14 Apr 2021 00:00:00 +1200</pubDate>
      
      <guid>https://mullikine.github.io/posts/gpt-3-like-the-thing/</guid>
      <description>Summary The Thing is a lot like GPT-3 due to it being extremely capable, but limited to the queries you give it.
It&amp;rsquo;s as powerful as your understanding on how to talk to it.
The Thing is a black box computer in the nomes series by Terry Pratchett.
I make reference to the first book in the series Truckers.
This book is freely available on the Internet Archive.
 Truckers by Terry Pratchett https://archive.</description>
    </item>
    
    <item>
      <title>Imaginary programming with GPT-3</title>
      <link>https://mullikine.github.io/posts/imaginary-programming-with-gpt-3/</link>
      <pubDate>Thu, 08 Apr 2021 00:00:00 +1200</pubDate>
      
      <guid>https://mullikine.github.io/posts/imaginary-programming-with-gpt-3/</guid>
      <description>Code https://github.com/semiosis/pen.el Prompts https://github.com/semiosis/prompts/ Disclaimer Please contribute as this is an open source project! It&amp;rsquo;s very hard to find free prompts online currently and that&amp;rsquo;s because everyone is out for themselves. Please support open source. Thank you.  Summary This is a demonstration of an imaginary programming environment. There may be nothing else like it in the world today.
The world needs to get ready for the next generations of Large LMs, such as GPT-4.</description>
    </item>
    
    <item>
      <title>Fictional statements of remorse with GPT-3 in the 1st and 3rd person</title>
      <link>https://mullikine.github.io/posts/fictional-statements-of-remorse-with-gpt-3/</link>
      <pubDate>Wed, 07 Apr 2021 00:00:00 +1200</pubDate>
      
      <guid>https://mullikine.github.io/posts/fictional-statements-of-remorse-with-gpt-3/</guid>
      <description>Summary I use GPT-3 to generate fictional statements of remorse.
It should be noted that this is only one such way that GPT-3 will upheave legal process.
RemorseBot (in the 1st person)  --  RemorseBot (in the 3rd person)  --  Prompts 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108  title: &amp;#34;Statement of remorse&amp;#34; # future-titles: &amp;#34;&amp;#34; # aims: |+ # - More abstractive rewording doc: &amp;#34;&amp;#34; # aims: |+ # - More abstractive rewording prompt-version: 1 # &amp;lt;:pp&amp;gt; defines a point where the following # text is concatenated before the postprocessor # is run.</description>
    </item>
    
    <item>
      <title>Translating with GPT-3 and Emacs</title>
      <link>https://mullikine.github.io/posts/translating-with-gpt-3-and-emacs/</link>
      <pubDate>Tue, 06 Apr 2021 00:00:00 +1200</pubDate>
      
      <guid>https://mullikine.github.io/posts/translating-with-gpt-3-and-emacs/</guid>
      <description>Summary Sorry for the lazy blog post today. I just ask GPT3 for some subtopics of ancient roman law as I am looking for a cool word to use. I would like to know what these words mean, so I use GPT3 for that too.
Subtopics of Ancient Roman Law These were generated by GPT-3.
1 2 3 4 5  aedilitas advocatus auctoritas augur auspex caupona cena clientela contio domus ius ludos ministra mos ora otium praetor quaestio res mancipi sacerdos status suovetaurilia tabella tribunus plebis via vir    --  GPT-3 Language detection and translation  language Latin  English translation:</description>
    </item>
    
    <item>
      <title>GPT-3 for working out what is better than what (Aggressive humor and satire)</title>
      <link>https://mullikine.github.io/posts/gpt-3-for-working-out-what-is-better-than-what/</link>
      <pubDate>Sat, 03 Apr 2021 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/gpt-3-for-working-out-what-is-better-than-what/</guid>
      <description>AI safety glossary https://github.com/mullikine/glossaries-gh/blob/master/ai-safety.txt  Warning. This is Aggressive humor and Satire This article showcases GPT-3&amp;rsquo;s amazing ability but also is meant to alert you to its dangers.
Summary On AI safety Be aware that GPT-3 has at least these two forms of bias:
1 2 3 4 5 6 7 8 9 10 11 12 13  explicit bias [#ai safety] Where output is clearly toxic (cursing, slander). implicit bias [#ai safety] Where the policy from the output changes based on context (e.</description>
    </item>
    
    <item>
      <title>GPT-3 mind maps with an AI tutor for any topic</title>
      <link>https://mullikine.github.io/posts/gpt-3-for-building-mind-maps-with-an-ai-tutor-for-any-topic/</link>
      <pubDate>Mon, 29 Mar 2021 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/gpt-3-for-building-mind-maps-with-an-ai-tutor-for-any-topic/</guid>
      <description>Code http://github.com/semiosis/pen.el Prompts http://github.com/semiosis/prompts  Summary I combine GPT-3 with org-brain to expand on topics, suggesting subtopics and providing an interactive tutor for any topic.
Demonstration Subtopic generation I demonstrate how to explore arbitrary topics with GPT-3 by automatically generating subtopics, and then allowing you to invoke the GPT-3 tutor to answer questions within that context.
 --  Tutor demonstration  Rolling conversation is a work in progress, but on its way.</description>
    </item>
    
    <item>
      <title>Generating pickup lines with GPT-3</title>
      <link>https://mullikine.github.io/posts/generating-pickup-lines-with-gpt-3/</link>
      <pubDate>Sat, 27 Mar 2021 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/generating-pickup-lines-with-gpt-3/</guid>
      <description>Latest version of the pick up lines prompt http://github.com/mullikine/prompts/blob/master/prompts/pick-up-line.prompt Original Pick-up-lines prompt by Jan Kolar https://www.producthunt.com/posts/500-openers-for-tinder-written-by-gpt-3 \
https://www.reddit.com/r/GPT3/comments/mdl7fl/500%5Fopeners%5Ffor%5Ftinder%5Fwritten%5Fby%5Fgpt3%5Fthe%5Fprompt/  Summary I create a prompt in my prompt description format and use it to generate some pickup lines.
Demonstration (v2 with emacs counsel integration) New results are fed into a fuzzy finder as they are generated. I can stop and select at any time.
 --  Demonstration (version 2)  --  Pick up lines with the topic &amp;ldquo;SETTLERS OF CATAN&amp;rdquo; ⌂ ⌂⌂ ━ ━ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28  When playing Settlers of Catan, the shortest route is a straight line to my heart.</description>
    </item>
    
    <item>
      <title>Quick demo: Summarizing with huggingface, GPT-3 and others</title>
      <link>https://mullikine.github.io/posts/summarizing-with-huggingface-gpt-3-and-others/</link>
      <pubDate>Sat, 27 Mar 2021 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/summarizing-with-huggingface-gpt-3-and-others/</guid>
      <description>Demonstration  Summarizing an arxiv paper Automating arxiv Comparing different GPT-3 prompts Configuration with emacs Developing the automations, pipelines and prompts   -- </description>
    </item>
    
    <item>
      <title>Autocompleting anything with GPT-3 in emacs</title>
      <link>https://mullikine.github.io/posts/autocompleting-anything-with-gpt-3-in-emacs/</link>
      <pubDate>Tue, 16 Mar 2021 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/autocompleting-anything-with-gpt-3-in-emacs/</guid>
      <description>Emacs package https://github.com/semiosis/pen.el Help/developers desperately needed in developing pen.el! Prompt engineering is very easy so don&amp;rsquo;t be intimidated! Prompt http://github.com/semiosis/prompts/blob/master/prompts/generic-file-type-completion.prompt  Summary I make a prompt for the OpenAI API which completes given a file type and some preceding text.
I then make a company-mode completion function for it, and then demo its usage.
This gives me a generic completion mechanism when dealing with any type of document.
Demonstration This is GPT-3 completing some text for me.</description>
    </item>
    
    <item>
      <title>A tour of Ryan Ong&#39;s - NLP 365</title>
      <link>https://mullikine.github.io/posts/ryan-ong-nlp-365/</link>
      <pubDate>Sat, 13 Mar 2021 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/ryan-ong-nlp-365/</guid>
      <description>Glossaries nlp-natural-language-processing.txt information-retrieval.txt fasttext.txt transformer.txt nmt-neural-machine-translation.txt information-theory.txt  Summary I go over Ryan Ong&amp;rsquo;s series on NLP, adding terms to my glossaries and reproducing what he&amp;rsquo;s done.
 Finished reading &amp;lt;2021-03-13 Sat&amp;gt;
 Day 1: What is Natural Language Processing - Ryan Ong Day 2: Damerau-Levenshtein Distance - Ryan Ong Day 3: Word Embeddings - Ryan Ong  Terms in the green text have added to my glossary.
  This is the project that introduced me to GloVe a few years ago.</description>
    </item>
    
    <item>
      <title>Context menus based on GPT-3</title>
      <link>https://mullikine.github.io/posts/context-menus-based-on-gpt-3/</link>
      <pubDate>Tue, 09 Mar 2021 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/context-menus-based-on-gpt-3/</guid>
      <description>Summary I create a GPT-3 prompt for testing to see if code is Haskell and use it as a test inside emacs to suggest further functions.
 Prompt file http://github.com/semiosis/prompts/blob/master/prompts/text-is-haskell.prompt  Demonstration As you can see, GPT-3 is able to detect the language and I can use that as a test in my emacs to provide further functions. The suggested function was yet another GPT-3 prompt function for translating Haskell into Clojure.</description>
    </item>
    
    <item>
      <title>spaCy in emacs</title>
      <link>https://mullikine.github.io/posts/spacy-in-emacs/</link>
      <pubDate>Tue, 09 Mar 2021 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/spacy-in-emacs/</guid>
      <description>Summary I begin construction of an environment for developing with spaCy.
 Goals  spaCy pipeline builder/wizard Select and analyse text with spaCy linguistic features spaCy python playground text selection configuration of spaCy using emacs custom.el    deplacy demo  deplacy code https://github.com/KoichiYasuoka/deplacy  
Code generation and bindings 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29  (defmacro etv-filter (cmd) (let* ((slug (slugify cmd)) (sym (str2sym (concat &amp;#34;etv-&amp;#34; slug)))) `(defun ,sym (&amp;amp;optional input) (interactive (list (my/selected-text))) (if (not input) (setq input (my/selected-text))) (etv (snc ,cmd input))))) (cl-loop for s in &amp;#39;(&amp;#34;partsofspeech&amp;#34; &amp;#34;entities&amp;#34; &amp;#34;displacy&amp;#34; &amp;#34;token-pos-dep&amp;#34; &amp;#34;sentiment&amp;#34; &amp;#34;segment-sentences&amp;#34;) do (eval (expand-macro `(etv-filter ,s)))) (define-key selected-keymap (kbd &amp;#34;Z n&amp;#34;) &amp;#39;ngram-query-replace) (define-key selected-keymap (kbd &amp;#34;Z S&amp;#34;) &amp;#39;sps-play-spacy) (define-key selected-keymap (kbd &amp;#34;Z P&amp;#34;) &amp;#39;etv-partsofspeech) (define-key selected-keymap (kbd &amp;#34;Z E&amp;#34;) &amp;#39;etv-entities) (define-key selected-keymap (kbd &amp;#34;Z D&amp;#34;) &amp;#39;etv-displacy) (define-key selected-keymap (kbd &amp;#34;Z T&amp;#34;) &amp;#39;etv-token-pos-dep) (define-key selected-keymap (kbd &amp;#34;Z N&amp;#34;) &amp;#39;etv-sentiment) (define-key selected-keymap (kbd &amp;#34;Z G&amp;#34;) &amp;#39;etv-segment-sentences)     Configuration yaml I store the configuration of spaCy inside a yaml file.</description>
    </item>
    
    <item>
      <title>An operating system based on GPT-3</title>
      <link>https://mullikine.github.io/posts/an-operating-system-based-on-gpt-3/</link>
      <pubDate>Mon, 08 Mar 2021 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/an-operating-system-based-on-gpt-3/</guid>
      <description>Reference http://github.com/semiosis/examplary  Summary I assume that GPT-3 or some descendant of it will become the primary interface to the computer.
I create a configuration option to enable / disable GPT-3.
When disabled, the environment will resort to alternative means of performing tasks.
Configuration 1  vim +/&amp;#34;use_gpt3: on&amp;#34; &amp;#34;$NOTES/myrc.yaml&amp;#34;     1  vim +/&amp;#34;summarize) {&amp;#34; &amp;#34;$SCRIPTS/s&amp;#34;   If gpt3 is enabled, filter through OpenAI API abstractive summarizer Otherwise, use sumy.</description>
    </item>
    
    <item>
      <title>A natural language database using a single GPT prompt</title>
      <link>https://mullikine.github.io/posts/a-natural-language-database-using-a-single-gpt-prompt/</link>
      <pubDate>Sun, 07 Mar 2021 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/a-natural-language-database-using-a-single-gpt-prompt/</guid>
      <description>Original article https://www.gwern.net/GPT-3#the-database-prompt  Summary A single prompt describes transactions to and from a database.
GPT-3 is able to answer questions about the transactions that have taken place.
GPT-3 isn&amp;rsquo;t actually a database.
The LM simply understands language so well that describing the transactions that have taken place would naturally lead to the GPT-3 response.
The prompt 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55  title: &amp;#34;database example&amp;#34; doc: &amp;#34;GPT-3 as a NL interface for semantically querying logic in prose&amp;#34; prompt: |+The database begins knowing nothing.</description>
    </item>
    
    <item>
      <title>Translating Haskell to Clojure with GPT-3</title>
      <link>https://mullikine.github.io/posts/translating-haskell-to-clojure-with-gpt-3/</link>
      <pubDate>Sun, 07 Mar 2021 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/translating-haskell-to-clojure-with-gpt-3/</guid>
      <description>Relevant material https://hyperpolyglot.org/ml  Summary Who needs hyperpolyglot when you have GPT-3?
I translate Haskell into Clojure using the following prompt.
haskell-to-clojure.prompt 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50  title: &amp;#34;Translate Haskell to Clojure&amp;#34; prompt: |+Haskell: zip (map show [1,5,9]) [&amp;#34;a&amp;#34;,&amp;#34;b&amp;#34;,&amp;#34;c&amp;#34;] Clojure: (println (map vector &amp;#39;(1 2 3) &amp;#39;(4 5 6))) Haskell: map toUpper &amp;#34;MiXeD cAsE&amp;#34; Clojure: (clojure.</description>
    </item>
    
    <item>
      <title>crontab.guru in emacs and making a prompt with GPT-3 to copy it</title>
      <link>https://mullikine.github.io/posts/crontab-guru-in-emacs/</link>
      <pubDate>Sat, 06 Mar 2021 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/crontab-guru-in-emacs/</guid>
      <description>Related https://crontab.guru/  Summary I build some functionality into emacs to use crontab.guru behind the scenes to interpret tab lines displaying inside of emacs, without using the web browser.
I then build a GPT-3 prompt which does exactly the same thing without crontab.guru and provide the initial script I made to examplary (my GPT-3 DSL) as an example generator, to enhance the prompt if that is needed later.
Initial steps When lines in cron format appear in an emacs buffer, the crontab-guru function is suggested, allowing you to easily understand crontabs.</description>
    </item>
    
    <item>
      <title>Fine-tuning GPT-3</title>
      <link>https://mullikine.github.io/posts/fine-tuning-gpt-3/</link>
      <pubDate>Sun, 21 Feb 2021 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/fine-tuning-gpt-3/</guid>
      <description>Notes on GPT-3 fine-tuning http://github.com/mullikine/fine-tuning-gpt-3  Fine-tuning GPT-3 to generate puns Aims Train GPT-3 to continue on sequences of puns Train GPT-3 to speak in puns Training Data    format     jsonl       Sources
 https://github.com/taivop/joke-dataset       Naive approach
1 2 3  [{&amp;#34;data&amp;#34; : &amp;#34;joke set 1&amp;#34;}, {&amp;#34;data&amp;#34;: &amp;#34;joke set 2&amp;#34;}         Better approach</description>
    </item>
    
    <item>
      <title>Creating a playground for GPT-3 in emacs</title>
      <link>https://mullikine.github.io/posts/creating-a-playground-for-gpt-3-in-emacs/</link>
      <pubDate>Thu, 18 Feb 2021 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/creating-a-playground-for-gpt-3-in-emacs/</guid>
      <description>Code https://github.com/mullikine/pen.el Prompts https://github.com/mullikine/prompts meetup.com event https://www.meetup.com/Code-Craft-Dunedin/events/276407816/ Slides http://github.com/mullikine/presentation-prompt-engineering-in-emacs/blob/master/presentation.pdf  Demonstration 
Prompt-Engineering Part 1: Building an environment Summary of talk I received a key for the OpenAI API 2 weeks ago, just a couple of days before my birthday.
I had applied around 5 to 10 times in the last year and I had finally been given a key.
I decided since the hour is late I would work on productivity tools that anyone can use, to facilitate programming in the new paradigm.</description>
    </item>
    
    <item>
      <title>OpenAI API for NLP</title>
      <link>https://mullikine.github.io/posts/openai-api-for-nlp/</link>
      <pubDate>Fri, 12 Jun 2020 00:00:00 +1200</pubDate>
      
      <guid>https://mullikine.github.io/posts/openai-api-for-nlp/</guid>
      <description>OpenAI API https://beta.openai.com/  Summary I&amp;rsquo;d like to add some OpenAI support to emacs. GPT-3 is easy to integrate because it is a very general-purpose transformer. In a later article, I&amp;rsquo;d like to integrate more specialised huggingface transformers.
Results Here are some of the things I had managed to do with emacs and GPT-3 so far.
 Imaginary programming with GPT-3 // Bodacious Blog  Here I assemble an imaginary programming environment in GPT-3.</description>
    </item>
    
    <item>
      <title>Review of &#39;GPT-3: Language Models are Few-Shot Learners (Paper Explained) - YouTube&#39;</title>
      <link>https://mullikine.github.io/posts/review-of-gpt-3-language-models-are-few-shot-learners-paper-explained-youtube/</link>
      <pubDate>Sat, 30 May 2020 00:00:00 +0800</pubDate>
      
      <guid>https://mullikine.github.io/posts/review-of-gpt-3-language-models-are-few-shot-learners-paper-explained-youtube/</guid>
      <description>Work in progress
 Original video GPT-3: Language Models are Few-Shot Learners (Paper Explained) - YouTube  Demonstration of me reading this video and taking notes 
GPT-3 GPT 3 has 175 billion parameters which this is absolutely crazy is an order of magnitude higher than anything that ever existed.
GPT-2 parameters by comparison This is what people are talking about when they say parameters.
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  Parameters Single Transformer block Conv1d attn/c_attn w 768 2304 1769472 b 2304 2304 attn/c_proj w 768 768 589824 b 768 768 mlp/c_fc w 768 3072 2359296 b 768 768 mlp/c_proj w 3072 3072 2359296 b 768 768 Norm ln_1 g 768 768 b 768 768 ln_2 g 768 768 b 768 768 total 7085568 per block X 12 blocks 85026816 In all blocks Embeddings 50257 768 38597376 Positional Embeddings 2024 768 786432 Grand Total 124410624   What you can do with giant LMs?</description>
    </item>
    
    <item>
      <title>Review of &#39;Microsoft: &#34;The future of tech, with Kevin Scott and guests // Microsoft Build&#34;&#39;</title>
      <link>https://mullikine.github.io/posts/review-of-microsoft-the-future-of-tech-with-kevin-scott-and-guests-microsoft-build/</link>
      <pubDate>Tue, 12 May 2020 00:00:00 +0800</pubDate>
      
      <guid>https://mullikine.github.io/posts/review-of-microsoft-the-future-of-tech-with-kevin-scott-and-guests-microsoft-build/</guid>
      <description>Original video Microsoft: &amp;ldquo;The future of tech, with Kevin Scott and guests // Microsoft Build&amp;rdquo; Original article News OpenAI Model Generates Python Code - YouTube Original video from microsoft https://www.youtube.com/watch?v=fZSFNUT6iY8  Summary At 29 min you can see a demo of code generation from comments.
This is similar to what deep tabnine currently does, though probably uses a more powerful language model.
GitHub CodeSpaces and VSCode will probably have this built-in.</description>
    </item>
    
    <item>
      <title>Creating Infinitely Generated Text Adventures with DL LMs</title>
      <link>https://mullikine.github.io/posts/infinitely-generated-text-adventures/</link>
      <pubDate>Fri, 06 Dec 2019 00:00:00 +0800</pubDate>
      
      <guid>https://mullikine.github.io/posts/infinitely-generated-text-adventures/</guid>
      <description>Original article AI Dungeon 2: Creating Infinitely Generated Text Adventures with Deep Learning Language Models - Perception, Control, Cognition  For each action you type the model is fed the context sentence as well as the past N action- result pairs in its memory to generate the result.
We found N=8 to be a good amount of memory to feed the model.
Other times the model has difficulty keeping track of who is who, especially in dialogue.</description>
    </item>
    
    <item>
      <title>BERT</title>
      <link>https://mullikine.github.io/posts/bert/</link>
      <pubDate>Tue, 03 Dec 2019 00:00:00 +0800</pubDate>
      
      <guid>https://mullikine.github.io/posts/bert/</guid>
      <description> Original article BERT Explained: A Complete Guide with Theory and Tutorial - Towards Machine Learning  </description>
    </item>
    
    <item>
      <title>AI-assisted coding tools for emacs</title>
      <link>https://mullikine.github.io/posts/ai-assisted-coding-tools-emacs/</link>
      <pubDate>Wed, 27 Nov 2019 00:00:00 +0800</pubDate>
      
      <guid>https://mullikine.github.io/posts/ai-assisted-coding-tools-emacs/</guid>
      <description>language-detection Emacs Lisp library that automatically detects the programming language in a buffer or string. Implemented as a random forest classifier, trained in scikit-learn and deployed to Emacs Lisp.
https://github.com/andreasjansson/language-detection.el
Example of usage 1 2 3 4 5 6 7 8 9  (defun new-buffer-from-selection-detect-language () &amp;#34;Creates a new buffer from the selection and tries to set the mode&amp;#34; (interactive) (if (selected-p) (let* ((b (new-buffer-from-string (selection)))) (with-current-buffer b (switch-to-buffer b) (guess-major-mode))))) (defalias &amp;#39;detect-language-set-mode &amp;#39;guess-major-mode)   1 2  ;; This works great (my/truly-selective-binding &amp;#34;Y&amp;#34; #&amp;#39;new-buffer-from-selection-detect-language)   expanded the macro</description>
    </item>
    
    <item>
      <title>Coding faster with Deep TabNine</title>
      <link>https://mullikine.github.io/posts/using-deep-tabnine/</link>
      <pubDate>Thu, 14 Nov 2019 00:00:00 +0800</pubDate>
      
      <guid>https://mullikine.github.io/posts/using-deep-tabnine/</guid>
      <description>Example of using Deep TabNine to write with example code</description>
    </item>
    
    <item>
      <title>TensorFlow BERT</title>
      <link>https://mullikine.github.io/posts/tensorflow-bert-keras/</link>
      <pubDate>Thu, 31 Oct 2019 00:00:00 +0800</pubDate>
      
      <guid>https://mullikine.github.io/posts/tensorflow-bert-keras/</guid>
      <description>Original article Hugging Face: State-of-the-Art Natural Language Processing in ten lines of TensorFlow 2.0  A list of transformer architectures    architecture     BERT   RoBERTa   GPT-2   DistilBERT    pip&amp;rsquo;s transformers library Builds on 3 main classes:  configuration class tokenizer class model class  configuration class Hosts relevant information concerning the model we will be using, such as:</description>
    </item>
    
    <item>
      <title>Deep TabNine and emacs</title>
      <link>https://mullikine.github.io/posts/emacs-deep-tab-nine/</link>
      <pubDate>Wed, 30 Oct 2019 00:00:00 +0800</pubDate>
      
      <guid>https://mullikine.github.io/posts/emacs-deep-tab-nine/</guid>
      <description>Deep TabNine can supplement your autocompletion needs by providing you with example code.
 plugin https://github.com/TommyX12/company-tabnine/  Objective Have two bindings; one for regular company complete and one for tabnine.
   kb f     M-` company-complete   M-Tab company-tabnine    1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25  (require &amp;#39;company-tabnine) ;; TODO Fix ;; While browsing the completions list, if I press &amp;lt;space&amp;gt; then company aborts.</description>
    </item>
    
    <item>
      <title>(WIP) Notes on &#34;The Illustrated GPT-2&#34;</title>
      <link>https://mullikine.github.io/posts/notes-on-illustrated-gpt2/</link>
      <pubDate>Thu, 24 Oct 2019 00:00:00 +0800</pubDate>
      
      <guid>https://mullikine.github.io/posts/notes-on-illustrated-gpt2/</guid>
      <description>Original article The Illustrated GPT-2 (Visualizing Transformer Language Models) Jay Alammar Visualizing machine learning one concept at a time  Prereading Overview of The Illustrated Transformer // Bodacious Blog
Parameters When an article talks about the number of parameters, this is what it&amp;rsquo;s referring to.
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  Parameters Single Transformer block Conv1d attn/c_attn w 768 2304 1769472 b 2304 2304 attn/c_proj w 768 768 589824 b 768 768 mlp/c_fc w 768 3072 2359296 b 768 768 mlp/c_proj w 3072 3072 2359296 b 768 768 Norm ln_1 g 768 768 b 768 768 ln_2 g 768 768 b 768 768 total 7085568 per block X 12 blocks 85026816 In all blocks Embeddings 50257 768 38597376 Positional Embeddings 2024 768 786432 Grand Total 124410624   Goal Supplement The Illustrated Transformer with more visuals explaining the inner-workings of transformers, and how they’ve evolved since the original paper.</description>
    </item>
    
    <item>
      <title>Notes on &#34;Generating Beatles’ Lyrics with Machine Learning&#34;</title>
      <link>https://mullikine.github.io/posts/notes-on-generating-beatles-lyrics-with-machine-learning/</link>
      <pubDate>Thu, 24 Oct 2019 00:00:00 +0800</pubDate>
      
      <guid>https://mullikine.github.io/posts/notes-on-generating-beatles-lyrics-with-machine-learning/</guid>
      <description>Original article Generating Beatles Lyrics with Machine Learning - Towards Data Science  Apparatus     URL      code https://github.com/EugenHotaj/beatles    dataset http://toti.eu.com/beatles/index.asp github.com/EugenHotaj/beatles/blob/master/scraper.py   embeddings     algorithm      Hypothesis Aim Questions  How might might generate sentences from a language model?  Method Factoids Unigram model Ignores any conditioning and simply chooses the next word randomly from the training data.</description>
    </item>
    
    <item>
      <title>The Illustrated Transformer</title>
      <link>https://mullikine.github.io/posts/review-of-the-illustrated-transformer/</link>
      <pubDate>Sat, 19 Oct 2019 00:00:00 +0800</pubDate>
      
      <guid>https://mullikine.github.io/posts/review-of-the-illustrated-transformer/</guid>
      <description>Original article https://jalammar.github.io/illustrated-transformer/ Source https://github.com/tensorflow/tensor2tensor  Prereading https://jalammar.github.io/illustrated-word2vec/
Helpful glossary 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51  Multilayer perceptron Feed-Forward Neural Network FFNN Basically, these are multi-level logistic regression classifiers.</description>
    </item>
    
    <item>
      <title>gwern.net GPT-2</title>
      <link>https://mullikine.github.io/posts/review-of-gewn-gpt2/</link>
      <pubDate>Thu, 17 Oct 2019 00:00:00 +0800</pubDate>
      
      <guid>https://mullikine.github.io/posts/review-of-gewn-gpt2/</guid>
      <description>original article https://www.gwern.net/GPT-2  Read using eww The website when viewed with a GUI browser, although impressive, is distracting.
The website was built with hackyll, a static site generator employing haskell.
1  eww &amp;#34;https://www.gwern.net/GPT-2&amp;#34;   Ease of reading Straight-forward once you understand ML jargon.</description>
    </item>
    
    <item>
      <title>(WIP) Review of NLP tools</title>
      <link>https://mullikine.github.io/posts/review-of-nlp-tools/</link>
      <pubDate>Wed, 16 Oct 2019 00:00:00 +0800</pubDate>
      
      <guid>https://mullikine.github.io/posts/review-of-nlp-tools/</guid>
      <description>lm-explorer Interactive explorer for language models (currently only OpenAI GPT-2).</description>
    </item>
    
    <item>
      <title>writeup.ai</title>
      <link>https://mullikine.github.io/posts/review-of-writeup-ai/</link>
      <pubDate>Wed, 16 Oct 2019 00:00:00 +0800</pubDate>
      
      <guid>https://mullikine.github.io/posts/review-of-writeup-ai/</guid>
      <description>Original article https://senrigan.io/blog/how-writeupai-runs-behind-the-scenes/#h%5F6068056784021570782144062  Glossary 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26  cross entropy loss (between two probability distributions) (and over the same underlying set of events) Higher loss is bad. Measures the performance of a classification model whose output is a probability value between 0 and 1. Measures the average number of bits needed to identify an event from the set.</description>
    </item>
    
    <item>
      <title>PClean: A probabilistic scripting DSL</title>
      <link>https://mullikine.github.io/posts/pclean-gen-gpl/</link>
      <pubDate>Mon, 30 Sep 2019 00:00:00 +0800</pubDate>
      
      <guid>https://mullikine.github.io/posts/pclean-gen-gpl/</guid>
      <description>Links  MIT Probabilistic Computing Project Introduction | Gen GitHub - probcomp/Gen: A general-purpose probabilistic programming system with programmable inference  Other talks by MIT Probabilistic Computing Project: Videos, Talks, and Podcasts - MIT Probabilistic Computing Project
Tools Gen  a package for the Julia programming language. consists of multiple modeling languages that are implemented as DSLs in Julia and a Julia library for inference programming.  PClean  A probabilistic scripting DSL in the Gen package.</description>
    </item>
    
  </channel>
</rss>
