<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>math on Bodacious Blog</title>
    <link>https://mullikine.github.io/tags/math/</link>
    <description>Recent content in math on Bodacious Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 04 Nov 2019 00:00:00 +1300</lastBuildDate>
    
	<atom:link href="https://mullikine.github.io/tags/math/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Latex and Machine Learning</title>
      <link>https://mullikine.github.io/posts/machine-learning/</link>
      <pubDate>Mon, 04 Nov 2019 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/machine-learning/</guid>
      <description>Information Gain  Original article Information Gain and Mutual Information for Machine Learning  \begin{equation} \mathbf{IG}(\mathbf{S}, a) = \mathbf{H}(\mathbf{S}) – \mathbf{H}(\mathbf{S} | a) \end{equation}
Mutual information  References Information Gain and Mutual Information for Machine Learning An introduction to mutual information - YouTube  Concerns the outcome of two random variables.
If we know the value of one of the random variables in a system there is a corresponding reduction in uncertainty for predicting the other one and mutual information measures that reduction in uncertainty.</description>
    </item>
    
    <item>
      <title>Entropy, Cross-Entropy and KL-Divergence</title>
      <link>https://mullikine.github.io/posts/entropy-cross-entropy-and-kl-divergence/</link>
      <pubDate>Sat, 02 Nov 2019 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/entropy-cross-entropy-and-kl-divergence/</guid>
      <description>Original video A Short Introduction to Entropy, Cross-Entropy and KL-Divergence - YouTube Related reading https://blog.floydhub.com/knowledge-distillation/  Glossary 1 2 3 4 5 6 7 8 9 10 11 12 13  marginalized Treated as insignificant or peripheral. marginal likelihood function integrated likelihood model evidence evidence [#statistics] [#bayesian statistics] A likelihood function in which some parameter variables have been marginalized.   Predicted distribution vs true distribution Predicted distribution When designing a code to represent weather predictions, you try to assign fewer bits for outcomes which are probably going to be more common.</description>
    </item>
    
    <item>
      <title>Variational Inference</title>
      <link>https://mullikine.github.io/posts/variational-inference/</link>
      <pubDate>Fri, 01 Nov 2019 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/variational-inference/</guid>
      <description>Original article https://fabiandablander.com/r/Variational-Inference.html  Prereading https://mullikine.github.io/posts/entropy-cross-entropy-and-kl-divergence/
Bayes&amp;rsquo; Theorm 1 2 3 4 5 6 7 8 9 10 11  \begin{equation} \underbrace{p(\mathbf{z} \mid \mathbf{x})}_{\text{Posterior}} = \underbrace{p(\mathbf{z})}_{\text{Prior}} \times \frac{\overbrace{p(\mathbf{x} \mid \mat hbf{z})}^{\text{Likelihood}}}{\underbrace{\int p(\mathbf{x} \mid \mathbf{z}) \, p(\mathbf{z}) \, \mathrm{d}\mathbf{z}}_{\text{Marginal Likelihood}}} \enspace , \end{equation} where $\mathbf{z}$ denotes latent parameters we want to infer and $\mathbf{x}$ denotes data.   \begin{equation} \underbrace{p(\mathbf{z} \mid \mathbf{x})}_{\text{Posterior}} = \underbrace{p(\mathbf{z})}_{\text{Prior}} \times \frac{\overbrace{p(\mathbf{x} \mid \mathbf{z})}^{\text{Likelihood}}}{\underbrace{\int p(\mathbf{x} \mid \mathbf{z}) \, p(\mathbf{z}) \, \mathrm{d}\mathbf{z}}_{\text{Marginal Likelihood}}} \enspace , \end{equation}</description>
    </item>
    
    <item>
      <title>Just a fun bit of math in my day</title>
      <link>https://mullikine.github.io/posts/fun-math/</link>
      <pubDate>Fri, 25 Oct 2019 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/fun-math/</guid>
      <description>Useful mathematical symbols    symbol tex digraph how it reads      &amp;lt;:   is a subtype of    ⊧ \vDash &amp;lt;bar&amp;gt; = entails    ⊢ \vdash &amp;lt;bar&amp;gt; - infers    → \to -&amp;gt; is mapped to maps sets to sets   ↦ \mapsto &amp;lt;bar&amp;gt; &amp;gt; is mapped to maps elements to elements    Euler&amp;rsquo;s Characteristic The second most beautiful equation and its surprising applications - YouTube</description>
    </item>
    
    <item>
      <title>Scripting Mathematica</title>
      <link>https://mullikine.github.io/posts/scripting-mathematica/</link>
      <pubDate>Fri, 25 Oct 2019 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/scripting-mathematica/</guid>
      <description>Mathematica keyboard shortcuts https://reference.wolfram.com/language/tutorial/KeyboardShortcutListing.html
shell commands mmadoc Getting documentation with Mathematica code ?Integrate ?WolframAlpha#!/bin/bash export TTY funcname=&amp;#34;$1&amp;#34; mma &amp;#34;?$funcname&amp;#34; mma #!/bin/bash export TTY # Mathematica tf_script=&amp;#34;$(ux tf script wls || echo /dev/null)&amp;#34; trap &amp;#34;rm \&amp;#34;$tf_script\&amp;#34; 2&amp;gt;/dev/null&amp;#34; 0 cat &amp;gt; &amp;#34;$tf_script&amp;#34; &amp;lt;&amp;lt;HEREDOC Print[$@] HEREDOC wolframscript -file &amp;#34;$tf_script&amp;#34; &amp;#34;$@&amp;#34; Example mmadoc Integrate mmadoc WolframAlpha mma &amp;#34;WolframAlpha[\&amp;#34;size of the moon\&amp;#34;]&amp;#34; WolframAlpha[&amp;#34;size of the moon&amp;#34;] wa size of the moon mma &amp;#34;Integrate[5x,x]&amp;#34;Integrate[f, x] gives the indefinite integral ∫ f dx.</description>
    </item>
    
  </channel>
</rss>