<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>review on Bodacious Blog</title>
    <link>http://mullikine.github.io/tags/review/</link>
    <description>Recent content in review on Bodacious Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 02 Nov 2019 00:00:00 +1300</lastBuildDate><atom:link href="http://mullikine.github.io/tags/review/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Entropy, Cross-Entropy and KL-Divergence</title>
      <link>http://mullikine.github.io/posts/entropy-cross-entropy-and-kl-divergence/</link>
      <pubDate>Sat, 02 Nov 2019 00:00:00 +1300</pubDate>
      
      <guid>http://mullikine.github.io/posts/entropy-cross-entropy-and-kl-divergence/</guid>
      <description>Original video A Short Introduction to Entropy, Cross-Entropy and KL-Divergence - YouTube Related reading https://blog.floydhub.com/knowledge-distillation/  Glossary 1 2 3 4 5 6 7 8 9 10 11 12 13  marginalized Treated as insignificant or peripheral. marginal likelihood function integrated likelihood model evidence evidence [#statistics] [#bayesian statistics] A likelihood function in which some parameter variables have been marginalized.   Predicted distribution vs true distribution Predicted distribution When designing a code to represent weather predictions, you try to assign fewer bits for outcomes which are probably going to be more common.</description>
    </item>
    
    <item>
      <title>Variational Inference</title>
      <link>http://mullikine.github.io/posts/variational-inference/</link>
      <pubDate>Fri, 01 Nov 2019 00:00:00 +1300</pubDate>
      
      <guid>http://mullikine.github.io/posts/variational-inference/</guid>
      <description>Original article https://fabiandablander.com/r/Variational-Inference.html  Prereading https://mullikine.github.io/posts/entropy-cross-entropy-and-kl-divergence/
Bayes&#39; Theorm 1 2 3 4 5 6 7 8 9 10 11  \begin{equation} \underbrace{p(\mathbf{z} \mid \mathbf{x})}_{\text{Posterior}} = \underbrace{p(\mathbf{z})}_{\text{Prior}} \times \frac{\overbrace{p(\mathbf{x} \mid \mat hbf{z})}^{\text{Likelihood}}}{\underbrace{\int p(\mathbf{x} \mid \mathbf{z}) \, p(\mathbf{z}) \, \mathrm{d}\mathbf{z}}_{\text{Marginal Likelihood}}} \enspace , \end{equation} where $\mathbf{z}$ denotes latent parameters we want to infer and $\mathbf{x}$ denotes data.   \begin{equation} \underbrace{p(\mathbf{z} \mid \mathbf{x})}_{\text{Posterior}} = \underbrace{p(\mathbf{z})}_{\text{Prior}} \times \frac{\overbrace{p(\mathbf{x} \mid \mathbf{z})}^{\text{Likelihood}}}{\underbrace{\int p(\mathbf{x} \mid \mathbf{z}) , p(\mathbf{z}) , \mathrm{d}\mathbf{z}}_{\text{Marginal Likelihood}}} \enspace , \end{equation}</description>
    </item>
    
    <item>
      <title>(WIP) Notes on &#34;The Illustrated GPT-2&#34;</title>
      <link>http://mullikine.github.io/posts/notes-on-illustrated-gpt2/</link>
      <pubDate>Thu, 24 Oct 2019 00:00:00 +0800</pubDate>
      
      <guid>http://mullikine.github.io/posts/notes-on-illustrated-gpt2/</guid>
      <description>Original article The Illustrated GPT-2 (Visualizing Transformer Language Models) Jay Alammar Visualizing machine learning one concept at a time  Prereading Overview of The Illustrated Transformer // Bodacious Blog
Parameters When an article talks about the number of parameters, this is what it&amp;rsquo;s referring to.
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  Parameters Single Transformer block Conv1d attn/c_attn w 768 2304 1769472 b 2304 2304 attn/c_proj w 768 768 589824 b 768 768 mlp/c_fc w 768 3072 2359296 b 768 768 mlp/c_proj w 3072 3072 2359296 b 768 768 Norm ln_1 g 768 768 b 768 768 ln_2 g 768 768 b 768 768 total 7085568 per block X 12 blocks 85026816 In all blocks Embeddings 50257 768 38597376 Positional Embeddings 2024 768 786432 Grand Total 124410624   Goal Supplement The Illustrated Transformer with more visuals explaining the inner-workings of transformers, and how they’ve evolved since the original paper.</description>
    </item>
    
    <item>
      <title>Notes on &#34;Generating Beatles’ Lyrics with Machine Learning&#34;</title>
      <link>http://mullikine.github.io/posts/notes-on-generating-beatles-lyrics-with-machine-learning/</link>
      <pubDate>Thu, 24 Oct 2019 00:00:00 +0800</pubDate>
      
      <guid>http://mullikine.github.io/posts/notes-on-generating-beatles-lyrics-with-machine-learning/</guid>
      <description>Original article Generating Beatles Lyrics with Machine Learning - Towards Data Science  Apparatus     URL      code https://github.com/EugenHotaj/beatles    dataset http://toti.eu.com/beatles/index.asp github.com/EugenHotaj/beatles/blob/master/scraper.py   embeddings     algorithm      Hypothesis Aim Questions  How might might generate sentences from a language model?  Method Factoids Unigram model Ignores any conditioning and simply chooses the next word randomly from the training data.</description>
    </item>
    
    <item>
      <title>Notes on &#34;Identifying the right meaning of the words using BERT&#34;</title>
      <link>http://mullikine.github.io/posts/notes-on-identifying-the-right-meaning-of-the-words-using-bert/</link>
      <pubDate>Thu, 24 Oct 2019 00:00:00 +1300</pubDate>
      
      <guid>http://mullikine.github.io/posts/notes-on-identifying-the-right-meaning-of-the-words-using-bert/</guid>
      <description>Original article Identifying the right meaning of the words using BERT  Glossary 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  Uncased [model] The text has been lowercased before WordPiece tokenization, e.g., John Smith becomes john smith. The Uncased model also strips out any accent markers. Cased [model] The true case and accent markers are preserved.</description>
    </item>
    
    <item>
      <title>Notes on &#34;Math4IQB Hopfield Networks&#34;</title>
      <link>http://mullikine.github.io/posts/notes-on-math4iqb-hopfield-networks/</link>
      <pubDate>Thu, 24 Oct 2019 00:00:00 +1300</pubDate>
      
      <guid>http://mullikine.github.io/posts/notes-on-math4iqb-hopfield-networks/</guid>
      <description>Original video https://www.youtube.com/watch?v=gfPUWwBkXZY  Glossary information gain [data mining] The amount of information that&amp;#39;s gained by knowing the value of the attribute, which is the entropy of the distribution before the split minus the entropy of the distribution after it. The largest information gain is equivalent to the smallest entropy. vim +/&amp;#34;mutual information&amp;#34; &amp;#34;$NOTES/ws/glossaries/information-theory.txt&amp;#34; information gain ratio [#decision tree learning] Ratio of information gain to the intrinsic information. It was proposed by Ross Quinlan, to reduce a bias towards multi-valued attributes by taking the number and size of branches into account when choosing an attribute.</description>
    </item>
    
    <item>
      <title>(WIP) Notes on &#34;Natural Language Processing by Jacob Eisenstein&#34;</title>
      <link>http://mullikine.github.io/posts/notes-on-eisenstein-nlp/</link>
      <pubDate>Wed, 23 Oct 2019 00:00:00 +1300</pubDate>
      
      <guid>http://mullikine.github.io/posts/notes-on-eisenstein-nlp/</guid>
      <description>Reading vs +/&amp;#34;Kneser-Ney&amp;#34; $NOTES/ws/nlp-natural-language-processing/reading/eisenstein-nlp-notes.txt Kneser-Ney smoothing Based on absolute discounting, but it redistributes the resulting probability mass in a different way from Katz backoff.
Empirical evidence points to Kneser-Ney smoothing as the state-of-art for n-gram LMing.</description>
    </item>
    
    <item>
      <title>Scalable Python and bottlenecks</title>
      <link>http://mullikine.github.io/posts/scalable-python-and-bottlenecks/</link>
      <pubDate>Mon, 21 Oct 2019 00:00:00 +1300</pubDate>
      
      <guid>http://mullikine.github.io/posts/scalable-python-and-bottlenecks/</guid>
      <description>Original article https://instagram-engineering.com/python-at-scale-strict-modules-c0bb9245c834     term description see     import-time when imports are executed     Sources of slowness import re from mywebframework import db, route VALID_NAME_RE = re.compile(&amp;#34;^[a-zA-Z0-9]+$&amp;#34;) @route(&amp;#39;/&amp;#39;) def home(): return &amp;#34;Hello World!&amp;#34; class Person(db.Model): name: str  top level scope defines &amp;ndash; regex compilation decorator with unknown behaviour class definition: runs code contained within base class may have a meta-class or __init_subclass__ method.</description>
    </item>
    
    <item>
      <title>The Illustrated Transformer</title>
      <link>http://mullikine.github.io/posts/review-of-the-illustrated-transformer/</link>
      <pubDate>Sat, 19 Oct 2019 00:00:00 +0800</pubDate>
      
      <guid>http://mullikine.github.io/posts/review-of-the-illustrated-transformer/</guid>
      <description>Original article https://jalammar.github.io/illustrated-transformer/ Source https://github.com/tensorflow/tensor2tensor  Prereading https://jalammar.github.io/illustrated-word2vec/
Helpful glossary 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51  Multilayer perceptron Feed-Forward Neural Network FFNN Basically, these are multi-level logistic regression classifiers.</description>
    </item>
    
    <item>
      <title>gwern.net GPT-2</title>
      <link>http://mullikine.github.io/posts/review-of-gewn-gpt2/</link>
      <pubDate>Thu, 17 Oct 2019 00:00:00 +0800</pubDate>
      
      <guid>http://mullikine.github.io/posts/review-of-gewn-gpt2/</guid>
      <description>original article https://www.gwern.net/GPT-2  Read using eww The website when viewed with a GUI browser, although impressive, is distracting.
The website was built with hackyll, a static site generator employing haskell.
1  eww &amp;#34;https://www.gwern.net/GPT-2&amp;#34;   Ease of reading Straight-forward once you understand ML jargon.</description>
    </item>
    
    <item>
      <title>writeup.ai</title>
      <link>http://mullikine.github.io/posts/review-of-writeup-ai/</link>
      <pubDate>Wed, 16 Oct 2019 00:00:00 +0800</pubDate>
      
      <guid>http://mullikine.github.io/posts/review-of-writeup-ai/</guid>
      <description>Original article https://senrigan.io/blog/how-writeupai-runs-behind-the-scenes/#h%5F6068056784021570782144062  Glossary 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26  cross entropy loss (between two probability distributions) (and over the same underlying set of events) Higher loss is bad. Measures the performance of a classification model whose output is a probability value between 0 and 1. Measures the average number of bits needed to identify an event from the set.</description>
    </item>
    
    <item>
      <title>Coherence in Natural Language (2006)</title>
      <link>http://mullikine.github.io/posts/review-coherence-in-natural-language/</link>
      <pubDate>Mon, 14 Oct 2019 00:00:00 +1300</pubDate>
      
      <guid>http://mullikine.github.io/posts/review-coherence-in-natural-language/</guid>
      <description>Terminological mess The term cohesion does not figure in the index of the book.
Coherence vs cohesion &amp;ldquo;cohesion&amp;rdquo; for microstructures and &amp;ldquo;coherence&amp;rdquo; for macrostructures.
cohesion when talking about text and coherence when talking about discourse i.e. &amp;ldquo;semantics vs. pragmatics&amp;rdquo;
Issues covered in the book in the realm of coherence structures:
 cognitive science natural language engineering information extraction  Definitions Coherence structures ommitted: a chrestomathy of coherent vs incoherent text Reviews Coherence in natural language.</description>
    </item>
    
    <item>
      <title>(WIP) Applying Multinomial Naive Bayes to NLP Problems: A Practical Explanation</title>
      <link>http://mullikine.github.io/posts/review-multinomial-naive-bayes-nlp/</link>
      <pubDate>Fri, 11 Oct 2019 00:00:00 +1300</pubDate>
      
      <guid>http://mullikine.github.io/posts/review-multinomial-naive-bayes-nlp/</guid>
      <description>Applying Multinomial Naive Bayes to NLP Problems: A Practical Explanation</description>
    </item>
    
  </channel>
</rss>
