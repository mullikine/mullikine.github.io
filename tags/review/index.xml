<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>review on Bodacious Blog</title>
    <link>https://mullikine.github.io/tags/review/</link>
    <description>Recent content in review on Bodacious Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 21 Oct 2019 00:00:00 +1300</lastBuildDate>
    
	<atom:link href="https://mullikine.github.io/tags/review/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Scalable Python and bottlenecks</title>
      <link>https://mullikine.github.io/posts/scalable-python-and-bottlenecks/</link>
      <pubDate>Mon, 21 Oct 2019 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/scalable-python-and-bottlenecks/</guid>
      <description>Original article https://instagram-engineering.com/python-at-scale-strict-modules-c0bb9245c834     term description see     import-time when imports are executed     Sources of slowness import re from mywebframework import db, route VALID_NAME_RE = re.compile(&amp;#34;^[a-zA-Z0-9]+$&amp;#34;) @route(&amp;#39;/&amp;#39;) def home(): return &amp;#34;Hello World!&amp;#34; class Person(db.Model): name: str  top level scope defines &amp;ndash; regex compilation decorator with unknown behaviour class definition: runs code contained within base class may have a meta-class or __init_subclass__ method.</description>
    </item>
    
    <item>
      <title>The Illustrated Transformer</title>
      <link>https://mullikine.github.io/posts/review-of-the-illustrated-transformer/</link>
      <pubDate>Sat, 19 Oct 2019 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/review-of-the-illustrated-transformer/</guid>
      <description>Original article https://jalammar.github.io/illustrated-transformer/ Source https://github.com/tensorflow/tensor2tensor  Prereading https://jalammar.github.io/illustrated-word2vec/
Helpful glossary Multilayer perceptron Feed-Forward Neural Network FFNN Basically, these are multi-level logistic regression classifiers. Many layers of scales are separated by non-linearities. Can be used for as autoencoders. Can be used to train a classifier or extract functions as autoencoders. self-attention intra-attention [attention mechanism] Intuition: Reflects on its own position/context within a greater whole. Relates different positions of a single sequence in order to compute a representation of the sequence.</description>
    </item>
    
    <item>
      <title>gwern.net GPT-2</title>
      <link>https://mullikine.github.io/posts/review-of-gewn-gpt2/</link>
      <pubDate>Thu, 17 Oct 2019 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/review-of-gewn-gpt2/</guid>
      <description>original article https://www.gwern.net/GPT-2  Read using eww The website when viewed with a GUI browser, although impressive, is distracting.
The website was built with hackyll, a static site generator employing haskell.
eww &amp;#34;https://www.gwern.net/GPT-2&amp;#34; Ease of reading Straight-forward once you understand ML jargon.</description>
    </item>
    
    <item>
      <title>writeup.ai</title>
      <link>https://mullikine.github.io/posts/review-of-writeup-ai/</link>
      <pubDate>Wed, 16 Oct 2019 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/review-of-writeup-ai/</guid>
      <description>Original article https://senrigan.io/blog/how-writeupai-runs-behind-the-scenes/#h%5F6068056784021570782144062  Glossary cross entropy loss (between two probability distributions) (and over the same underlying set of events) Higher loss is bad. Measures the performance of a classification model whose output is a probability value between 0 and 1. Measures the average number of bits needed to identify an event from the set. Data scrubbing [error correction technique] Uses a background task to periodically inspect main memory or storage for errors, then correct detected errors using redundant data in the form of different checksums or copies of data.</description>
    </item>
    
    <item>
      <title>Coherence in Natural Language (2006)</title>
      <link>https://mullikine.github.io/posts/review-coherence-in-natural-language/</link>
      <pubDate>Mon, 14 Oct 2019 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/review-coherence-in-natural-language/</guid>
      <description>Terminological mess The term cohesion does not figure in the index of the book.
Coherence vs cohesion &amp;ldquo;cohesion&amp;rdquo; for microstructures and &amp;ldquo;coherence&amp;rdquo; for macrostructures.
cohesion when talking about text and coherence when talking about discourse i.e. &amp;ldquo;semantics vs. pragmatics&amp;rdquo;
Issues covered in the book in the realm of coherence structures:
 cognitive science natural language engineering information extraction  Definitions Coherence structures ommitted: a chrestomathy of coherent vs incoherent text Reviews Coherence in natural language.</description>
    </item>
    
    <item>
      <title>(WIP) Applying Multinomial Naive Bayes to NLP Problems: A Practical Explanation</title>
      <link>https://mullikine.github.io/posts/review-multinomial-naive-bayes-nlp/</link>
      <pubDate>Fri, 11 Oct 2019 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/review-multinomial-naive-bayes-nlp/</guid>
      <description>Applying Multinomial Naive Bayes to NLP Problems: A Practical Explanation</description>
    </item>
    
  </channel>
</rss>