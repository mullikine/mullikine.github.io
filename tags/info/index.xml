<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>info on Bodacious Blog</title>
    <link>https://mullikine.github.io/tags/info/</link>
    <description>Recent content in info on Bodacious Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 04 Nov 2019 00:00:00 +1300</lastBuildDate>
    
	<atom:link href="https://mullikine.github.io/tags/info/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Latex and Machine Learning</title>
      <link>https://mullikine.github.io/posts/machine-learning/</link>
      <pubDate>Mon, 04 Nov 2019 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/machine-learning/</guid>
      <description>Information Gain  Original article Information Gain and Mutual Information for Machine Learning  \begin{equation} \mathbf{IG}(\mathbf{S}, a) = \mathbf{H}(\mathbf{S}) â€“ \mathbf{H}(\mathbf{S} | a) \end{equation}
Mutual information  References Information Gain and Mutual Information for Machine Learning \
An introduction to mutual information - YouTube  Concerns the outcome of two random variables.
If we know the value of one of the random variables in a system there is a corresponding reduction in uncertainty for predicting the other one and mutual information measures that reduction in uncertainty.</description>
    </item>
    
    <item>
      <title>Entropy, Cross-Entropy and KL-Divergence</title>
      <link>https://mullikine.github.io/posts/entropy-cross-entropy-and-kl-divergence/</link>
      <pubDate>Sat, 02 Nov 2019 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/entropy-cross-entropy-and-kl-divergence/</guid>
      <description>Original video A Short Introduction to Entropy, Cross-Entropy and KL-Divergence - YouTube Related reading https://blog.floydhub.com/knowledge-distillation/  Glossary 1 2 3 4 5 6 7 8 9 10 11 12 13  marginalized Treated as insignificant or peripheral. marginal likelihood function integrated likelihood model evidence evidence [#statistics] [#bayesian statistics] A likelihood function in which some parameter variables have been marginalized.   Predicted distribution vs true distribution Predicted distribution When designing a code to represent weather predictions, you try to assign fewer bits for outcomes which are probably going to be more common.</description>
    </item>
    
    <item>
      <title>Variational Inference</title>
      <link>https://mullikine.github.io/posts/variational-inference/</link>
      <pubDate>Fri, 01 Nov 2019 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/variational-inference/</guid>
      <description>Original article https://fabiandablander.com/r/Variational-Inference.html  Prereading https://mullikine.github.io/posts/entropy-cross-entropy-and-kl-divergence/
Bayes&amp;rsquo; Theorm 1 2 3 4 5 6 7 8 9 10 11  \begin{equation} \underbrace{p(\mathbf{z} \mid \mathbf{x})}_{\text{Posterior}} = \underbrace{p(\mathbf{z})}_{\text{Prior}} \times \frac{\overbrace{p(\mathbf{x} \mid \mat hbf{z})}^{\text{Likelihood}}}{\underbrace{\int p(\mathbf{x} \mid \mathbf{z}) \, p(\mathbf{z}) \, \mathrm{d}\mathbf{z}}_{\text{Marginal Likelihood}}} \enspace , \end{equation} where $\mathbf{z}$ denotes latent parameters we want to infer and $\mathbf{x}$ denotes data.   \begin{equation} \underbrace{p(\mathbf{z} \mid \mathbf{x})}_{\text{Posterior}} = \underbrace{p(\mathbf{z})}_{\text{Prior}} \times \frac{\overbrace{p(\mathbf{x} \mid \mathbf{z})}^{\text{Likelihood}}}{\underbrace{\int p(\mathbf{x} \mid \mathbf{z}) , p(\mathbf{z}) , \mathrm{d}\mathbf{z}}_{\text{Marginal Likelihood}}} \enspace , \end{equation}</description>
    </item>
    
    <item>
      <title>Notes on &#34;Math4IQB Hopfield Networks&#34;</title>
      <link>https://mullikine.github.io/posts/notes-on-math4iqb-hopfield-networks/</link>
      <pubDate>Thu, 24 Oct 2019 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/notes-on-math4iqb-hopfield-networks/</guid>
      <description>Original video https://www.youtube.com/watch?v=gfPUWwBkXZY  Glossary information gain [data mining] The amount of information that&amp;#39;s gained by knowing the value of the attribute, which is the entropy of the distribution before the split minus the entropy of the distribution after it. The largest information gain is equivalent to the smallest entropy. vim +/&amp;#34;mutual information&amp;#34; &amp;#34;$NOTES/ws/glossaries/information-theory.txt&amp;#34; information gain ratio [#decision tree learning] Ratio of information gain to the intrinsic information. It was proposed by Ross Quinlan, to reduce a bias towards multi-valued attributes by taking the number and size of branches into account when choosing an attribute.</description>
    </item>
    
    <item>
      <title>Blogs and Vlogs</title>
      <link>https://mullikine.github.io/posts/blogs-and-vlogs/</link>
      <pubDate>Mon, 14 Oct 2019 00:00:00 +0800</pubDate>
      
      <guid>https://mullikine.github.io/posts/blogs-and-vlogs/</guid>
      <description>Blogs and vlogs These people have been influential to me.
2019    source type url Topics     Math4IQB vlog Math4IQB - YouTube Information Theory   Gwern Bransen blog GPT-2 Neural Network Poetry Gwern.net Haskell, NLP, R   Fabian Dablander blog Fabian Dablander - PhD Student Methods Statistics Math, statistics, latex   John Kitchen blog The Kitchin Research Group Emacs   John D.</description>
    </item>
    
  </channel>
</rss>