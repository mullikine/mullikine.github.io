<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>info on Bodacious Blog</title>
    <link>https://mullikine.github.io/tags/info/</link>
    <description>Recent content in info on Bodacious Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 02 Nov 2019 00:00:00 +1300</lastBuildDate>
    
	<atom:link href="https://mullikine.github.io/tags/info/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Entropy, Cross-Entropy and KL-Divergence</title>
      <link>https://mullikine.github.io/posts/entropy-cross-entropy-and-kl-divergence/</link>
      <pubDate>Sat, 02 Nov 2019 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/entropy-cross-entropy-and-kl-divergence/</guid>
      <description>Original video A Short Introduction to Entropy, Cross-Entropy and KL-Divergence - YouTube  Glossary entropy expected information Measures the average amount of information that you get when you learn the weather each day, or more generally the average amount of information that you get from one sample drawn from a given probability distribution p. It tells you how unpredictable that probability distribution is. If you live in the middle of a desert where it’s sunny every day, on average you won’t get much information from the weather station.</description>
    </item>
    
    <item>
      <title>Variational Inference</title>
      <link>https://mullikine.github.io/posts/variational-inference/</link>
      <pubDate>Fri, 01 Nov 2019 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/variational-inference/</guid>
      <description>Original article https://fabiandablander.com/r/Variational-Inference.html  Prereading https://mullikine.github.io/posts/entropy-cross-entropy-and-kl-divergence/
Bayes&amp;rsquo; Theorm \begin{equation} \underbrace{p(\mathbf{z} \mid \mathbf{x})}_{\text{Posterior}} = \underbrace{p(\mathbf{z})}_{\text{Prior}} \times \frac{\overbrace{p(\mathbf{x} \mid \mat hbf{z})}^{\text{Likelihood}}}{\underbrace{\int p(\mathbf{x} \mid \mathbf{z}) \, p(\mathbf{z}) \, \mathrm{d}\mathbf{z}}_{\text{Marginal Likelihood}}} \enspace , \end{equation} where $\mathbf{z}$ denotes latent parameters we want to infer and $\mathbf{x}$ denotes data. \begin{equation} \underbrace{p(\mathbf{z} \mid \mathbf{x})}_{\text{Posterior}} = \underbrace{p(\mathbf{z})}_{\text{Prior}} \times \frac{\overbrace{p(\mathbf{x} \mid \mathbf{z})}^{\text{Likelihood}}}{\underbrace{\int p(\mathbf{x} \mid \mathbf{z}) \, p(\mathbf{z}) \, \mathrm{d}\mathbf{z}}_{\text{Marginal Likelihood}}} \enspace , \end{equation}
where \(\mathbf{z}\) denotes latent parameters we want to infer and \(\mathbf{x}\) denotes data.</description>
    </item>
    
    <item>
      <title>Notes on &#34;Math4IQB Hopfield Networks&#34;</title>
      <link>https://mullikine.github.io/posts/notes-on-math4iqb-hopfield-networks/</link>
      <pubDate>Thu, 24 Oct 2019 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/notes-on-math4iqb-hopfield-networks/</guid>
      <description>Original video https://www.youtube.com/watch?v=gfPUWwBkXZY  Glossary information gain [data mining] The amount of information that&amp;#39;s gained by knowing the value of the attribute, which is the entropy of the distribution before the split minus the entropy of the distribution after it. The largest information gain is equivalent to the smallest entropy. vim +/&amp;#34;mutual information&amp;#34; &amp;#34;$NOTES/ws/glossaries/information-theory.txt&amp;#34; information gain ratio [#decision tree learning] Ratio of information gain to the intrinsic information. It was proposed by Ross Quinlan, to reduce a bias towards multi-valued attributes by taking the number and size of branches into account when choosing an attribute.</description>
    </item>
    
    <item>
      <title>Blogs and Vlogs</title>
      <link>https://mullikine.github.io/posts/blogs-and-vlogs/</link>
      <pubDate>Mon, 14 Oct 2019 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/blogs-and-vlogs/</guid>
      <description> These are some of my favourite learning resources.
   source type url Topics     Math4IQB vlog https://www.youtube.com/user/Math4IQB/videos Information Theory   Gwern Bransen blog https://www.gwern.net/GPT-2 Haskell, NLP, R    Misc articles NLP writeup.ai https://senrigan.io/blog/how-writeupai-runs-behind-the-scenes/#h%5F6068056784021570782144062
Text generator with Keras https://www.thepythoncode.com/article/text-generation-keras-python
Keyword extraction  nltk, sklearn
https://medium.com/analytics-vidhya/automated-keyword-extraction-from-articles-using-nlp-bfd864f41b34
 numpy, spacy
https://towardsdatascience.com/textrank-for-keyword-extraction-by-python-c0bae21bcec0
 ngram, modified skip-gram, spacy
https://medium.com/reputation-com-datascience-blog/keywords-extraction-with-ngram-and-modified-skip-gram-based-on-spacy-14e5625fce23
  </description>
    </item>
    
  </channel>
</rss>