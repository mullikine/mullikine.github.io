<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>transformer on Bodacious Blog</title>
    <link>https://mullikine.github.io/tags/transformer/</link>
    <description>Recent content in transformer on Bodacious Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 18 Nov 2020 00:00:00 +1300</lastBuildDate>
    
	<atom:link href="https://mullikine.github.io/tags/transformer/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Suggesting new words for the glossary with KeyBERT and pytextrank</title>
      <link>https://mullikine.github.io/posts/suggesting-new-words-for-the-glossary-with-keybert-and-pytextrank/</link>
      <pubDate>Wed, 18 Nov 2020 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/suggesting-new-words-for-the-glossary-with-keybert-and-pytextrank/</guid>
      <description>Demo: Adding to the glossary using suggested keyphrases 
Fleshing out a glossary on microbiology 
Adding words from Stackexchange 
Code Python pytextrank turned out to be more fit for purpose than KeyBERT.
The issue with KeyBERT is that it is more abstractive in creating keywords, suggesting keywords omitting stopwords, etc.. KeyBERT is good for finding topic keywords, but not great at finding extractive (as opposed to abstractive) keywords.</description>
    </item>
    
    <item>
      <title>Rewrite of &#39;Understanding Transformers in NLP: State-of-the-Art Models&#39;</title>
      <link>https://mullikine.github.io/posts/review-of-understanding-transformers-in-nlp-state-of-the-art-models/</link>
      <pubDate>Wed, 10 Jun 2020 00:00:00 +0800</pubDate>
      
      <guid>https://mullikine.github.io/posts/review-of-understanding-transformers-in-nlp-state-of-the-art-models/</guid>
      <description>Original article Understanding Transformers in NLP: State-of-the-Art Models  Table of Contents  Sequence-to-Sequence Models &amp;ndash; A Backdrop  RNN based Sequence-to-Sequence Model Challenges   Introduction to the Transformer in NLP  Understanding the Model Architecture Grokking Self-Attention Calculation of Self-Attention Limitations of the Transformer   Understanding Transformer-XL  Using Transformer for Language Modeling Using Transformer-XL for Language Modeling   The New Sensation in NLP: Google&amp;rsquo;s BERT  Model Architecture BERT Pre-Training Tasks    Sequence-to-Sequence Models &amp;ndash; A Backdrop seq2seq models in NLP are used to convert sequences of Type A to sequences of Type B.</description>
    </item>
    
    <item>
      <title>Review of &#39;Google AI Blog: Reformer: The Efficient Transformer&#39;</title>
      <link>https://mullikine.github.io/posts/review-of-google-ai-blog-reformer-the-efficient-transformer/</link>
      <pubDate>Mon, 20 Jan 2020 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/review-of-google-ai-blog-reformer-the-efficient-transformer/</guid>
      <description>Original article Google AI Blog: Reformer: The Efficient Transformer  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32  Reformer [Transformer model] Designed to handle context windows of up to 1 million words, all on a single accelerator and using only 16GB of memory. It combines two crucial techniques to solve the problems of attention and memory allocation that limit Transformer’s application to long context windows.</description>
    </item>
    
    <item>
      <title>Notes on BERT</title>
      <link>https://mullikine.github.io/posts/google-bert/</link>
      <pubDate>Sat, 16 Nov 2019 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/google-bert/</guid>
      <description>https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html
Pre-trained representations can either be context-free or contextual, and contextual representations can further be unidirectional or bidirectional.
1 2 3 4 5  node [style=filled,fillcolor=lightgrey,shape=box]; &amp;#34;Pre-trained representation&amp;#34; -&amp;gt; &amp;#34;context-free&amp;#34; &amp;#34;Pre-trained representation&amp;#34; -&amp;gt; contextual contextual -&amp;gt; unidirectional contextual -&amp;gt; bidirectional        LM supervision contextual Pre-trained Training data bi-directional     BERT unsupervised ✓ ✓ Plain text ✓   word2vec semi/self-supervised ✗      GloVe semi-supervised ✗       Supervision is a bit grey  Supervised If you consider that the network has to learn from it’s errors through back prop.</description>
    </item>
    
    <item>
      <title>The Illustrated Transformer</title>
      <link>https://mullikine.github.io/posts/review-of-the-illustrated-transformer/</link>
      <pubDate>Sat, 19 Oct 2019 00:00:00 +0800</pubDate>
      
      <guid>https://mullikine.github.io/posts/review-of-the-illustrated-transformer/</guid>
      <description>Original article https://jalammar.github.io/illustrated-transformer/ Source https://github.com/tensorflow/tensor2tensor  Prereading https://jalammar.github.io/illustrated-word2vec/
Helpful glossary 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51  Multilayer perceptron Feed-Forward Neural Network FFNN Basically, these are multi-level logistic regression classifiers.</description>
    </item>
    
  </channel>
</rss>