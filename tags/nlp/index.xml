<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>NLP on Bodacious Blog</title>
    <link>https://mullikine.github.io/tags/nlp/</link>
    <description>Recent content in NLP on Bodacious Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 18 Nov 2020 00:00:00 +1300</lastBuildDate>
    
	<atom:link href="https://mullikine.github.io/tags/nlp/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Suggesting new words for the glossary with KeyBERT and pytextrank</title>
      <link>https://mullikine.github.io/posts/suggesting-new-words-for-the-glossary-with-keybert-and-pytextrank/</link>
      <pubDate>Wed, 18 Nov 2020 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/suggesting-new-words-for-the-glossary-with-keybert-and-pytextrank/</guid>
      <description>Demo: Adding to the glossary using suggested keyphrases 
Code Python pytextrank turned out to be more fit for purpose than KeyBERT.
The issue with KeyBERT is that it is more abstractive in creating keywords, suggesting keywords omitting stopwords, etc.. KeyBERT is good for finding topic keywords, but not great at finding extractive (as opposed to abstractive) keywords.
pytextrank
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28  #!</description>
    </item>
    
    <item>
      <title>Setting up opensemanticsearch, fixing a docker-compose bug, making a PR, or 2</title>
      <link>https://mullikine.github.io/posts/setting-up-opensemanticsearch-fixing-a-docker-compose-bug-making-a-pr/</link>
      <pubDate>Fri, 13 Nov 2020 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/setting-up-opensemanticsearch-fixing-a-docker-compose-bug-making-a-pr/</guid>
      <description>Synopsis I&amp;rsquo;d like to use opensemanticsearch to index my own filesystem.
I&amp;rsquo;d like to use Apache Tika to create a text mirror of my file system in place of my own, makeshift text mirroring software.
I&amp;rsquo;d like to make use of opensemanticsearch&#39;s spacy-services module.
Demo: Running Solr, Tika and spacy-services servers 
A bug in starting the docker composure It appears that somebody else has also encountered this.
https://github.com/opensemanticsearch/open-semantic-search/issues/329
Error 1 2 3  x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -D__LITTLE_ENDIAN__=1 -I/usr/include/python3.</description>
    </item>
    
    <item>
      <title>Updates and demonstrations of the glossary system</title>
      <link>https://mullikine.github.io/posts/updates-and-demonstrations-of-the-glossary-system/</link>
      <pubDate>Sat, 24 Oct 2020 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/updates-and-demonstrations-of-the-glossary-system/</guid>
      <description>Demonstrations Learning clojure Here I demonstrate how easy it is to see and navigate glossary items highlighted across code, documentation and websites.

Learning the language of Lord of the Rings 
Learning archaic English from the KJV Bible</description>
    </item>
    
    <item>
      <title>Updates to the emacs glossary system</title>
      <link>https://mullikine.github.io/posts/updates-to-the-emacs-glossary-system/</link>
      <pubDate>Sat, 17 Oct 2020 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/updates-to-the-emacs-glossary-system/</guid>
      <description>Key bindings    kb f      L glossary-add-link Create a link for a term to a topic. From then on, that topic&amp;rsquo;s glossary is loaded when the term is seen.   A add-to-glossary-file-for-buffer Add a new term to one of the most relevant glossaries.    Creating / adding to the ansible glossary 
Learning Lord of the Rings lore as I&amp;rsquo;m reading</description>
    </item>
    
    <item>
      <title>Glossaries for learning in emacs</title>
      <link>https://mullikine.github.io/posts/glossaries-for-learning-in-emacs/</link>
      <pubDate>Fri, 09 Oct 2020 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/glossaries-for-learning-in-emacs/</guid>
      <description>Overview of the glossary system Purpose The glossary system is like a wiki system, but more dynamic.
Glossaries can be overlain onto anything &amp;ndash; books, websites, Facebook conversations, code, etc.
The glossary system provides the following abilities:
 Document-specific glossaries Topic-specific glossaries Visual set functions (Union and difference) for glossaries Highlighting to see what has been entered into the glossary already  The glossary system allows me to read, ingest and learn from documents, seeing what I know from a glance.</description>
    </item>
    
    <item>
      <title>Generating poetry with GPT-2</title>
      <link>https://mullikine.github.io/posts/generating-poetry-with-gpt-2/</link>
      <pubDate>Thu, 27 Aug 2020 00:00:00 +1200</pubDate>
      
      <guid>https://mullikine.github.io/posts/generating-poetry-with-gpt-2/</guid>
      <description>Resources https://github.com/kylemcdonald/gpt-2-poetry https://kylemcdonald.github.io/gpt-2-poetry/ Final code https://github.com/mullikine/gpt-2  Running the Dockerfile.cpu 
Original steps  Download the HTML from poetryfoundation.org based on the urls in romantic-urls.txt. https://gist.github.com/bea516e3726d0a0ab139bca534a43fe9   Use Parse Poetry.py to extract
 title author poem    Save that data to output/.
 The metadata is contained in the first few lines.    Use Generate GPT-2.py to generate poems based on random chunks from the poems and the seed words.</description>
    </item>
    
    <item>
      <title>Review of &#39;Google AI Blog: PEGASUS: A State-of-the-Art Model for Abstractive Text Summarization&#39;</title>
      <link>https://mullikine.github.io/posts/review-of-google-ai-blog-pegasus-a-state-of-the-art-model-for-abstractive-text-summarization/</link>
      <pubDate>Sat, 13 Jun 2020 00:00:00 +0800</pubDate>
      
      <guid>https://mullikine.github.io/posts/review-of-google-ai-blog-pegasus-a-state-of-the-art-model-for-abstractive-text-summarization/</guid>
      <description>Original article Google AI Blog: PEGASUS: A State-of-the-Art Model for Abstractive Text Summarization Source code GitHub - google-research/pegasus text summarization one of the most challenging tasks in natural language processing, involving understanding of long passages, information compression, and language generation.  The dominant paradigm for training ML models to do this is seq2seq learning, where a NN learns to map input sequences to output sequences.
seq2seq models were initially developed using RNN.</description>
    </item>
    
    <item>
      <title>OpenAI API for NLP</title>
      <link>https://mullikine.github.io/posts/openai-api-for-nlp/</link>
      <pubDate>Fri, 12 Jun 2020 00:00:00 +0800</pubDate>
      
      <guid>https://mullikine.github.io/posts/openai-api-for-nlp/</guid>
      <description>OpenAI API https://beta.openai.com/  Related articles I&amp;rsquo;d like to add some OpenAI support to emacs.
 Emacs Websockets // Bodacious Blog  API  Apply the API to any language task  semantic search, summarization, sentiment analysis, content generation, translation, and more&amp;hellip;    Use only a few examples or by specifying your task in English.
Semantic search 1 2 3 4 5 6  Semantic Search [openai-api] Allows searching over documents based on the natural-language meaning of queries rather than keyword matching.</description>
    </item>
    
    <item>
      <title>Rewrite of &#39;Understanding Transformers in NLP: State-of-the-Art Models&#39;</title>
      <link>https://mullikine.github.io/posts/review-of-understanding-transformers-in-nlp-state-of-the-art-models/</link>
      <pubDate>Wed, 10 Jun 2020 00:00:00 +0800</pubDate>
      
      <guid>https://mullikine.github.io/posts/review-of-understanding-transformers-in-nlp-state-of-the-art-models/</guid>
      <description>Original article Understanding Transformers in NLP: State-of-the-Art Models  Table of Contents  Sequence-to-Sequence Models &amp;ndash; A Backdrop  RNN based Sequence-to-Sequence Model Challenges   Introduction to the Transformer in NLP  Understanding the Model Architecture Grokking Self-Attention Calculation of Self-Attention Limitations of the Transformer   Understanding Transformer-XL  Using Transformer for Language Modeling Using Transformer-XL for Language Modeling   The New Sensation in NLP: Google&amp;rsquo;s BERT  Model Architecture BERT Pre-Training Tasks    Sequence-to-Sequence Models &amp;ndash; A Backdrop seq2seq models in NLP are used to convert sequences of Type A to sequences of Type B.</description>
    </item>
    
    <item>
      <title>Digest of &#39;Self Supervised Representation Learning in NLP&#39;</title>
      <link>https://mullikine.github.io/posts/review-of-self-supervised-representation-learning-in-nlp/</link>
      <pubDate>Mon, 25 May 2020 00:00:00 +0800</pubDate>
      
      <guid>https://mullikine.github.io/posts/review-of-self-supervised-representation-learning-in-nlp/</guid>
      <description>Original article Self Supervised Representation Learning in NLP   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70  Self-Supervised Learning Autonomous supervised learning.</description>
    </item>
    
    <item>
      <title>Using Facebook&#39;s Duckling parser</title>
      <link>https://mullikine.github.io/posts/using-facebook-s-duckling-parser/</link>
      <pubDate>Mon, 25 May 2020 00:00:00 +0800</pubDate>
      
      <guid>https://mullikine.github.io/posts/using-facebook-s-duckling-parser/</guid>
      <description>Demonstration 
Obtain the docker image 1  docker pull rasa/duckling   Create duckling-parse script 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  #!/bin/bash export TTY input=&amp;#34;$(urlencode)&amp;#34; set -m ( unbuffer docker \  run \  --name duckling-container \  --rm \  --network=host \  --entrypoint= \  rasa/duckling:latest \  duckling-example-exe -p 8000 --no-access-log --no-error-log ) &amp;amp;&amp;gt;/dev/null &amp;amp; sleep 2 /usr/bin/curl -XPOST http://0.</description>
    </item>
    
    <item>
      <title>Review of &#39;Microsoft: &#34;The future of tech, with Kevin Scott and guests // Microsoft Build&#34;&#39;</title>
      <link>https://mullikine.github.io/posts/review-of-microsoft-the-future-of-tech-with-kevin-scott-and-guests-microsoft-build/</link>
      <pubDate>Tue, 12 May 2020 00:00:00 +0800</pubDate>
      
      <guid>https://mullikine.github.io/posts/review-of-microsoft-the-future-of-tech-with-kevin-scott-and-guests-microsoft-build/</guid>
      <description>Original video Microsoft: &amp;ldquo;The future of tech, with Kevin Scott and guests // Microsoft Build&amp;rdquo; Original article News OpenAI Model Generates Python Code - YouTube Original video from microsoft https://www.youtube.com/watch?v=fZSFNUT6iY8  Summary At 29 min you can see a demo of code generation from comments.
This is similar to what deep tabnine currently does, though probably uses a more powerful language model.
GitHub CodeSpaces and VSCode will probably have this built-in.</description>
    </item>
    
    <item>
      <title>Review of &#39;Polyglot Word Embeddings Discover Language Clusters&#39;</title>
      <link>https://mullikine.github.io/posts/review-of-polyglot-word-embeddings-discover-language-clusters/</link>
      <pubDate>Tue, 04 Feb 2020 00:00:00 +0800</pubDate>
      
      <guid>https://mullikine.github.io/posts/review-of-polyglot-word-embeddings-discover-language-clusters/</guid>
      <description>Original article Polyglot Word Embeddings Discover Language Clusters  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28  Word2Vec A group of models that tries to represent each word in a large text as a vector in a space of N dimensions (which we will call features) making similar words also be close to each other.</description>
    </item>
    
    <item>
      <title>Parsr</title>
      <link>https://mullikine.github.io/posts/parsr/</link>
      <pubDate>Tue, 21 Jan 2020 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/parsr/</guid>
      <description>Playing around with lazydocker, Parsr and jq 
Running the parser 1  cd &amp;#34;$MYGIT/axa-group/Parsr/docs&amp;#34;; npm run run:debug -- --input-file samples/bitcoin.pdf --output-folder dist/ --document-name example --config server/defaultConfig.json --pretty-logs   1  vs $MYGIT/axa-group/Parsr/dist/example.txt   1  cat $MYGIT/axa-group/Parsr/dist/example.json | jiq   Parsr is kinda cool It parses documents, provides you with a gui to find what you want and then a json parse tree.
  1  cat $MYGIT/axa-group/Parsr/dist/example.</description>
    </item>
    
    <item>
      <title>Review of &#39;Language Engineering; Harnessing the Power of Language (2004)&#39;</title>
      <link>https://mullikine.github.io/posts/review-of-language-engineering-harnessing-the-power-of-language/</link>
      <pubDate>Tue, 17 Dec 2019 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/review-of-language-engineering-harnessing-the-power-of-language/</guid>
      <description>This article is from 2004 or earlier.
It&amp;rsquo;s still an interesting insight into perspectives on Language Engineering from the early 2000s.
 Original article Language Engineering; Harnessing the Power of Language  The use of language is currently restricted.
Even between humans, understanding is usually limited to those groups who share a common language. †
 † &amp;hellip;let alone the language barrier with computers.  Language can be seen as much a barrier to communication as an aid.</description>
    </item>
    
    <item>
      <title>Review of &#39;Indexing Billions of Text Vectors&#39;</title>
      <link>https://mullikine.github.io/posts/review-of-indexing-billions-of-text-vectors/</link>
      <pubDate>Tue, 10 Dec 2019 00:00:00 +0800</pubDate>
      
      <guid>https://mullikine.github.io/posts/review-of-indexing-billions-of-text-vectors/</guid>
      <description>A New Search Engine  Original article A New Search Engine  Building a search engine from scratch  Original article Building a search engine from scratch  Indexing Billions of Text Vectors  Original article Indexing Billions of Text Vectors  A frequently occurring IR problem Finding similar pieces of text.
A query in this context can either be a user- generated one, (i.e. the piece of text that a user enters into a search engine), or a synthetic one generated by us.</description>
    </item>
    
    <item>
      <title>Controlled Text Generation</title>
      <link>https://mullikine.github.io/posts/controlled-text-generation/</link>
      <pubDate>Fri, 06 Dec 2019 00:00:00 +0800</pubDate>
      
      <guid>https://mullikine.github.io/posts/controlled-text-generation/</guid>
      <description>Original article https://eng.uber.com/pplm/ https://github.com/uber-research/PPLM https://github.com/huggingface/transformers/blob/master/examples/pplm/README.md Uber AI Plug and Play Language Model  Controlling Text Generation with Plug and Play Language Model (PPLM) PPLM builds on top of other large transformer- based generative models (like GPT-2), where it enables finer-grained control of attributes of the generated language (e.g. gradually switching topic 🐱 or sentiment 😃).
This controlled LG method consists of plugging in simple bag-of-words or one-layer classifiers as attribute controllers, and making updates in the activation space, without changing any model parameters.</description>
    </item>
    
    <item>
      <title>Creating Infinitely Generated Text Adventures with DL LMs</title>
      <link>https://mullikine.github.io/posts/infinitely-generated-text-adventures/</link>
      <pubDate>Fri, 06 Dec 2019 00:00:00 +0800</pubDate>
      
      <guid>https://mullikine.github.io/posts/infinitely-generated-text-adventures/</guid>
      <description>Original article AI Dungeon 2: Creating Infinitely Generated Text Adventures with Deep Learning Language Models - Perception, Control, Cognition  For each action you type the model is fed the context sentence as well as the past N action- result pairs in its memory to generate the result.
We found N=8 to be a good amount of memory to feed the model.
Other times the model has difficulty keeping track of who is who, especially in dialogue.</description>
    </item>
    
    <item>
      <title>Chatbot with Rasa</title>
      <link>https://mullikine.github.io/posts/chatbot-with-rasa/</link>
      <pubDate>Wed, 04 Dec 2019 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/chatbot-with-rasa/</guid>
      <description>Original article A Chatbot from Future: Building an end-to-end Conversational Assistant with Rasa  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24  intent vs goal Some chatbot tools use the word intent to refer to the user goal. This is confusing because only some messages tell you what a user’s goal is. dialogue elements - highest level: user goals - middle level: dialogue elements - lowest level: intents, entities, actions, slots, and templates.</description>
    </item>
    
    <item>
      <title>Named Entity Recognition</title>
      <link>https://mullikine.github.io/posts/named-entity-recognition/</link>
      <pubDate>Wed, 04 Dec 2019 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/named-entity-recognition/</guid>
      <description>Original article Named Entity Recognition with NLTK and SpaCy - Towards Data Science Code https://github.com/susanli2016/NLP-with-Python/blob/master/NER%5FNLTK%5FSpacy.ipynb My fork https://github.com/mullikine/NLP-with-Python/blob/master/NER%5FNLTK%5FSpacy.py Related articles Part of Speech Labels // Bodacious Blog  1  sp $MYGIT/susanli2016/NLP-with-Python/NER_NLTK_Spacy.py   Missing libraries 1 2 3 4 5  Resource averaged_perceptron_tagger not found. Please use the NLTK Downloader to obtain the resource: &amp;gt;&amp;gt;&amp;gt; import nltk &amp;gt;&amp;gt;&amp;gt; nltk.download(&amp;#39;averaged_perceptron_tagger&amp;#39;)   1 2 3  import nltk nltk.download(&amp;#39;averaged_perceptron_tagger&amp;#39;) # and then type &amp;#39;d&amp;#39; for download and install &amp;#39;punkt&amp;#39;   I had to do it again for this 1 2  import nltk nltk.</description>
    </item>
    
    <item>
      <title>Part of Speech Labels</title>
      <link>https://mullikine.github.io/posts/part-of-speech-labels/</link>
      <pubDate>Wed, 04 Dec 2019 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/part-of-speech-labels/</guid>
      <description>1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25  conjunction CONJ CNJ A part of speech that connects words, phrases, or clauses that are called the conjuncts of the conjunctions. subordinating conjunction Preposition IN A conjunction that introduces a subordinating clause, e.g. although, because. Coordinating conjunction CC A conjunction placed between words, phrases, clauses, or sentences of equal rank, e.</description>
    </item>
    
    <item>
      <title>Searching for gists</title>
      <link>https://mullikine.github.io/posts/searching-for-gists/</link>
      <pubDate>Wed, 04 Dec 2019 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/searching-for-gists/</guid>
      <description>Create a gist-search script 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  #!/bin/bash export TTY ( hs &amp;#34;$(basename &amp;#34;$0&amp;#34;)&amp;#34; &amp;#34;$@&amp;#34; &amp;#34;#&amp;#34; &amp;#34;&amp;lt;==&amp;#34; &amp;#34;$(ps -o comm= $PPID)&amp;#34; 0&amp;lt;/dev/null ) &amp;amp;&amp;gt;/dev/null is_tty() { # If stout is a tty [[ -t 1 ]] } gr gist &amp;#34;$@&amp;#34; | grep //gist.github.com/ | urldecode | scrape &amp;#34;.*gist.github.com/[^?/]+&amp;#34; | sort | uniq | scrape &amp;#34;[^/]+$&amp;#34; | { if is_tty; then fzf | xa gist-list else cat fi }   Demonstration</description>
    </item>
    
    <item>
      <title>spaCy</title>
      <link>https://mullikine.github.io/posts/spacy/</link>
      <pubDate>Wed, 04 Dec 2019 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/spacy/</guid>
      <description>Original gist https://gist.github.com/aparrish/697b7f56ac28f4e59af77a66ac573b8f  After loading into spacy Right off the bat, the spaCy library gives us access to a number of interesting units of text:
   code description     doc.sents sentences   doc words   doc.ents named entitites   doc.noun_chunks nouns in the text plus surrounding matter like adjectives and articles    1 2 3 4  sentences = list(doc.</description>
    </item>
    
    <item>
      <title>Reading 25.11.19</title>
      <link>https://mullikine.github.io/posts/reading-25.11.19/</link>
      <pubDate>Mon, 25 Nov 2019 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/reading-25.11.19/</guid>
      <description>Language Models as Knowledge Bases?  Original article https://arxiv.org/abs/1909.01066 Code https://github.com/facebookresearch/LAMA  1 2 3 4 5 6 7 8 9 10 11  Cloze statements An excellent method to encourage speech production in children. A cloze statement involves saying a familiar phrase and leaving out a word, then waiting for your child to fill in the blank. Familiar nursery rhymes, songs, and poems are perfect for this activity.   Recent progress in pretraining LMs on large textual corpora led to a surge of improvements for downstream NLP tasks.</description>
    </item>
    
    <item>
      <title>Notes on BERT</title>
      <link>https://mullikine.github.io/posts/google-bert/</link>
      <pubDate>Sat, 16 Nov 2019 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/google-bert/</guid>
      <description>https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html
Pre-trained representations can either be context-free or contextual, and contextual representations can further be unidirectional or bidirectional.
1 2 3 4 5  node [style=filled,fillcolor=lightgrey,shape=box]; &amp;#34;Pre-trained representation&amp;#34; -&amp;gt; &amp;#34;context-free&amp;#34; &amp;#34;Pre-trained representation&amp;#34; -&amp;gt; contextual contextual -&amp;gt; unidirectional contextual -&amp;gt; bidirectional        LM supervision contextual Pre-trained Training data bi-directional     BERT unsupervised ✓ ✓ Plain text ✓   word2vec semi/self-supervised ✗      GloVe semi-supervised ✗       Supervision is a bit grey  Supervised If you consider that the network has to learn from it’s errors through back prop.</description>
    </item>
    
    <item>
      <title>TensorFlow BERT</title>
      <link>https://mullikine.github.io/posts/tensorflow-bert-keras/</link>
      <pubDate>Thu, 31 Oct 2019 00:00:00 +0800</pubDate>
      
      <guid>https://mullikine.github.io/posts/tensorflow-bert-keras/</guid>
      <description>Original article Hugging Face: State-of-the-Art Natural Language Processing in ten lines of TensorFlow 2.0  A list of transformer architectures    architecture     BERT   RoBERTa   GPT-2   DistilBERT    pip&#39;s transformers library Builds on 3 main classes:  configuration class tokenizer class model class  configuration class Hosts relevant information concerning the model we will be using, such as:</description>
    </item>
    
    <item>
      <title>Notes on &#34;Generating Beatles’ Lyrics with Machine Learning&#34;</title>
      <link>https://mullikine.github.io/posts/notes-on-generating-beatles-lyrics-with-machine-learning/</link>
      <pubDate>Thu, 24 Oct 2019 00:00:00 +0800</pubDate>
      
      <guid>https://mullikine.github.io/posts/notes-on-generating-beatles-lyrics-with-machine-learning/</guid>
      <description>Original article Generating Beatles Lyrics with Machine Learning - Towards Data Science  Apparatus     URL      code https://github.com/EugenHotaj/beatles    dataset http://toti.eu.com/beatles/index.asp github.com/EugenHotaj/beatles/blob/master/scraper.py   embeddings     algorithm      Hypothesis Aim Questions  How might might generate sentences from a language model?  Method Factoids Unigram model Ignores any conditioning and simply chooses the next word randomly from the training data.</description>
    </item>
    
    <item>
      <title>Notes on &#34;Identifying the right meaning of the words using BERT&#34;</title>
      <link>https://mullikine.github.io/posts/notes-on-identifying-the-right-meaning-of-the-words-using-bert/</link>
      <pubDate>Thu, 24 Oct 2019 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/notes-on-identifying-the-right-meaning-of-the-words-using-bert/</guid>
      <description>Original article Identifying the right meaning of the words using BERT  Glossary 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  Uncased [model] The text has been lowercased before WordPiece tokenization, e.g., John Smith becomes john smith. The Uncased model also strips out any accent markers. Cased [model] The true case and accent markers are preserved.</description>
    </item>
    
    <item>
      <title>(WIP) Notes on &#34;Natural Language Processing by Jacob Eisenstein&#34;</title>
      <link>https://mullikine.github.io/posts/notes-on-eisenstein-nlp/</link>
      <pubDate>Wed, 23 Oct 2019 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/notes-on-eisenstein-nlp/</guid>
      <description>Reading vs +/&amp;#34;Kneser-Ney&amp;#34; $NOTES/ws/nlp-natural-language-processing/reading/eisenstein-nlp-notes.txt Kneser-Ney smoothing Based on absolute discounting, but it redistributes the resulting probability mass in a different way from Katz backoff.
Empirical evidence points to Kneser-Ney smoothing as the state-of-art for n-gram LMing.</description>
    </item>
    
    <item>
      <title>The Illustrated Transformer</title>
      <link>https://mullikine.github.io/posts/review-of-the-illustrated-transformer/</link>
      <pubDate>Sat, 19 Oct 2019 00:00:00 +0800</pubDate>
      
      <guid>https://mullikine.github.io/posts/review-of-the-illustrated-transformer/</guid>
      <description>Original article https://jalammar.github.io/illustrated-transformer/ Source https://github.com/tensorflow/tensor2tensor  Prereading https://jalammar.github.io/illustrated-word2vec/
Helpful glossary 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51  Multilayer perceptron Feed-Forward Neural Network FFNN Basically, these are multi-level logistic regression classifiers.</description>
    </item>
    
    <item>
      <title>gwern.net GPT-2</title>
      <link>https://mullikine.github.io/posts/review-of-gewn-gpt2/</link>
      <pubDate>Thu, 17 Oct 2019 00:00:00 +0800</pubDate>
      
      <guid>https://mullikine.github.io/posts/review-of-gewn-gpt2/</guid>
      <description>original article https://www.gwern.net/GPT-2  Read using eww The website when viewed with a GUI browser, although impressive, is distracting.
The website was built with hackyll, a static site generator employing haskell.
1  eww &amp;#34;https://www.gwern.net/GPT-2&amp;#34;   Ease of reading Straight-forward once you understand ML jargon.</description>
    </item>
    
    <item>
      <title>Rewrite of gwern.net GPT-2 Neural Network Poetry</title>
      <link>https://mullikine.github.io/posts/rewrite-of-gwern-gpt2-poetry/</link>
      <pubDate>Thu, 17 Oct 2019 00:00:00 +0800</pubDate>
      
      <guid>https://mullikine.github.io/posts/rewrite-of-gwern-gpt2-poetry/</guid>
      <description>original article https://www.gwern.net/GPT-2  Read using eww The website when viewed with a GUI browser, although impressive, is distracting.
The website was built with hackyll, a static site generator employing haskell.
1  eww &amp;#34;https://www.gwern.net/GPT-2&amp;#34;   Ease of reading Straight-forward once you understand ML jargon.</description>
    </item>
    
    <item>
      <title>Keyword extraction to analyse articles</title>
      <link>https://mullikine.github.io/posts/keyword-extraction/</link>
      <pubDate>Thu, 17 Oct 2019 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/keyword-extraction/</guid>
      <description>sparsity [#text mining] Huge matrices are created based on word frequencies with many cells having zero values. This problem is called sparsity and is minimized using various techniques. Articles keyword extraction: nltk, sklearn Automated Keyword Extraction from Articles using NLP
kag datasets download benhamner/nips-papers textrank: numpy, spacy towardsdatascience.com/textrank-for-keyword-extraction-by-python-c0bae21bcec0
ngram, modified skip-gram, spacy Keywords Extraction with Ngram and Modified Skip-gram based on spaCy
TODO Turn the math4IQB lectures into keywords readsubs &amp;#34;https://www.</description>
    </item>
    
    <item>
      <title>(WIP) Review of NLP tools</title>
      <link>https://mullikine.github.io/posts/review-of-nlp-tools/</link>
      <pubDate>Wed, 16 Oct 2019 00:00:00 +0800</pubDate>
      
      <guid>https://mullikine.github.io/posts/review-of-nlp-tools/</guid>
      <description>lm-explorer Interactive explorer for language models (currently only OpenAI GPT-2).</description>
    </item>
    
    <item>
      <title>writeup.ai</title>
      <link>https://mullikine.github.io/posts/review-of-writeup-ai/</link>
      <pubDate>Wed, 16 Oct 2019 00:00:00 +0800</pubDate>
      
      <guid>https://mullikine.github.io/posts/review-of-writeup-ai/</guid>
      <description>Original article https://senrigan.io/blog/how-writeupai-runs-behind-the-scenes/#h%5F6068056784021570782144062  Glossary 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26  cross entropy loss (between two probability distributions) (and over the same underlying set of events) Higher loss is bad. Measures the performance of a classification model whose output is a probability value between 0 and 1. Measures the average number of bits needed to identify an event from the set.</description>
    </item>
    
    <item>
      <title>Blogs and Vlogs</title>
      <link>https://mullikine.github.io/posts/blogs-and-vlogs/</link>
      <pubDate>Mon, 14 Oct 2019 00:00:00 +0800</pubDate>
      
      <guid>https://mullikine.github.io/posts/blogs-and-vlogs/</guid>
      <description>Blogs and vlogs These people have been influential to me.
2019    source type url Topics     Math4IQB vlog Math4IQB - YouTube Information Theory   Gwern Bransen blog GPT-2 Neural Network Poetry Gwern.net Haskell, NLP, R   Fabian Dablander blog Fabian Dablander - PhD Student Methods Statistics Math, statistics, latex   John Kitchen blog The Kitchin Research Group Emacs   John D.</description>
    </item>
    
    <item>
      <title>Coherence in Natural Language (2006)</title>
      <link>https://mullikine.github.io/posts/review-coherence-in-natural-language/</link>
      <pubDate>Mon, 14 Oct 2019 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/review-coherence-in-natural-language/</guid>
      <description>Terminological mess The term cohesion does not figure in the index of the book.
Coherence vs cohesion &amp;ldquo;cohesion&amp;rdquo; for microstructures and &amp;ldquo;coherence&amp;rdquo; for macrostructures.
cohesion when talking about text and coherence when talking about discourse i.e. &amp;ldquo;semantics vs. pragmatics&amp;rdquo;
Issues covered in the book in the realm of coherence structures:
 cognitive science natural language engineering information extraction  Definitions Coherence structures ommitted: a chrestomathy of coherent vs incoherent text Reviews Coherence in natural language.</description>
    </item>
    
    <item>
      <title>(WIP) Applying Multinomial Naive Bayes to NLP Problems: A Practical Explanation</title>
      <link>https://mullikine.github.io/posts/review-multinomial-naive-bayes-nlp/</link>
      <pubDate>Fri, 11 Oct 2019 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/review-multinomial-naive-bayes-nlp/</guid>
      <description>Applying Multinomial Naive Bayes to NLP Problems: A Practical Explanation</description>
    </item>
    
    <item>
      <title>(WIP) Extending WordNut for generating blog titles</title>
      <link>https://mullikine.github.io/posts/extending-wordnut-for-generating-blog-titles/</link>
      <pubDate>Mon, 07 Oct 2019 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/extending-wordnut-for-generating-blog-titles/</guid>
      <description>Obtain an org-mode parser https://orgmode.org/worg/org-tools/index.html
Parse wordnut output to scrape synonyms from the buffer Given 2 words, create a list of synonyms for each Look for one word from each list with the same starting letter </description>
    </item>
    
    <item>
      <title>(WIP) Review of Language, trees, and geometry in neural networks</title>
      <link>https://mullikine.github.io/posts/language-trees-and-geometry-in-neural-networks/</link>
      <pubDate>Mon, 07 Oct 2019 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/language-trees-and-geometry-in-neural-networks/</guid>
      <description>1906.02715 Visualizing and Measuring the Geometry of BERT
https://pair-code.github.io/interpretability/bert-tree/
https://pair-code.github.io/interpretability/context-atlas/blogpost/
Existing representation: word embeddings Language is made of discrete structures, yet neural networks operate on continuous data: vectors in high-dimensional space.
A successful language-processing network must translate this symbolic information into some kind of geometric representation—but in what form?
Word embeddings provide two well-known examples: distance encodes semantic similarity, while certain directions correspond to polarities (e.g. male vs. female).
New representation A recent, fascinating discovery points to an entirely new type of representation.</description>
    </item>
    
    <item>
      <title>PClean: A probabilistic scripting DSL</title>
      <link>https://mullikine.github.io/posts/pclean-gen-gpl/</link>
      <pubDate>Mon, 30 Sep 2019 00:00:00 +0800</pubDate>
      
      <guid>https://mullikine.github.io/posts/pclean-gen-gpl/</guid>
      <description>Links  MIT Probabilistic Computing Project Introduction | Gen GitHub - probcomp/Gen: A general-purpose probabilistic programming system with programmable inference  Other talks by MIT Probabilistic Computing Project: Videos, Talks, and Podcasts - MIT Probabilistic Computing Project
Tools Gen  a package for the Julia programming language. consists of multiple modeling languages that are implemented as DSLs in Julia and a Julia library for inference programming.  PClean  A probabilistic scripting DSL in the Gen package.</description>
    </item>
    
    <item>
      <title>Xenolinguistics</title>
      <link>https://mullikine.github.io/posts/xenolinguistics/</link>
      <pubDate>Mon, 16 Feb 2009 00:00:00 +0900</pubDate>
      
      <guid>https://mullikine.github.io/posts/xenolinguistics/</guid>
      <description>Real research The study of what form alien languages might take The question of what form alien languages might take and the possibility for humans to recognize and translate them has been part of the linguistics and language studies courses, e.g., at the Bowling Green State University (2001).
A lesson from star trek Xenolinguistics - Star Trek TNG - The Ensigns of Command - YouTube
Zismareth. What did I just say?</description>
    </item>
    
  </channel>
</rss>