<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>NLP on Bodacious Blog</title>
    <link>https://mullikine.github.io/tags/nlp/</link>
    <description>Recent content in NLP on Bodacious Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 17 Dec 2019 00:00:00 +1300</lastBuildDate>
    
	<atom:link href="https://mullikine.github.io/tags/nlp/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Review of &#39;Language Engineering; Harnessing the Power of Language (2004)&#39;</title>
      <link>https://mullikine.github.io/posts/review-of-language-engineering-harnessing-the-power-of-language/</link>
      <pubDate>Tue, 17 Dec 2019 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/review-of-language-engineering-harnessing-the-power-of-language/</guid>
      <description>This article is from 2004 or earlier.
It&amp;rsquo;s still an interesting insight into perspectives on Language Engineering from the early 2000s.
 Original article Language Engineering; Harnessing the Power of Language  The use of language is currently restricted.
Even between humans, understanding is usually limited to those groups who share a common language. ‚Ä†
 ‚Ä† &amp;hellip;let alone the language barrier with computers.  Language can be seen as much a barrier to communication as an aid.</description>
    </item>
    
    <item>
      <title>Review of &#39;Indexing Billions of Text Vectors&#39;</title>
      <link>https://mullikine.github.io/posts/review-of-indexing-billions-of-text-vectors/</link>
      <pubDate>Tue, 10 Dec 2019 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/review-of-indexing-billions-of-text-vectors/</guid>
      <description>A New Search Engine  Original article A New Search Engine  Building a search engine from scratch  Original article Building a search engine from scratch  Indexing Billions of Text Vectors  Original article Indexing Billions of Text Vectors  A frequently occurring IR problem Finding similar pieces of text.
A query in this context can either be a user- generated one, (i.e. the piece of text that a user enters into a search engine), or a synthetic one generated by us.</description>
    </item>
    
    <item>
      <title>Controlled Text Generation</title>
      <link>https://mullikine.github.io/posts/controlled-text-generation/</link>
      <pubDate>Fri, 06 Dec 2019 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/controlled-text-generation/</guid>
      <description>Original article https://eng.uber.com/pplm/ https://github.com/uber-research/PPLM https://github.com/huggingface/transformers/blob/master/examples/pplm/README.md Uber AI Plug and Play Language Model  Controlling Text Generation with Plug and Play Language Model (PPLM) PPLM builds on top of other large transformer- based generative models (like GPT-2), where it enables finer-grained control of attributes of the generated language (e.g. gradually switching topic üê± or sentiment üòÉ).
This controlled LG method consists of plugging in simple bag-of-words or one-layer classifiers as attribute controllers, and making updates in the activation space, without changing any model parameters.</description>
    </item>
    
    <item>
      <title>Creating Infinitely Generated Text Adventures with DL LMs</title>
      <link>https://mullikine.github.io/posts/infinitely-generated-text-adventures/</link>
      <pubDate>Fri, 06 Dec 2019 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/infinitely-generated-text-adventures/</guid>
      <description>Original article AI Dungeon 2: Creating Infinitely Generated Text Adventures with Deep Learning Language Models - Perception, Control, Cognition  For each action you type the model is fed the context sentence as well as the past N action- result pairs in its memory to generate the result.
We found N=8 to be a good amount of memory to feed the model.
Other times the model has difficulty keeping track of who is who, especially in dialogue.</description>
    </item>
    
    <item>
      <title>Chatbot with Rasa</title>
      <link>https://mullikine.github.io/posts/chatbot-with-rasa/</link>
      <pubDate>Wed, 04 Dec 2019 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/chatbot-with-rasa/</guid>
      <description>Original article A Chatbot from Future: Building an end-to-end Conversational Assistant with Rasa  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24  intent vs goal Some chatbot tools use the word intent to refer to the user goal. This is confusing because only some messages tell you what a user‚Äôs goal is.</description>
    </item>
    
    <item>
      <title>Named Entity Recognition</title>
      <link>https://mullikine.github.io/posts/named-entity-recognition/</link>
      <pubDate>Wed, 04 Dec 2019 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/named-entity-recognition/</guid>
      <description>Original article Named Entity Recognition with NLTK and SpaCy - Towards Data Science Code https://github.com/susanli2016/NLP-with-Python/blob/master/NER%5FNLTK%5FSpacy.ipynb My fork https://github.com/mullikine/NLP-with-Python/blob/master/NER%5FNLTK%5FSpacy.py Related articles Part of Speech Labels // Bodacious Blog  1  sp $MYGIT/susanli2016/NLP-with-Python/NER_NLTK_Spacy.py   Missing libraries 1 2 3 4 5  Resource averaged_perceptron_tagger not found. Please use the NLTK Downloader to obtain the resource: &amp;gt;&amp;gt;&amp;gt; import nltk &amp;gt;&amp;gt;&amp;gt; nltk.download(&amp;#39;averaged_perceptron_tagger&amp;#39;)   1 2 3  import nltk nltk.</description>
    </item>
    
    <item>
      <title>Part of Speech Labels</title>
      <link>https://mullikine.github.io/posts/part-of-speech-labels/</link>
      <pubDate>Wed, 04 Dec 2019 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/part-of-speech-labels/</guid>
      <description>1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25  conjunction CONJ CNJ A part of speech that connects words, phrases, or clauses that are called the conjuncts of the conjunctions. subordinating conjunction Preposition IN A conjunction that introduces a subordinating clause, e.g. although, because. Coordinating conjunction CC A conjunction placed between words, phrases, clauses, or sentences of equal rank, e.</description>
    </item>
    
    <item>
      <title>Searching for gists</title>
      <link>https://mullikine.github.io/posts/searching-for-gists/</link>
      <pubDate>Wed, 04 Dec 2019 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/searching-for-gists/</guid>
      <description>Create a gist-search script 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  #!/bin/bash export TTY ( hs &amp;#34;$(basename &amp;#34;$0&amp;#34;)&amp;#34; &amp;#34;$@&amp;#34; &amp;#34;#&amp;#34; &amp;#34;&amp;lt;==&amp;#34; &amp;#34;$(ps -o comm= $PPID)&amp;#34; 0&amp;lt;/dev/null ) &amp;amp;&amp;gt;/dev/null is_tty() { # If stout is a tty [[ -t 1 ]] } gr gist &amp;#34;$@&amp;#34; | grep //gist.github.com/ | urldecode | scrape &amp;#34;.*gist.github.com/[^?/]+&amp;#34; | sort | uniq | scrape &amp;#34;[^/]+$&amp;#34; | { if is_tty; then fzf | xa gist-list else cat fi }   Demonstration</description>
    </item>
    
    <item>
      <title>spaCy</title>
      <link>https://mullikine.github.io/posts/spacy/</link>
      <pubDate>Wed, 04 Dec 2019 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/spacy/</guid>
      <description>Original gist https://gist.github.com/aparrish/697b7f56ac28f4e59af77a66ac573b8f  After loading into spacy Right off the bat, the spaCy library gives us access to a number of interesting units of text:
   code description     doc.sents sentences   doc words   doc.ents named entitites   doc.noun_chunks nouns in the text plus surrounding matter like adjectives and articles    1 2 3 4  sentences = list(doc.</description>
    </item>
    
    <item>
      <title>Reading 25.11.19</title>
      <link>https://mullikine.github.io/posts/reading-25.11.19/</link>
      <pubDate>Mon, 25 Nov 2019 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/reading-25.11.19/</guid>
      <description>Language Models as Knowledge Bases?  Original article https://arxiv.org/abs/1909.01066 Code https://github.com/facebookresearch/LAMA  1 2 3 4 5 6 7 8 9 10 11  Cloze statements An excellent method to encourage speech production in children. A cloze statement involves saying a familiar phrase and leaving out a word, then waiting for your child to fill in the blank. Familiar nursery rhymes, songs, and poems are perfect for this activity.   Recent progress in pretraining LMs on large textual corpora led to a surge of improvements for downstream NLP tasks.</description>
    </item>
    
    <item>
      <title>Notes on BERT</title>
      <link>https://mullikine.github.io/posts/google-bert/</link>
      <pubDate>Sat, 16 Nov 2019 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/google-bert/</guid>
      <description>https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html
Pre-trained representations can either be context-free or contextual, and contextual representations can further be unidirectional or bidirectional.
1 2 3 4 5  node [style=filled,fillcolor=lightgrey,shape=box]; &amp;#34;Pre-trained representation&amp;#34; -&amp;gt; &amp;#34;context-free&amp;#34; &amp;#34;Pre-trained representation&amp;#34; -&amp;gt; contextual contextual -&amp;gt; unidirectional contextual -&amp;gt; bidirectional        LM supervision contextual Pre-trained Training data bi-directional     BERT unsupervised ‚úì ‚úì Plain text ‚úì   word2vec semi/self-supervised ‚úó      GloVe semi-supervised ‚úó       Supervision is a bit grey  Supervised If you consider that the network has to learn from it‚Äôs errors through back prop.</description>
    </item>
    
    <item>
      <title>TensorFlow BERT</title>
      <link>https://mullikine.github.io/posts/tensorflow-bert-keras/</link>
      <pubDate>Thu, 31 Oct 2019 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/tensorflow-bert-keras/</guid>
      <description>Original article Hugging Face: State-of-the-Art Natural Language Processing in ten lines of TensorFlow 2.0  A list of transformer architectures    architecture     BERT   RoBERTa   GPT-2   DistilBERT    pip&amp;rsquo;s transformers library Builds on 3 main classes:  configuration class tokenizer class model class  configuration class Hosts relevant information concerning the model we will be using, such as:</description>
    </item>
    
    <item>
      <title>Notes on &#34;Generating Beatles‚Äô Lyrics with Machine Learning&#34;</title>
      <link>https://mullikine.github.io/posts/notes-on-generating-beatles-lyrics-with-machine-learning/</link>
      <pubDate>Thu, 24 Oct 2019 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/notes-on-generating-beatles-lyrics-with-machine-learning/</guid>
      <description>Original article Generating Beatles Lyrics with Machine Learning - Towards Data Science  Apparatus     URL      code https://github.com/EugenHotaj/beatles    dataset http://toti.eu.com/beatles/index.asp github.com/EugenHotaj/beatles/blob/master/scraper.py   embeddings     algorithm      Hypothesis Aim Questions  How might might generate sentences from a language model?  Method Factoids Unigram model Ignores any conditioning and simply chooses the next word randomly from the training data.</description>
    </item>
    
    <item>
      <title>Notes on &#34;Identifying the right meaning of the words using BERT&#34;</title>
      <link>https://mullikine.github.io/posts/notes-on-identifying-the-right-meaning-of-the-words-using-bert/</link>
      <pubDate>Thu, 24 Oct 2019 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/notes-on-identifying-the-right-meaning-of-the-words-using-bert/</guid>
      <description>Original article Identifying the right meaning of the words using BERT  Glossary 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  Uncased [model] The text has been lowercased before WordPiece tokenization, e.g., John Smith becomes john smith. The Uncased model also strips out any accent markers. Cased [model] The true case and accent markers are preserved.</description>
    </item>
    
    <item>
      <title>(WIP) Notes on &#34;Natural Language Processing by Jacob Eisenstein&#34;</title>
      <link>https://mullikine.github.io/posts/notes-on-eisenstein-nlp/</link>
      <pubDate>Wed, 23 Oct 2019 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/notes-on-eisenstein-nlp/</guid>
      <description>Reading vs +/&amp;#34;Kneser-Ney&amp;#34; $NOTES/ws/nlp-natural-language-processing/reading/eisenstein-nlp-notes.txt Kneser-Ney smoothing Based on absolute discounting, but it redistributes the resulting probability mass in a different way from Katz backoff.
Empirical evidence points to Kneser-Ney smoothing as the state-of-art for n-gram LMing.</description>
    </item>
    
    <item>
      <title>The Illustrated Transformer</title>
      <link>https://mullikine.github.io/posts/review-of-the-illustrated-transformer/</link>
      <pubDate>Sat, 19 Oct 2019 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/review-of-the-illustrated-transformer/</guid>
      <description>Original article https://jalammar.github.io/illustrated-transformer/ Source https://github.com/tensorflow/tensor2tensor  Prereading https://jalammar.github.io/illustrated-word2vec/
Helpful glossary 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51  Multilayer perceptron Feed-Forward Neural Network FFNN Basically, these are multi-level logistic regression classifiers.</description>
    </item>
    
    <item>
      <title>gwern.net GPT-2</title>
      <link>https://mullikine.github.io/posts/review-of-gewn-gpt2/</link>
      <pubDate>Thu, 17 Oct 2019 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/review-of-gewn-gpt2/</guid>
      <description>original article https://www.gwern.net/GPT-2  Read using eww The website when viewed with a GUI browser, although impressive, is distracting.
The website was built with hackyll, a static site generator employing haskell.
1  eww &amp;#34;https://www.gwern.net/GPT-2&amp;#34;   Ease of reading Straight-forward once you understand ML jargon.</description>
    </item>
    
    <item>
      <title>Keyword extraction to analyse articles</title>
      <link>https://mullikine.github.io/posts/keyword-extraction/</link>
      <pubDate>Thu, 17 Oct 2019 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/keyword-extraction/</guid>
      <description>sparsity [#text mining] Huge matrices are created based on word frequencies with many cells having zero values. This problem is called sparsity and is minimized using various techniques. Articles keyword extraction: nltk, sklearn Automated Keyword Extraction from Articles using NLP
kag datasets download benhamner/nips-papers textrank: numpy, spacy towardsdatascience.com/textrank-for-keyword-extraction-by-python-c0bae21bcec0
ngram, modified skip-gram, spacy Keywords Extraction with Ngram and Modified Skip-gram based on spaCy
TODO Turn the math4IQB lectures into keywords readsubs &amp;#34;https://www.</description>
    </item>
    
    <item>
      <title>Rewrite of gwern.net GPT-2 Neural Network Poetry</title>
      <link>https://mullikine.github.io/posts/rewrite-of-gwern-gpt2-poetry/</link>
      <pubDate>Thu, 17 Oct 2019 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/rewrite-of-gwern-gpt2-poetry/</guid>
      <description>original article https://www.gwern.net/GPT-2  Read using eww The website when viewed with a GUI browser, although impressive, is distracting.
The website was built with hackyll, a static site generator employing haskell.
1  eww &amp;#34;https://www.gwern.net/GPT-2&amp;#34;   Ease of reading Straight-forward once you understand ML jargon.</description>
    </item>
    
    <item>
      <title>(WIP) Review of NLP tools</title>
      <link>https://mullikine.github.io/posts/review-of-nlp-tools/</link>
      <pubDate>Wed, 16 Oct 2019 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/review-of-nlp-tools/</guid>
      <description>lm-explorer Interactive explorer for language models (currently only OpenAI GPT-2).</description>
    </item>
    
    <item>
      <title>writeup.ai</title>
      <link>https://mullikine.github.io/posts/review-of-writeup-ai/</link>
      <pubDate>Wed, 16 Oct 2019 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/review-of-writeup-ai/</guid>
      <description>Original article https://senrigan.io/blog/how-writeupai-runs-behind-the-scenes/#h%5F6068056784021570782144062  Glossary 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26  cross entropy loss (between two probability distributions) (and over the same underlying set of events) Higher loss is bad. Measures the performance of a classification model whose output is a probability value between 0 and 1. Measures the average number of bits needed to identify an event from the set.</description>
    </item>
    
    <item>
      <title>Blogs and Vlogs</title>
      <link>https://mullikine.github.io/posts/blogs-and-vlogs/</link>
      <pubDate>Mon, 14 Oct 2019 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/blogs-and-vlogs/</guid>
      <description>These are some of my favourite learning resources.
   source type url Topics     Math4IQB vlog https://www.youtube.com/user/Math4IQB/videos Information Theory   Gwern Bransen blog https://www.gwern.net/GPT-2 Haskell, NLP, R   Fabian Dablander blog https://fabiandablander.com Math, statistics, latex   John D. Cook blog https://www.johndcook.com/blog/ Math, statistics   Sacha Chua blog https://sachachua.com/blog/0000/00/05/ New emacs packages   Cameron Kerr blog http://humbledown.org https://distracted-it.blogspot.com Telecommunications   Brendan Gregg blog http://www.</description>
    </item>
    
    <item>
      <title>Coherence in Natural Language (2006)</title>
      <link>https://mullikine.github.io/posts/review-coherence-in-natural-language/</link>
      <pubDate>Mon, 14 Oct 2019 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/review-coherence-in-natural-language/</guid>
      <description>Terminological mess The term cohesion does not figure in the index of the book.
Coherence vs cohesion &amp;ldquo;cohesion&amp;rdquo; for microstructures and &amp;ldquo;coherence&amp;rdquo; for macrostructures.
cohesion when talking about text and coherence when talking about discourse i.e. &amp;ldquo;semantics vs. pragmatics&amp;rdquo;
Issues covered in the book in the realm of coherence structures:
 cognitive science natural language engineering information extraction  Definitions Coherence structures ommitted: a chrestomathy of coherent vs incoherent text Reviews Coherence in natural language.</description>
    </item>
    
    <item>
      <title>(WIP) Applying Multinomial Naive Bayes to NLP Problems: A Practical Explanation</title>
      <link>https://mullikine.github.io/posts/review-multinomial-naive-bayes-nlp/</link>
      <pubDate>Fri, 11 Oct 2019 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/review-multinomial-naive-bayes-nlp/</guid>
      <description>Applying Multinomial Naive Bayes to NLP Problems: A Practical Explanation</description>
    </item>
    
    <item>
      <title>(WIP) Extending WordNut for generating blog titles</title>
      <link>https://mullikine.github.io/posts/extending-wordnut-for-generating-blog-titles/</link>
      <pubDate>Mon, 07 Oct 2019 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/extending-wordnut-for-generating-blog-titles/</guid>
      <description> Obtain an org-mode parser https://orgmode.org/worg/org-tools/index.html
Parse wordnut output to scrape synonyms from the buffer Given 2 words, create a list of synonyms for each Look for one word from each list with the same starting letter </description>
    </item>
    
    <item>
      <title>(WIP) Review of Language, trees, and geometry in neural networks</title>
      <link>https://mullikine.github.io/posts/language-trees-and-geometry-in-neural-networks/</link>
      <pubDate>Mon, 07 Oct 2019 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/language-trees-and-geometry-in-neural-networks/</guid>
      <description>1906.02715 Visualizing and Measuring the Geometry of BERT
https://pair-code.github.io/interpretability/bert-tree/
https://pair-code.github.io/interpretability/context-atlas/blogpost/
Existing representation: word embeddings Language is made of discrete structures, yet neural networks operate on continuous data: vectors in high-dimensional space.
A successful language-processing network must translate this symbolic information into some kind of geometric representation‚Äîbut in what form?
Word embeddings provide two well-known examples: distance encodes semantic similarity, while certain directions correspond to polarities (e.g. male vs. female).</description>
    </item>
    
    <item>
      <title>PClean: A probabilistic scripting DSL</title>
      <link>https://mullikine.github.io/posts/pclean-gen-gpl/</link>
      <pubDate>Mon, 30 Sep 2019 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/pclean-gen-gpl/</guid>
      <description>Links  MIT Probabilistic Computing Project Introduction | Gen GitHub - probcomp/Gen: A general-purpose probabilistic programming system with programmable inference  Other talks by MIT Probabilistic Computing Project: Videos, Talks, and Podcasts - MIT Probabilistic Computing Project
Tools Gen  a package for the Julia programming language. consists of multiple modeling languages that are implemented as DSLs in Julia and a Julia library for inference programming.  PClean  A probabilistic scripting DSL in the Gen package.</description>
    </item>
    
  </channel>
</rss>