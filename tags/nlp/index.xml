<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>NLP on Bodacious Blog</title>
    <link>https://mullikine.github.io/tags/nlp/</link>
    <description>Recent content in NLP on Bodacious Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 04 Dec 2019 00:00:00 +1300</lastBuildDate>
    
	<atom:link href="https://mullikine.github.io/tags/nlp/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Named Entity Recognition</title>
      <link>https://mullikine.github.io/posts/named-entity-recognition/</link>
      <pubDate>Wed, 04 Dec 2019 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/named-entity-recognition/</guid>
      <description>Original article Named Entity Recognition with NLTK and SpaCy - Towards Data Science  Missing libraries 1 2 3 4 5  Resource averaged_perceptron_tagger not found. Please use the NLTK Downloader to obtain the resource: &amp;gt;&amp;gt;&amp;gt; import nltk &amp;gt;&amp;gt;&amp;gt; nltk.download(&amp;#39;averaged_perceptron_tagger&amp;#39;)   1 2 3  import nltk nltk.download(&amp;#39;averaged_perceptron_tagger&amp;#39;) # and then type &amp;#39;d&amp;#39; for download and install &amp;#39;punkt&amp;#39;   I had to do it again for this 1 2  import nltk nltk.</description>
    </item>
    
    <item>
      <title>Part of Speech Labels</title>
      <link>https://mullikine.github.io/posts/part-of-speech-labels/</link>
      <pubDate>Wed, 04 Dec 2019 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/part-of-speech-labels/</guid>
      <description>1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  conjunction CONJ CNJ A part of speech that connects words, phrases, or clauses that are called the conjuncts of the conjunctions. Coordinating conjunction CC A conjunction placed between words, phrases, clauses, or sentences of equal rank, e.g. and, but, or. Predeterminer PDT A word or phrase that occurs before a determiner, typically quantifying the noun phrase, for example both or a lot of.</description>
    </item>
    
    <item>
      <title>Reading 25.11.19</title>
      <link>https://mullikine.github.io/posts/reading-25.11.19/</link>
      <pubDate>Mon, 25 Nov 2019 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/reading-25.11.19/</guid>
      <description>Language Models as Knowledge Bases?  Original article https://arxiv.org/abs/1909.01066 Code https://github.com/facebookresearch/LAMA  1 2 3 4 5 6 7 8 9 10 11  Cloze statements An excellent method to encourage speech production in children. A cloze statement involves saying a familiar phrase and leaving out a word, then waiting for your child to fill in the blank. Familiar nursery rhymes, songs, and poems are perfect for this activity.   Recent progress in pretraining LMs on large textual corpora led to a surge of improvements for downstream NLP tasks.</description>
    </item>
    
    <item>
      <title>Notes on BERT</title>
      <link>https://mullikine.github.io/posts/google-bert/</link>
      <pubDate>Sat, 16 Nov 2019 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/google-bert/</guid>
      <description>https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html
Pre-trained representations can either be context-free or contextual, and contextual representations can further be unidirectional or bidirectional.
1 2 3 4 5  node [style=filled,fillcolor=lightgrey,shape=box]; &amp;#34;Pre-trained representation&amp;#34; -&amp;gt; &amp;#34;context-free&amp;#34; &amp;#34;Pre-trained representation&amp;#34; -&amp;gt; contextual contextual -&amp;gt; unidirectional contextual -&amp;gt; bidirectional        LM supervision contextual Pre-trained Training data bi-directional     BERT unsupervised ✓ ✓ Plain text ✓   word2vec semi/self-supervised ✗      GloVe semi-supervised ✗       Supervision is a bit grey  Supervised If you consider that the network has to learn from it’s errors through back prop.</description>
    </item>
    
    <item>
      <title>TensorFlow BERT</title>
      <link>https://mullikine.github.io/posts/tensorflow-bert-keras/</link>
      <pubDate>Thu, 31 Oct 2019 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/tensorflow-bert-keras/</guid>
      <description>Original article Hugging Face: State-of-the-Art Natural Language Processing in ten lines of TensorFlow 2.0  A list of transformer architectures    architecture     BERT   RoBERTa   GPT-2   DistilBERT    pip&amp;rsquo;s transformers library Builds on 3 main classes:  configuration class tokenizer class model class  configuration class Hosts relevant information concerning the model we will be using, such as:</description>
    </item>
    
    <item>
      <title>Notes on &#34;Generating Beatles’ Lyrics with Machine Learning&#34;</title>
      <link>https://mullikine.github.io/posts/notes-on-generating-beatles-lyrics-with-machine-learning/</link>
      <pubDate>Thu, 24 Oct 2019 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/notes-on-generating-beatles-lyrics-with-machine-learning/</guid>
      <description>Original article Generating Beatles Lyrics with Machine Learning - Towards Data Science  Apparatus     URL      code https://github.com/EugenHotaj/beatles    dataset http://toti.eu.com/beatles/index.asp github.com/EugenHotaj/beatles/blob/master/scraper.py   embeddings     algorithm      Hypothesis Aim Questions  How might might generate sentences from a language model?  Method Factoids Unigram model Ignores any conditioning and simply chooses the next word randomly from the training data.</description>
    </item>
    
    <item>
      <title>Notes on &#34;Identifying the right meaning of the words using BERT&#34;</title>
      <link>https://mullikine.github.io/posts/notes-on-identifying-the-right-meaning-of-the-words-using-bert/</link>
      <pubDate>Thu, 24 Oct 2019 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/notes-on-identifying-the-right-meaning-of-the-words-using-bert/</guid>
      <description>Original article Identifying the right meaning of the words using BERT  Glossary 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  Uncased [model] The text has been lowercased before WordPiece tokenization, e.g., John Smith becomes john smith. The Uncased model also strips out any accent markers. Cased [model] The true case and accent markers are preserved.</description>
    </item>
    
    <item>
      <title>(WIP) Notes on &#34;Natural Language Processing by Jacob Eisenstein&#34;</title>
      <link>https://mullikine.github.io/posts/notes-on-eisenstein-nlp/</link>
      <pubDate>Wed, 23 Oct 2019 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/notes-on-eisenstein-nlp/</guid>
      <description>Reading vs +/&amp;#34;Kneser-Ney&amp;#34; $NOTES/ws/nlp-natural-language-processing/reading/eisenstein-nlp-notes.txt Kneser-Ney smoothing Based on absolute discounting, but it redistributes the resulting probability mass in a different way from Katz backoff.
Empirical evidence points to Kneser-Ney smoothing as the state-of-art for n-gram LMing.</description>
    </item>
    
    <item>
      <title>The Illustrated Transformer</title>
      <link>https://mullikine.github.io/posts/review-of-the-illustrated-transformer/</link>
      <pubDate>Sat, 19 Oct 2019 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/review-of-the-illustrated-transformer/</guid>
      <description>Original article https://jalammar.github.io/illustrated-transformer/ Source https://github.com/tensorflow/tensor2tensor  Prereading https://jalammar.github.io/illustrated-word2vec/
Helpful glossary 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51  Multilayer perceptron Feed-Forward Neural Network FFNN Basically, these are multi-level logistic regression classifiers.</description>
    </item>
    
    <item>
      <title>gwern.net GPT-2</title>
      <link>https://mullikine.github.io/posts/review-of-gewn-gpt2/</link>
      <pubDate>Thu, 17 Oct 2019 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/review-of-gewn-gpt2/</guid>
      <description>original article https://www.gwern.net/GPT-2  Read using eww The website when viewed with a GUI browser, although impressive, is distracting.
The website was built with hackyll, a static site generator employing haskell.
1  eww &amp;#34;https://www.gwern.net/GPT-2&amp;#34;   Ease of reading Straight-forward once you understand ML jargon.</description>
    </item>
    
    <item>
      <title>Keyword extraction to analyse articles</title>
      <link>https://mullikine.github.io/posts/keyword-extraction/</link>
      <pubDate>Thu, 17 Oct 2019 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/keyword-extraction/</guid>
      <description>sparsity [#text mining] Huge matrices are created based on word frequencies with many cells having zero values. This problem is called sparsity and is minimized using various techniques. Articles keyword extraction: nltk, sklearn Automated Keyword Extraction from Articles using NLP
kag datasets download benhamner/nips-papers textrank: numpy, spacy towardsdatascience.com/textrank-for-keyword-extraction-by-python-c0bae21bcec0
ngram, modified skip-gram, spacy Keywords Extraction with Ngram and Modified Skip-gram based on spaCy
TODO Turn the math4IQB lectures into keywords readsubs &amp;#34;https://www.</description>
    </item>
    
    <item>
      <title>Rewrite of gwern.net GPT-2 Neural Network Poetry</title>
      <link>https://mullikine.github.io/posts/rewrite-of-gwern-gpt2-poetry/</link>
      <pubDate>Thu, 17 Oct 2019 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/rewrite-of-gwern-gpt2-poetry/</guid>
      <description>original article https://www.gwern.net/GPT-2  Read using eww The website when viewed with a GUI browser, although impressive, is distracting.
The website was built with hackyll, a static site generator employing haskell.
1  eww &amp;#34;https://www.gwern.net/GPT-2&amp;#34;   Ease of reading Straight-forward once you understand ML jargon.</description>
    </item>
    
    <item>
      <title>(WIP) Review of NLP tools</title>
      <link>https://mullikine.github.io/posts/review-of-nlp-tools/</link>
      <pubDate>Wed, 16 Oct 2019 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/review-of-nlp-tools/</guid>
      <description>lm-explorer Interactive explorer for language models (currently only OpenAI GPT-2).</description>
    </item>
    
    <item>
      <title>writeup.ai</title>
      <link>https://mullikine.github.io/posts/review-of-writeup-ai/</link>
      <pubDate>Wed, 16 Oct 2019 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/review-of-writeup-ai/</guid>
      <description>Original article https://senrigan.io/blog/how-writeupai-runs-behind-the-scenes/#h%5F6068056784021570782144062  Glossary 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26  cross entropy loss (between two probability distributions) (and over the same underlying set of events) Higher loss is bad. Measures the performance of a classification model whose output is a probability value between 0 and 1. Measures the average number of bits needed to identify an event from the set.</description>
    </item>
    
    <item>
      <title>Blogs and Vlogs</title>
      <link>https://mullikine.github.io/posts/blogs-and-vlogs/</link>
      <pubDate>Mon, 14 Oct 2019 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/blogs-and-vlogs/</guid>
      <description>These are some of my favourite learning resources.
   source type url Topics     Math4IQB vlog https://www.youtube.com/user/Math4IQB/videos Information Theory   Gwern Bransen blog https://www.gwern.net/GPT-2 Haskell, NLP, R   Fabian Dablander blog https://fabiandablander.com Math, statistics, latex   John D. Cook blog https://www.johndcook.com/blog/ Math, statistics   Sacha Chua blog https://sachachua.com/blog/0000/00/05/ New emacs packages   Cameron Kerr blog http://humbledown.org https://distracted-it.blogspot.com Telecommunications    Misc articles Math &amp;amp; Statistics johndcook.</description>
    </item>
    
    <item>
      <title>Coherence in Natural Language (2006)</title>
      <link>https://mullikine.github.io/posts/review-coherence-in-natural-language/</link>
      <pubDate>Mon, 14 Oct 2019 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/review-coherence-in-natural-language/</guid>
      <description>Terminological mess The term cohesion does not figure in the index of the book.
Coherence vs cohesion &amp;ldquo;cohesion&amp;rdquo; for microstructures and &amp;ldquo;coherence&amp;rdquo; for macrostructures.
cohesion when talking about text and coherence when talking about discourse i.e. &amp;ldquo;semantics vs. pragmatics&amp;rdquo;
Issues covered in the book in the realm of coherence structures:
 cognitive science natural language engineering information extraction  Definitions Coherence structures ommitted: a chrestomathy of coherent vs incoherent text Reviews Coherence in natural language.</description>
    </item>
    
    <item>
      <title>(WIP) Applying Multinomial Naive Bayes to NLP Problems: A Practical Explanation</title>
      <link>https://mullikine.github.io/posts/review-multinomial-naive-bayes-nlp/</link>
      <pubDate>Fri, 11 Oct 2019 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/review-multinomial-naive-bayes-nlp/</guid>
      <description>Applying Multinomial Naive Bayes to NLP Problems: A Practical Explanation</description>
    </item>
    
    <item>
      <title>(WIP) Extending WordNut for generating blog titles</title>
      <link>https://mullikine.github.io/posts/extending-wordnut-for-generating-blog-titles/</link>
      <pubDate>Mon, 07 Oct 2019 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/extending-wordnut-for-generating-blog-titles/</guid>
      <description> Obtain an org-mode parser https://orgmode.org/worg/org-tools/index.html
Parse wordnut output to scrape synonyms from the buffer Given 2 words, create a list of synonyms for each Look for one word from each list with the same starting letter </description>
    </item>
    
    <item>
      <title>(WIP) Review of Language, trees, and geometry in neural networks</title>
      <link>https://mullikine.github.io/posts/language-trees-and-geometry-in-neural-networks/</link>
      <pubDate>Mon, 07 Oct 2019 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/language-trees-and-geometry-in-neural-networks/</guid>
      <description>1906.02715 Visualizing and Measuring the Geometry of BERT
https://pair-code.github.io/interpretability/bert-tree/
https://pair-code.github.io/interpretability/context-atlas/blogpost/
Existing representation: word embeddings Language is made of discrete structures, yet neural networks operate on continuous data: vectors in high-dimensional space.
A successful language-processing network must translate this symbolic information into some kind of geometric representation—but in what form?
Word embeddings provide two well-known examples: distance encodes semantic similarity, while certain directions correspond to polarities (e.g. male vs. female).</description>
    </item>
    
    <item>
      <title>PClean: A probabilistic scripting DSL</title>
      <link>https://mullikine.github.io/posts/pclean-gen-gpl/</link>
      <pubDate>Mon, 30 Sep 2019 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/pclean-gen-gpl/</guid>
      <description>Links  MIT Probabilistic Computing Project Introduction | Gen GitHub - probcomp/Gen: A general-purpose probabilistic programming system with programmable inference  Other talks by MIT Probabilistic Computing Project: Videos, Talks, and Podcasts - MIT Probabilistic Computing Project
Tools Gen  a package for the Julia programming language. consists of multiple modeling languages that are implemented as DSLs in Julia and a Julia library for inference programming.  PClean  A probabilistic scripting DSL in the Gen package.</description>
    </item>
    
  </channel>
</rss>