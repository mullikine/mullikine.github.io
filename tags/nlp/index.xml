<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>NLP on Bodacious Blog</title>
    <link>https://mullikine.github.io/tags/nlp/</link>
    <description>Recent content in NLP on Bodacious Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 24 Oct 2019 00:00:00 +1300</lastBuildDate>
    
	<atom:link href="https://mullikine.github.io/tags/nlp/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Notes on &#34;Generating Beatlesâ€™ Lyrics with Machine Learning&#34;</title>
      <link>https://mullikine.github.io/posts/notes-on-generating-beatles-lyrics-with-machine-learning/</link>
      <pubDate>Thu, 24 Oct 2019 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/notes-on-generating-beatles-lyrics-with-machine-learning/</guid>
      <description>Original article Generating Beatles Lyrics with Machine Learning - Towards Data Science  Apparatus     URL      code https://github.com/EugenHotaj/beatles    dataset http://toti.eu.com/beatles/index.asp github.com/EugenHotaj/beatles/blob/master/scraper.py   embeddings     algorithm      Hypothesis Aim Questions  How might might generate sentences from a language model?  Method Factoids Unigram model Ignores any conditioning and simply chooses the next word randomly from the training data.</description>
    </item>
    
    <item>
      <title>Notes on &#34;Identifying the right meaning of the words using BERT&#34;</title>
      <link>https://mullikine.github.io/posts/notes-on-identifying-the-right-meaning-of-the-words-using-bert/</link>
      <pubDate>Thu, 24 Oct 2019 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/notes-on-identifying-the-right-meaning-of-the-words-using-bert/</guid>
      <description>Original article Identifying the right meaning of the words using BERT  Glossary Uncased [model] The text has been lowercased before WordPiece tokenization, e.g., John Smith becomes john smith. The Uncased model also strips out any accent markers. Cased [model] The true case and accent markers are preserved. Typically, the Uncased model is better unless you know that case information is important for your task (e.g., Named Entity Recognition or Part-of-Speech tagging).</description>
    </item>
    
    <item>
      <title>(WIP) Notes on &#34;Natural Language Processing by Jacob Eisenstein&#34;</title>
      <link>https://mullikine.github.io/posts/notes-on-eisenstein-nlp/</link>
      <pubDate>Wed, 23 Oct 2019 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/notes-on-eisenstein-nlp/</guid>
      <description>Reading vs +/&amp;#34;Kneser-Ney&amp;#34; $NOTES/ws/nlp-natural-language-processing/reading/eisenstein-nlp-notes.txt Kneser-Ney smoothing Based on absolute discounting, but it redistributes the resulting probability mass in a different way from Katz backoff.
Empirical evidence points to Kneser-Ney smoothing as the state-of-art for n-gram LMing.</description>
    </item>
    
    <item>
      <title>Overview of The Illustrated Transformer</title>
      <link>https://mullikine.github.io/posts/review-of-the-illustrated-transformer/</link>
      <pubDate>Sat, 19 Oct 2019 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/review-of-the-illustrated-transformer/</guid>
      <description>Original article https://jalammar.github.io/illustrated-transformer/ Source https://github.com/tensorflow/tensor2tensor  Prereading https://jalammar.github.io/illustrated-word2vec/
Helpful glossary Multilayer perceptron Feed-Forward Neural Network FFNN Basically, these are multi-level logistic regression classifiers. Many layers of scales are separated by non-linearities. Can be used for as autoencoders. Can be used to train a classifier or extract functions as autoencoders. self-attention intra-attention [attention mechanism] Intuition: Reflects on its own position/context within a greater whole. Relates different positions of a single sequence in order to compute a representation of the sequence.</description>
    </item>
    
    <item>
      <title>Keyword extraction to analyse articles</title>
      <link>https://mullikine.github.io/posts/keyword-extraction/</link>
      <pubDate>Thu, 17 Oct 2019 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/keyword-extraction/</guid>
      <description>sparsity [#text mining] Huge matrices are created based on word frequencies with many cells having zero values. This problem is called sparsity and is minimized using various techniques. Articles keyword extraction: nltk, sklearn Automated Keyword Extraction from Articles using NLP
kag datasets download benhamner/nips-papers textrank: numpy, spacy towardsdatascience.com/textrank-for-keyword-extraction-by-python-c0bae21bcec0
ngram, modified skip-gram, spacy Keywords Extraction with Ngram and Modified Skip-gram based on spaCy
TODO Turn the math4IQB lectures into keywords readsubs &amp;#34;https://www.</description>
    </item>
    
    <item>
      <title>Overview of gwern.net GPT-2</title>
      <link>https://mullikine.github.io/posts/review-of-gewn-gpt2/</link>
      <pubDate>Thu, 17 Oct 2019 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/review-of-gewn-gpt2/</guid>
      <description>original article https://www.gwern.net/GPT-2  Read using eww The website when viewed with a GUI browser, although impressive, is distracting.
The website was built with hackyll, a static site generator employing haskell.
eww &amp;#34;https://www.gwern.net/GPT-2&amp;#34; Ease of reading Straight-forward once you understand ML jargon.</description>
    </item>
    
    <item>
      <title>(WIP) Review of NLP tools</title>
      <link>https://mullikine.github.io/posts/review-of-nlp-tools/</link>
      <pubDate>Wed, 16 Oct 2019 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/review-of-nlp-tools/</guid>
      <description>lm-explorer Interactive explorer for language models (currently only OpenAI GPT-2).</description>
    </item>
    
    <item>
      <title>Review of writeup.ai</title>
      <link>https://mullikine.github.io/posts/review-of-writeup-ai/</link>
      <pubDate>Wed, 16 Oct 2019 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/review-of-writeup-ai/</guid>
      <description>https://senrigan.io/blog/how-writeupai-runs-behind-the-scenes/#h%5F6068056784021570782144062
Glossary cross entropy loss (between two probability distributions) (and over the same underlying set of events) Higher loss is bad. Measures the performance of a classification model whose output is a probability value between 0 and 1. Measures the average number of bits needed to identify an event from the set. Data scrubbing [error correction technique] Uses a background task to periodically inspect main memory or storage for errors, then correct detected errors using redundant data in the form of different checksums or copies of data.</description>
    </item>
    
    <item>
      <title>Blogs and Vlogs</title>
      <link>https://mullikine.github.io/posts/blogs-and-vlogs/</link>
      <pubDate>Mon, 14 Oct 2019 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/blogs-and-vlogs/</guid>
      <description> These are some of my favourite learning resources.
   source type url Topics     Math4IQB vlog https://www.youtube.com/user/Math4IQB/videos Information Theory   Gwern Bransen blog https://www.gwern.net/GPT-2 Haskell, NLP, R    Misc articles NLP writeup.ai https://senrigan.io/blog/how-writeupai-runs-behind-the-scenes/#h%5F6068056784021570782144062
Text generator with Keras https://www.thepythoncode.com/article/text-generation-keras-python
Keyword extraction  nltk, sklearn
https://medium.com/analytics-vidhya/automated-keyword-extraction-from-articles-using-nlp-bfd864f41b34
 numpy, spacy
https://towardsdatascience.com/textrank-for-keyword-extraction-by-python-c0bae21bcec0
 ngram, modified skip-gram, spacy
https://medium.com/reputation-com-datascience-blog/keywords-extraction-with-ngram-and-modified-skip-gram-based-on-spacy-14e5625fce23
  </description>
    </item>
    
    <item>
      <title>(WIP) Overview of &#34;Applying Multinomial Naive Bayes to NLP Problems: A Practical Explanation&#34;</title>
      <link>https://mullikine.github.io/posts/review-multinomial-naive-bayes-nlp/</link>
      <pubDate>Fri, 11 Oct 2019 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/review-multinomial-naive-bayes-nlp/</guid>
      <description>Applying Multinomial Naive Bayes to NLP Problems: A Practical Explanation</description>
    </item>
    
    <item>
      <title>(WIP) Extending WordNut for generating blog titles</title>
      <link>https://mullikine.github.io/posts/extending-wordnut-for-generating-blog-titles/</link>
      <pubDate>Mon, 07 Oct 2019 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/extending-wordnut-for-generating-blog-titles/</guid>
      <description> Obtain an org-mode parser https://orgmode.org/worg/org-tools/index.html
Parse wordnut output to scrape synonyms from the buffer Given 2 words, create a list of synonyms for each Look for one word from each list with the same starting letter </description>
    </item>
    
    <item>
      <title>(WIP) Review of Language, trees, and geometry in neural networks</title>
      <link>https://mullikine.github.io/posts/language-trees-and-geometry-in-neural-networks/</link>
      <pubDate>Mon, 07 Oct 2019 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/language-trees-and-geometry-in-neural-networks/</guid>
      <description>1906.02715 Visualizing and Measuring the Geometry of BERT
https://pair-code.github.io/interpretability/bert-tree/
https://pair-code.github.io/interpretability/context-atlas/blogpost/
Existing representation: word embeddings Language is made of discrete structures, yet neural networks operate on continuous data: vectors in high-dimensional space.
A successful language-processing network must translate this symbolic information into some kind of geometric representationâ€”but in what form?
Word embeddings provide two well-known examples: distance encodes semantic similarity, while certain directions correspond to polarities (e.g. male vs. female).</description>
    </item>
    
  </channel>
</rss>