<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>reading on Bodacious Blog</title>
    <link>https://mullikine.github.io/tags/reading/</link>
    <description>Recent content in reading on Bodacious Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 07 Oct 2019 00:00:00 +1300</lastBuildDate><atom:link href="https://mullikine.github.io/tags/reading/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>(WIP) Review of Language, trees, and geometry in neural networks</title>
      <link>https://mullikine.github.io/posts/language-trees-and-geometry-in-neural-networks/</link>
      <pubDate>Mon, 07 Oct 2019 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/language-trees-and-geometry-in-neural-networks/</guid>
      <description>1906.02715 Visualizing and Measuring the Geometry of BERT
https://pair-code.github.io/interpretability/bert-tree/
https://pair-code.github.io/interpretability/context-atlas/blogpost/
Existing representation: word embeddings Language is made of discrete structures, yet neural networks operate on continuous data: vectors in high-dimensional space.
A successful language-processing network must translate this symbolic information into some kind of geometric representationâ€”but in what form?
Word embeddings provide two well-known examples: distance encodes semantic similarity, while certain directions correspond to polarities (e.g. male vs. female).
New representation A recent, fascinating discovery points to an entirely new type of representation.</description>
    </item>
    
  </channel>
</rss>
