<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>latex on Bodacious Blog</title>
    <link>https://mullikine.github.io/tags/latex/</link>
    <description>Recent content in latex on Bodacious Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 12 Apr 2021 00:00:00 +1200</lastBuildDate><atom:link href="https://mullikine.github.io/tags/latex/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Blogs and Vlogs</title>
      <link>https://mullikine.github.io/posts/blogs-and-vlogs/</link>
      <pubDate>Mon, 12 Apr 2021 00:00:00 +1200</pubDate>
      
      <guid>https://mullikine.github.io/posts/blogs-and-vlogs/</guid>
      <description>Blogs and vlogs These people have been influential to me.
2021    source type url topics     Laria Reynolds blog https://generative.ink/ Prompt Engineering, OpenAI, EleutherAI   Basile Verhulst blog http://crossingvalleys.com/ Travelling, Digital nomad   Carin Meier blog http://gigasquidsoftware.com/ Clojure, NLP   Mark Watson books https://markwatson.com/ Haskell, Clojure, Lisp, NLP, Ontology   John Stevenson blog http://jr0cket.co.uk/ Clojure, emacs    2019    source type url Topics     Math4IQB vlog Math4IQB - YouTube Information Theory   Gwern Bransen blog GPT-2 Neural Network Poetry Gwern.</description>
    </item>
    
    <item>
      <title>latex, math and emacs</title>
      <link>https://mullikine.github.io/posts/latex-math/</link>
      <pubDate>Sat, 23 Nov 2019 00:00:00 +0800</pubDate>
      
      <guid>https://mullikine.github.io/posts/latex-math/</guid>
      <description>n choose k \({n \choose k} = _{n}^{k}\textrm{C}= \frac{n!}{k!(n-k)!}\)
https://byjus.com/n-choose-k-formula/
\((x+y)^n = \sum_{k=0}^n %{n \choose k} x^{n - k} y^k\)
\begin{equation} \label{eq:1} C = W\log_{2} (1+\mathrm{SNR}) \end{equation}
binomial theorem In elementary algebra, the binomial theorem describes the algebraic expansion of powers of a binomial.
\((a+b)^n=\sum_{k=0}^n{n\choose k}a^{n-k}b^k\)</description>
    </item>
    
    <item>
      <title>Latex and Machine Learning</title>
      <link>https://mullikine.github.io/posts/machine-learning/</link>
      <pubDate>Mon, 04 Nov 2019 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/machine-learning/</guid>
      <description>Information Gain  Original article Information Gain and Mutual Information for Machine Learning  \begin{equation} \mathbf{IG}(\mathbf{S}, a) = \mathbf{H}(\mathbf{S}) – \mathbf{H}(\mathbf{S} | a) \end{equation}
Mutual information  References Information Gain and Mutual Information for Machine Learning \
An introduction to mutual information - YouTube  Concerns the outcome of two random variables.
If we know the value of one of the random variables in a system there is a corresponding reduction in uncertainty for predicting the other one and mutual information measures that reduction in uncertainty.</description>
    </item>
    
    <item>
      <title>Entropy, Cross-Entropy and KL-Divergence</title>
      <link>https://mullikine.github.io/posts/entropy-cross-entropy-and-kl-divergence/</link>
      <pubDate>Sat, 02 Nov 2019 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/entropy-cross-entropy-and-kl-divergence/</guid>
      <description>Original video A Short Introduction to Entropy, Cross-Entropy and KL-Divergence - YouTube Related reading https://blog.floydhub.com/knowledge-distillation/  Glossary 1 2 3 4 5 6 7 8 9 10 11 12 13  marginalized Treated as insignificant or peripheral. marginal likelihood function integrated likelihood model evidence evidence [#statistics] [#bayesian statistics] A likelihood function in which some parameter variables have been marginalized.   Predicted distribution vs true distribution Predicted distribution When designing a code to represent weather predictions, you try to assign fewer bits for outcomes which are probably going to be more common.</description>
    </item>
    
    <item>
      <title>Variational Inference</title>
      <link>https://mullikine.github.io/posts/variational-inference/</link>
      <pubDate>Fri, 01 Nov 2019 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/variational-inference/</guid>
      <description>Original article https://fabiandablander.com/r/Variational-Inference.html  Prereading https://mullikine.github.io/posts/entropy-cross-entropy-and-kl-divergence/
Bayes&#39; Theorm 1 2 3 4 5 6 7 8 9 10 11  \begin{equation} \underbrace{p(\mathbf{z} \mid \mathbf{x})}_{\text{Posterior}} = \underbrace{p(\mathbf{z})}_{\text{Prior}} \times \frac{\overbrace{p(\mathbf{x} \mid \mat hbf{z})}^{\text{Likelihood}}}{\underbrace{\int p(\mathbf{x} \mid \mathbf{z}) \, p(\mathbf{z}) \, \mathrm{d}\mathbf{z}}_{\text{Marginal Likelihood}}} \enspace , \end{equation} where $\mathbf{z}$ denotes latent parameters we want to infer and $\mathbf{x}$ denotes data.   \begin{equation} \underbrace{p(\mathbf{z} \mid \mathbf{x})}_{\text{Posterior}} = \underbrace{p(\mathbf{z})}_{\text{Prior}} \times \frac{\overbrace{p(\mathbf{x} \mid \mathbf{z})}^{\text{Likelihood}}}{\underbrace{\int p(\mathbf{x} \mid \mathbf{z}) , p(\mathbf{z}) , \mathrm{d}\mathbf{z}}_{\text{Marginal Likelihood}}} \enspace , \end{equation}</description>
    </item>
    
    <item>
      <title>LaTeX in emacs</title>
      <link>https://mullikine.github.io/posts/latex-in-emacs/</link>
      <pubDate>Wed, 30 Oct 2019 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/latex-in-emacs/</guid>
      <description>Prereading Compounding Confoundment: arbitrary interpreters for Babel // Bodacious Blog
Setup Create the texalg2png script 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  #!/bin/bash export TTY read -r -d &amp;#39;&amp;#39; texcode &amp;lt;&amp;lt;HEREDOC \documentclass{standalone} \usepackage{varwidth} \usepackage{algorithm} %ctan.org\pkg\algorithms \usepackage{algpseudocode} \begin{document} \begin{varwidth}{\linewidth} \par\noindent \begin{algorithmic}[1] $(cat) \end{algorithmic} \end{varwidth} \end{document} HEREDOC printf -- &amp;#34;%s&amp;#34; &amp;#34;$texcode&amp;#34; | tex2png &amp;#34;$@&amp;#34;   Create the tex2png script 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39  #!</description>
    </item>
    
    <item>
      <title>Just a fun bit of math in my day</title>
      <link>https://mullikine.github.io/posts/fun-math/</link>
      <pubDate>Fri, 25 Oct 2019 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/fun-math/</guid>
      <description>Useful mathematical symbols    symbol tex digraph how it reads      &amp;lt;:   is a subtype of    ⊧ \vDash &amp;lt;bar&amp;gt; = entails    ⊢ \vdash &amp;lt;bar&amp;gt; - infers    → \to -&amp;gt; is mapped to maps sets to sets   ↦ \mapsto &amp;lt;bar&amp;gt; &amp;gt; is mapped to maps elements to elements    Euler&amp;rsquo;s Characteristic The second most beautiful equation and its surprising applications - YouTube</description>
    </item>
    
    <item>
      <title>Compounding Confoundment: arbitrary interpreters for Babel</title>
      <link>https://mullikine.github.io/posts/arbitrary-interpreters-for-babel/</link>
      <pubDate>Sun, 20 Oct 2019 00:00:00 +0800</pubDate>
      
      <guid>https://mullikine.github.io/posts/arbitrary-interpreters-for-babel/</guid>
      <description>Genesis 11:7 “&amp;hellip;Come, let us go down and confuse their language so they will not understand each other.” If supporting many languages in Babel was not confounding enough, lets support arbitrary interpreters too!  The need to specify a custom interpreter arose when I needed to provide my own interpreter for generating an ASCII graph from a dot script.
Objective Specify an :interpreter and/or :filter command to override the execute behaviour.</description>
    </item>
    
  </channel>
</rss>
