<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>GPT-3 on Bodacious Blog</title>
    <link>https://mullikine.github.io/tags/gpt-3/</link>
    <description>Recent content in GPT-3 on Bodacious Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 07 Mar 2021 00:00:00 +1300</lastBuildDate><atom:link href="https://mullikine.github.io/tags/gpt-3/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>A natural language database using a single GPT prompt</title>
      <link>https://mullikine.github.io/posts/a-natural-language-database-using-a-single-gpt-prompt/</link>
      <pubDate>Sun, 07 Mar 2021 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/a-natural-language-database-using-a-single-gpt-prompt/</guid>
      <description>Original article https://www.gwern.net/GPT-3#the-database-prompt  Summary A single prompt describes transactions to and from a database.
GPT-3 is able to answer questions about the transactions that have taken place.
GPT-3 isn&amp;rsquo;t actually a database.
The LM simply understands language so well that describing the transactions that have taken place would naturally lead to the GPT-3 response.
The prompt 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55  title: &amp;#34;database example&amp;#34; doc: &amp;#34;GPT-3 as a NL interface for semantically querying logic in prose&amp;#34; prompt: |+The database begins knowing nothing.</description>
    </item>
    
    <item>
      <title>Translating Haskell to Clojure with GPT-3</title>
      <link>https://mullikine.github.io/posts/translating-haskell-to-clojure-with-gpt-3/</link>
      <pubDate>Sun, 07 Mar 2021 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/translating-haskell-to-clojure-with-gpt-3/</guid>
      <description>Relevant material https://hyperpolyglot.org/ml  Summary Who needs hyperpolyglot when you have GPT-3?
I translate Haskell into Clojure using the following prompt.
haskell-to-clojure.prompt 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50  title: &amp;#34;Translate Haskell to Clojure&amp;#34; prompt: |+Haskell: zip (map show [1,5,9]) [&amp;#34;a&amp;#34;,&amp;#34;b&amp;#34;,&amp;#34;c&amp;#34;] Clojure: (println (map vector &amp;#39;(1 2 3) &amp;#39;(4 5 6))) Haskell: map toUpper &amp;#34;MiXeD cAsE&amp;#34; Clojure: (clojure.</description>
    </item>
    
    <item>
      <title>crontab.guru in emacs and making a prompt with GPT-3 to copy it</title>
      <link>https://mullikine.github.io/posts/crontab-guru-in-emacs/</link>
      <pubDate>Sat, 06 Mar 2021 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/crontab-guru-in-emacs/</guid>
      <description>Related https://crontab.guru/  Summary I build some functionality into emacs to use crontab.guru behind the scenes to interpret tab lines displaying inside of emacs, without using the web browser.
I then build a GPT-3 prompt which does exactly the same thing without crontab.guru and provide the initial script I made to examplary (my GPT-3 DSL) as an example generator, to enhance the prompt if that is needed later.
Initial steps When lines in cron format appear in an emacs buffer, the crontab-guru function is suggested, allowing you to easily understand crontabs.</description>
    </item>
    
    <item>
      <title>Fine-tuning GPT-3</title>
      <link>https://mullikine.github.io/posts/fine-tuning-gpt-3/</link>
      <pubDate>Sun, 21 Feb 2021 00:00:00 +1300</pubDate>
      
      <guid>https://mullikine.github.io/posts/fine-tuning-gpt-3/</guid>
      <description>Notes on GPT-3 fine-tuning http://github.com/mullikine/fine-tuning-gpt-3  Fine-tuning GPT-3 to generate puns Aims Train GPT-3 to continue on sequences of puns Train GPT-3 to speak in puns Training Data    format     jsonl       Sources
 https://github.com/taivop/joke-dataset       Naive approach
1 2 3  [{&amp;#34;data&amp;#34; : &amp;#34;joke set 1&amp;#34;}, {&amp;#34;data&amp;#34;: &amp;#34;joke set 2&amp;#34;}         Better approach</description>
    </item>
    
  </channel>
</rss>
